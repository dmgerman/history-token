multiline_comment|/*&n; * lib/kernel_lock.c&n; *&n; * This is the traditional BKL - big kernel lock. Largely&n; * relegated to obsolescense, but used by various less&n; * important (or lazy) subsystems.&n; */
macro_line|#include &lt;linux/smp_lock.h&gt;
macro_line|#include &lt;linux/module.h&gt;
multiline_comment|/*&n; * The &squot;big kernel lock&squot;&n; *&n; * This spinlock is taken and released recursively by lock_kernel()&n; * and unlock_kernel().  It is transparently dropped and reaquired&n; * over schedule().  It is used to protect legacy code that hasn&squot;t&n; * been migrated to a proper locking design yet.&n; *&n; * Don&squot;t use in new code.&n; */
DECL|variable|__cacheline_aligned_in_smp
r_static
id|spinlock_t
id|kernel_flag
id|__cacheline_aligned_in_smp
op_assign
id|SPIN_LOCK_UNLOCKED
suffix:semicolon
multiline_comment|/*&n; * Acquire/release the underlying lock from the scheduler.&n; *&n; * This is called with preemption disabled, and should&n; * return an error value if it cannot get the lock and&n; * TIF_NEED_RESCHED gets set.&n; *&n; * If it successfully gets the lock, it should increment&n; * the preemption count like any spinlock does.&n; *&n; * (This works on UP too - _raw_spin_trylock will never&n; * return false in that case)&n; */
DECL|function|get_kernel_lock
r_int
id|__lockfunc
id|get_kernel_lock
c_func
(paren
r_void
)paren
(brace
r_while
c_loop
(paren
op_logical_neg
id|_raw_spin_trylock
c_func
(paren
op_amp
id|kernel_flag
)paren
)paren
(brace
r_if
c_cond
(paren
id|test_thread_flag
c_func
(paren
id|TIF_NEED_RESCHED
)paren
)paren
r_return
op_minus
id|EAGAIN
suffix:semicolon
id|cpu_relax
c_func
(paren
)paren
suffix:semicolon
)brace
id|preempt_disable
c_func
(paren
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
DECL|function|put_kernel_lock
r_void
id|__lockfunc
id|put_kernel_lock
c_func
(paren
r_void
)paren
(brace
id|_raw_spin_unlock
c_func
(paren
op_amp
id|kernel_flag
)paren
suffix:semicolon
id|preempt_enable_no_resched
c_func
(paren
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * These are the BKL spinlocks - we try to be polite about preemption. &n; * If SMP is not on (ie UP preemption), this all goes away because the&n; * _raw_spin_trylock() will always succeed.&n; */
macro_line|#ifdef CONFIG_PREEMPT
DECL|function|__lock_kernel
r_static
r_inline
r_void
id|__lock_kernel
c_func
(paren
r_void
)paren
(brace
id|preempt_disable
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|_raw_spin_trylock
c_func
(paren
op_amp
id|kernel_flag
)paren
)paren
)paren
(brace
multiline_comment|/*&n;&t;&t; * If preemption was disabled even before this&n;&t;&t; * was called, there&squot;s nothing we can be polite&n;&t;&t; * about - just spin.&n;&t;&t; */
r_if
c_cond
(paren
id|preempt_count
c_func
(paren
)paren
OG
l_int|1
)paren
(brace
id|_raw_spin_lock
c_func
(paren
op_amp
id|kernel_flag
)paren
suffix:semicolon
r_return
suffix:semicolon
)brace
multiline_comment|/*&n;&t;&t; * Otherwise, let&squot;s wait for the kernel lock&n;&t;&t; * with preemption enabled..&n;&t;&t; */
r_do
(brace
id|preempt_enable
c_func
(paren
)paren
suffix:semicolon
r_while
c_loop
(paren
id|spin_is_locked
c_func
(paren
op_amp
id|kernel_flag
)paren
)paren
id|cpu_relax
c_func
(paren
)paren
suffix:semicolon
id|preempt_disable
c_func
(paren
)paren
suffix:semicolon
)brace
r_while
c_loop
(paren
op_logical_neg
id|_raw_spin_trylock
c_func
(paren
op_amp
id|kernel_flag
)paren
)paren
suffix:semicolon
)brace
)brace
macro_line|#else
multiline_comment|/*&n; * Non-preemption case - just get the spinlock&n; */
DECL|function|__lock_kernel
r_static
r_inline
r_void
id|__lock_kernel
c_func
(paren
r_void
)paren
(brace
id|_raw_spin_lock
c_func
(paren
op_amp
id|kernel_flag
)paren
suffix:semicolon
)brace
macro_line|#endif
DECL|function|__unlock_kernel
r_static
r_inline
r_void
id|__unlock_kernel
c_func
(paren
r_void
)paren
(brace
id|_raw_spin_unlock
c_func
(paren
op_amp
id|kernel_flag
)paren
suffix:semicolon
id|preempt_enable
c_func
(paren
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Getting the big kernel lock.&n; *&n; * This cannot happen asynchronously, so we only need to&n; * worry about other CPU&squot;s.&n; */
DECL|function|lock_kernel
r_void
id|__lockfunc
id|lock_kernel
c_func
(paren
r_void
)paren
(brace
r_int
id|depth
op_assign
id|current-&gt;lock_depth
op_plus
l_int|1
suffix:semicolon
r_if
c_cond
(paren
id|likely
c_func
(paren
op_logical_neg
id|depth
)paren
)paren
id|__lock_kernel
c_func
(paren
)paren
suffix:semicolon
id|current-&gt;lock_depth
op_assign
id|depth
suffix:semicolon
)brace
DECL|function|unlock_kernel
r_void
id|__lockfunc
id|unlock_kernel
c_func
(paren
r_void
)paren
(brace
id|BUG_ON
c_func
(paren
id|current-&gt;lock_depth
OL
l_int|0
)paren
suffix:semicolon
r_if
c_cond
(paren
id|likely
c_func
(paren
op_decrement
id|current-&gt;lock_depth
OL
l_int|0
)paren
)paren
id|__unlock_kernel
c_func
(paren
)paren
suffix:semicolon
)brace
DECL|variable|lock_kernel
id|EXPORT_SYMBOL
c_func
(paren
id|lock_kernel
)paren
suffix:semicolon
DECL|variable|unlock_kernel
id|EXPORT_SYMBOL
c_func
(paren
id|unlock_kernel
)paren
suffix:semicolon
eof
