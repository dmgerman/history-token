multiline_comment|/*&n; * Dynamic DMA mapping support.&n; *&n; * This implementation is for IA-64 platforms that do not support&n; * I/O TLBs (aka DMA address translation hardware).&n; * Copyright (C) 2000 Asit Mallick &lt;Asit.K.Mallick@intel.com&gt;&n; * Copyright (C) 2000 Goutham Rao &lt;goutham.rao@intel.com&gt;&n; *&n; * 00/12/13 davidm&t;Rename to swiotlb.c and add mark_clean() to avoid&n; *&t;&t;&t;unnecessary i-cache flushing.&n; */
macro_line|#include &lt;linux/cache.h&gt;
macro_line|#include &lt;linux/mm.h&gt;
macro_line|#include &lt;linux/module.h&gt;
macro_line|#include &lt;linux/pci.h&gt;
macro_line|#include &lt;linux/spinlock.h&gt;
macro_line|#include &lt;linux/string.h&gt;
macro_line|#include &lt;linux/types.h&gt;
macro_line|#include &lt;asm/io.h&gt;
macro_line|#include &lt;asm/pci.h&gt;
macro_line|#include &lt;asm/dma.h&gt;
macro_line|#include &lt;linux/init.h&gt;
macro_line|#include &lt;linux/bootmem.h&gt;
DECL|macro|OFFSET
mdefine_line|#define OFFSET(val,align) ((unsigned long)&t;&bslash;&n;&t;                   ( (val) &amp; ( (align) - 1)))
DECL|macro|SG_ENT_VIRT_ADDRESS
mdefine_line|#define SG_ENT_VIRT_ADDRESS(sg)&t;(page_address((sg)-&gt;page) + (sg)-&gt;offset)
DECL|macro|SG_ENT_PHYS_ADDRESS
mdefine_line|#define SG_ENT_PHYS_ADDRESS(SG)&t;virt_to_phys(SG_ENT_VIRT_ADDRESS(SG))
multiline_comment|/*&n; * Maximum allowable number of contiguous slabs to map,&n; * must be a power of 2.  What is the appropriate value ?&n; * The complexity of {map,unmap}_single is linearly dependent on this value.&n; */
DECL|macro|IO_TLB_SEGSIZE
mdefine_line|#define IO_TLB_SEGSIZE&t;128
multiline_comment|/*&n; * log of the size of each IO TLB slab.  The number of slabs is command line controllable.&n; */
DECL|macro|IO_TLB_SHIFT
mdefine_line|#define IO_TLB_SHIFT 11
multiline_comment|/*&n; * Used to do a quick range check in swiotlb_unmap_single and swiotlb_sync_single, to see&n; * if the memory was in fact allocated by this API.&n; */
DECL|variable|io_tlb_start
DECL|variable|io_tlb_end
r_static
r_char
op_star
id|io_tlb_start
comma
op_star
id|io_tlb_end
suffix:semicolon
multiline_comment|/*&n; * The number of IO TLB blocks (in groups of 64) betweeen io_tlb_start and io_tlb_end.&n; * This is command line adjustable via setup_io_tlb_npages.&n; */
DECL|variable|io_tlb_nslabs
r_static
r_int
r_int
id|io_tlb_nslabs
op_assign
l_int|1024
suffix:semicolon
multiline_comment|/*&n; * This is a free list describing the number of free entries available from each index&n; */
DECL|variable|io_tlb_list
r_static
r_int
r_int
op_star
id|io_tlb_list
suffix:semicolon
DECL|variable|io_tlb_index
r_static
r_int
r_int
id|io_tlb_index
suffix:semicolon
multiline_comment|/*&n; * We need to save away the original address corresponding to a mapped entry for the sync&n; * operations.&n; */
DECL|variable|io_tlb_orig_addr
r_static
r_int
r_char
op_star
op_star
id|io_tlb_orig_addr
suffix:semicolon
multiline_comment|/*&n; * Protect the above data structures in the map and unmap calls&n; */
DECL|variable|io_tlb_lock
r_static
id|spinlock_t
id|io_tlb_lock
op_assign
id|SPIN_LOCK_UNLOCKED
suffix:semicolon
r_static
r_int
id|__init
DECL|function|setup_io_tlb_npages
id|setup_io_tlb_npages
(paren
r_char
op_star
id|str
)paren
(brace
id|io_tlb_nslabs
op_assign
id|simple_strtoul
c_func
(paren
id|str
comma
l_int|NULL
comma
l_int|0
)paren
op_lshift
(paren
id|PAGE_SHIFT
op_minus
id|IO_TLB_SHIFT
)paren
suffix:semicolon
multiline_comment|/* avoid tail segment of size &lt; IO_TLB_SEGSIZE */
id|io_tlb_nslabs
op_assign
id|ALIGN
c_func
(paren
id|io_tlb_nslabs
comma
id|IO_TLB_SEGSIZE
)paren
suffix:semicolon
r_return
l_int|1
suffix:semicolon
)brace
id|__setup
c_func
(paren
l_string|&quot;swiotlb=&quot;
comma
id|setup_io_tlb_npages
)paren
suffix:semicolon
multiline_comment|/*&n; * Statically reserve bounce buffer space and initialize bounce buffer data structures for&n; * the software IO TLB used to implement the PCI DMA API.&n; */
r_void
DECL|function|swiotlb_init
id|swiotlb_init
(paren
r_void
)paren
(brace
r_int
id|i
suffix:semicolon
multiline_comment|/*&n;&t; * Get IO TLB memory from the low pages&n;&t; */
id|io_tlb_start
op_assign
id|alloc_bootmem_low_pages
c_func
(paren
id|io_tlb_nslabs
op_star
(paren
l_int|1
op_lshift
id|IO_TLB_SHIFT
)paren
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|io_tlb_start
)paren
id|BUG
c_func
(paren
)paren
suffix:semicolon
id|io_tlb_end
op_assign
id|io_tlb_start
op_plus
id|io_tlb_nslabs
op_star
(paren
l_int|1
op_lshift
id|IO_TLB_SHIFT
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Allocate and initialize the free list array.  This array is used&n;&t; * to find contiguous free memory regions of size up to IO_TLB_SEGSIZE&n;&t; * between io_tlb_start and io_tlb_end.&n;&t; */
id|io_tlb_list
op_assign
id|alloc_bootmem
c_func
(paren
id|io_tlb_nslabs
op_star
r_sizeof
(paren
r_int
)paren
)paren
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
id|io_tlb_nslabs
suffix:semicolon
id|i
op_increment
)paren
id|io_tlb_list
(braket
id|i
)braket
op_assign
id|IO_TLB_SEGSIZE
op_minus
id|OFFSET
c_func
(paren
id|i
comma
id|IO_TLB_SEGSIZE
)paren
suffix:semicolon
id|io_tlb_index
op_assign
l_int|0
suffix:semicolon
id|io_tlb_orig_addr
op_assign
id|alloc_bootmem
c_func
(paren
id|io_tlb_nslabs
op_star
r_sizeof
(paren
r_char
op_star
)paren
)paren
suffix:semicolon
id|printk
c_func
(paren
l_string|&quot;Placing software IO TLB between 0x%p - 0x%p&bslash;n&quot;
comma
(paren
r_void
op_star
)paren
id|io_tlb_start
comma
(paren
r_void
op_star
)paren
id|io_tlb_end
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Allocates bounce buffer and returns its kernel virtual address.&n; */
r_static
r_void
op_star
DECL|function|map_single
id|map_single
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
r_char
op_star
id|buffer
comma
r_int
id|size
comma
r_int
id|direction
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
r_char
op_star
id|dma_addr
suffix:semicolon
r_int
r_int
id|nslots
comma
id|stride
comma
id|index
comma
id|wrap
suffix:semicolon
r_int
id|i
suffix:semicolon
multiline_comment|/*&n;&t; * For mappings greater than a page size, we limit the stride (and hence alignment)&n;&t; * to a page size.&n;&t; */
id|nslots
op_assign
id|ALIGN
c_func
(paren
id|size
comma
l_int|1
op_lshift
id|IO_TLB_SHIFT
)paren
op_rshift
id|IO_TLB_SHIFT
suffix:semicolon
r_if
c_cond
(paren
id|size
OG
(paren
l_int|1
op_lshift
id|PAGE_SHIFT
)paren
)paren
id|stride
op_assign
(paren
l_int|1
op_lshift
(paren
id|PAGE_SHIFT
op_minus
id|IO_TLB_SHIFT
)paren
)paren
suffix:semicolon
r_else
id|stride
op_assign
l_int|1
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|nslots
)paren
id|BUG
c_func
(paren
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Find suitable number of IO TLB entries size that will fit this request and&n;&t; * allocate a buffer from that IO TLB pool.&n;&t; */
id|spin_lock_irqsave
c_func
(paren
op_amp
id|io_tlb_lock
comma
id|flags
)paren
suffix:semicolon
(brace
id|wrap
op_assign
id|index
op_assign
id|ALIGN
c_func
(paren
id|io_tlb_index
comma
id|stride
)paren
suffix:semicolon
r_if
c_cond
(paren
id|index
op_ge
id|io_tlb_nslabs
)paren
id|wrap
op_assign
id|index
op_assign
l_int|0
suffix:semicolon
r_do
(brace
multiline_comment|/*&n;&t;&t;&t; * If we find a slot that indicates we have &squot;nslots&squot; number of&n;&t;&t;&t; * contiguous buffers, we allocate the buffers from that slot and&n;&t;&t;&t; * mark the entries as &squot;0&squot; indicating unavailable.&n;&t;&t;&t; */
r_if
c_cond
(paren
id|io_tlb_list
(braket
id|index
)braket
op_ge
id|nslots
)paren
(brace
r_int
id|count
op_assign
l_int|0
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
id|index
suffix:semicolon
id|i
OL
id|index
op_plus
id|nslots
suffix:semicolon
id|i
op_increment
)paren
id|io_tlb_list
(braket
id|i
)braket
op_assign
l_int|0
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
id|index
op_minus
l_int|1
suffix:semicolon
(paren
id|OFFSET
c_func
(paren
id|i
comma
id|IO_TLB_SEGSIZE
)paren
op_ne
id|IO_TLB_SEGSIZE
op_minus
l_int|1
)paren
op_logical_and
id|io_tlb_list
(braket
id|i
)braket
suffix:semicolon
id|i
op_decrement
)paren
id|io_tlb_list
(braket
id|i
)braket
op_assign
op_increment
id|count
suffix:semicolon
id|dma_addr
op_assign
id|io_tlb_start
op_plus
(paren
id|index
op_lshift
id|IO_TLB_SHIFT
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t;&t;&t; * Update the indices to avoid searching in the next round.&n;&t;&t;&t;&t; */
id|io_tlb_index
op_assign
(paren
(paren
id|index
op_plus
id|nslots
)paren
OL
id|io_tlb_nslabs
ques
c_cond
(paren
id|index
op_plus
id|nslots
)paren
suffix:colon
l_int|0
)paren
suffix:semicolon
r_goto
id|found
suffix:semicolon
)brace
id|index
op_add_assign
id|stride
suffix:semicolon
r_if
c_cond
(paren
id|index
op_ge
id|io_tlb_nslabs
)paren
id|index
op_assign
l_int|0
suffix:semicolon
)brace
r_while
c_loop
(paren
id|index
op_ne
id|wrap
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t; * XXX What is a suitable recovery mechanism here?  We cannot&n;&t;&t; * sleep because we are called from with in interrupts!&n;&t;&t; */
id|panic
c_func
(paren
l_string|&quot;map_single: could not allocate software IO TLB (%ld bytes)&quot;
comma
id|size
)paren
suffix:semicolon
)brace
id|found
suffix:colon
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|io_tlb_lock
comma
id|flags
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Save away the mapping from the original address to the DMA address.  This is&n;&t; * needed when we sync the memory.  Then we sync the buffer if needed.&n;&t; */
id|io_tlb_orig_addr
(braket
id|index
)braket
op_assign
id|buffer
suffix:semicolon
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_TODEVICE
op_logical_or
id|direction
op_eq
id|PCI_DMA_BIDIRECTIONAL
)paren
id|memcpy
c_func
(paren
id|dma_addr
comma
id|buffer
comma
id|size
)paren
suffix:semicolon
r_return
id|dma_addr
suffix:semicolon
)brace
multiline_comment|/*&n; * dma_addr is the kernel virtual address of the bounce buffer to unmap.&n; */
r_static
r_void
DECL|function|unmap_single
id|unmap_single
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
r_char
op_star
id|dma_addr
comma
r_int
id|size
comma
r_int
id|direction
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
r_int
id|i
comma
id|nslots
op_assign
id|ALIGN
c_func
(paren
id|size
comma
l_int|1
op_lshift
id|IO_TLB_SHIFT
)paren
op_rshift
id|IO_TLB_SHIFT
suffix:semicolon
r_int
id|index
op_assign
(paren
id|dma_addr
op_minus
id|io_tlb_start
)paren
op_rshift
id|IO_TLB_SHIFT
suffix:semicolon
r_char
op_star
id|buffer
op_assign
id|io_tlb_orig_addr
(braket
id|index
)braket
suffix:semicolon
multiline_comment|/*&n;&t; * First, sync the memory before unmapping the entry&n;&t; */
r_if
c_cond
(paren
(paren
id|direction
op_eq
id|PCI_DMA_FROMDEVICE
)paren
op_logical_or
(paren
id|direction
op_eq
id|PCI_DMA_BIDIRECTIONAL
)paren
)paren
multiline_comment|/*&n;&t;&t; * bounce... copy the data back into the original buffer * and delete the&n;&t;&t; * bounce buffer.&n;&t;&t; */
id|memcpy
c_func
(paren
id|buffer
comma
id|dma_addr
comma
id|size
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Return the buffer to the free list by setting the corresponding entries to&n;&t; * indicate the number of contigous entries available.  While returning the&n;&t; * entries to the free list, we merge the entries with slots below and above the&n;&t; * pool being returned.&n;&t; */
id|spin_lock_irqsave
c_func
(paren
op_amp
id|io_tlb_lock
comma
id|flags
)paren
suffix:semicolon
(brace
r_int
id|count
op_assign
(paren
(paren
id|index
op_plus
id|nslots
)paren
OL
id|ALIGN
c_func
(paren
id|index
op_plus
l_int|1
comma
id|IO_TLB_SEGSIZE
)paren
ques
c_cond
id|io_tlb_list
(braket
id|index
op_plus
id|nslots
)braket
suffix:colon
l_int|0
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t; * Step 1: return the slots to the free list, merging the slots with&n;&t;&t; * superceeding slots&n;&t;&t; */
r_for
c_loop
(paren
id|i
op_assign
id|index
op_plus
id|nslots
op_minus
l_int|1
suffix:semicolon
id|i
op_ge
id|index
suffix:semicolon
id|i
op_decrement
)paren
id|io_tlb_list
(braket
id|i
)braket
op_assign
op_increment
id|count
suffix:semicolon
multiline_comment|/*&n;&t;&t; * Step 2: merge the returned slots with the preceeding slots, if&n;&t;&t; * available (non zero)&n;&t;&t; */
r_for
c_loop
(paren
id|i
op_assign
id|index
op_minus
l_int|1
suffix:semicolon
(paren
id|OFFSET
c_func
(paren
id|i
comma
id|IO_TLB_SEGSIZE
)paren
op_ne
id|IO_TLB_SEGSIZE
op_minus
l_int|1
)paren
op_logical_and
id|io_tlb_list
(braket
id|i
)braket
suffix:semicolon
id|i
op_decrement
)paren
id|io_tlb_list
(braket
id|i
)braket
op_assign
op_increment
id|count
suffix:semicolon
)brace
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|io_tlb_lock
comma
id|flags
)paren
suffix:semicolon
)brace
r_static
r_void
DECL|function|sync_single
id|sync_single
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
r_char
op_star
id|dma_addr
comma
r_int
id|size
comma
r_int
id|direction
)paren
(brace
r_int
id|index
op_assign
(paren
id|dma_addr
op_minus
id|io_tlb_start
)paren
op_rshift
id|IO_TLB_SHIFT
suffix:semicolon
r_char
op_star
id|buffer
op_assign
id|io_tlb_orig_addr
(braket
id|index
)braket
suffix:semicolon
multiline_comment|/*&n;&t; * bounce... copy the data back into/from the original buffer&n;&t; * XXX How do you handle PCI_DMA_BIDIRECTIONAL here ?&n;&t; */
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_FROMDEVICE
)paren
id|memcpy
c_func
(paren
id|buffer
comma
id|dma_addr
comma
id|size
)paren
suffix:semicolon
r_else
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_TODEVICE
)paren
id|memcpy
c_func
(paren
id|dma_addr
comma
id|buffer
comma
id|size
)paren
suffix:semicolon
r_else
id|BUG
c_func
(paren
)paren
suffix:semicolon
)brace
r_void
op_star
DECL|function|swiotlb_alloc_consistent
id|swiotlb_alloc_consistent
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
r_int
id|size
comma
id|dma_addr_t
op_star
id|dma_handle
)paren
(brace
r_int
r_int
id|pci_addr
suffix:semicolon
r_int
id|gfp
op_assign
id|GFP_ATOMIC
suffix:semicolon
r_void
op_star
id|ret
suffix:semicolon
multiline_comment|/*&n;&t; * Alloc_consistent() is defined to return memory &lt; 4GB, no matter what the DMA&n;&t; * mask says.&n;&t; */
id|gfp
op_or_assign
id|GFP_DMA
suffix:semicolon
multiline_comment|/* XXX fix me: should change this to GFP_32BIT or ZONE_32BIT */
id|ret
op_assign
(paren
r_void
op_star
)paren
id|__get_free_pages
c_func
(paren
id|gfp
comma
id|get_order
c_func
(paren
id|size
)paren
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|ret
)paren
r_return
l_int|NULL
suffix:semicolon
id|memset
c_func
(paren
id|ret
comma
l_int|0
comma
id|size
)paren
suffix:semicolon
id|pci_addr
op_assign
id|virt_to_phys
c_func
(paren
id|ret
)paren
suffix:semicolon
r_if
c_cond
(paren
id|hwdev
op_logical_and
(paren
id|pci_addr
op_amp
op_complement
id|hwdev-&gt;dma_mask
)paren
op_ne
l_int|0
)paren
id|panic
c_func
(paren
l_string|&quot;swiotlb_alloc_consistent: allocated memory is out of range for PCI device&quot;
)paren
suffix:semicolon
op_star
id|dma_handle
op_assign
id|pci_addr
suffix:semicolon
r_return
id|ret
suffix:semicolon
)brace
r_void
DECL|function|swiotlb_free_consistent
id|swiotlb_free_consistent
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
r_int
id|size
comma
r_void
op_star
id|vaddr
comma
id|dma_addr_t
id|dma_handle
)paren
(brace
id|free_pages
c_func
(paren
(paren
r_int
r_int
)paren
id|vaddr
comma
id|get_order
c_func
(paren
id|size
)paren
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Map a single buffer of the indicated size for DMA in streaming mode.  The PCI address&n; * to use is returned.&n; *&n; * Once the device is given the dma address, the device owns this memory until either&n; * swiotlb_unmap_single or swiotlb_dma_sync_single is performed.&n; */
id|dma_addr_t
DECL|function|swiotlb_map_single
id|swiotlb_map_single
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
r_void
op_star
id|ptr
comma
r_int
id|size
comma
r_int
id|direction
)paren
(brace
r_int
r_int
id|pci_addr
op_assign
id|virt_to_phys
c_func
(paren
id|ptr
)paren
suffix:semicolon
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_NONE
)paren
id|BUG
c_func
(paren
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Check if the PCI device can DMA to ptr... if so, just return ptr&n;&t; */
r_if
c_cond
(paren
(paren
id|pci_addr
op_amp
op_complement
id|hwdev-&gt;dma_mask
)paren
op_eq
l_int|0
)paren
multiline_comment|/*&n;&t;&t; * Device is bit capable of DMA&squot;ing to the buffer... just return the PCI&n;&t;&t; * address of ptr&n;&t;&t; */
r_return
id|pci_addr
suffix:semicolon
multiline_comment|/*&n;&t; * get a bounce buffer:&n;&t; */
id|pci_addr
op_assign
id|virt_to_phys
c_func
(paren
id|map_single
c_func
(paren
id|hwdev
comma
id|ptr
comma
id|size
comma
id|direction
)paren
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Ensure that the address returned is DMA&squot;ble:&n;&t; */
r_if
c_cond
(paren
(paren
id|pci_addr
op_amp
op_complement
id|hwdev-&gt;dma_mask
)paren
op_ne
l_int|0
)paren
id|panic
c_func
(paren
l_string|&quot;map_single: bounce buffer is not DMA&squot;ble&quot;
)paren
suffix:semicolon
r_return
id|pci_addr
suffix:semicolon
)brace
multiline_comment|/*&n; * Since DMA is i-cache coherent, any (complete) pages that were written via&n; * DMA can be marked as &quot;clean&quot; so that update_mmu_cache() doesn&squot;t have to&n; * flush them when they get mapped into an executable vm-area.&n; */
r_static
r_void
DECL|function|mark_clean
id|mark_clean
(paren
r_void
op_star
id|addr
comma
r_int
id|size
)paren
(brace
r_int
r_int
id|pg_addr
comma
id|end
suffix:semicolon
id|pg_addr
op_assign
id|PAGE_ALIGN
c_func
(paren
(paren
r_int
r_int
)paren
id|addr
)paren
suffix:semicolon
id|end
op_assign
(paren
r_int
r_int
)paren
id|addr
op_plus
id|size
suffix:semicolon
r_while
c_loop
(paren
id|pg_addr
op_plus
id|PAGE_SIZE
op_le
id|end
)paren
(brace
r_struct
id|page
op_star
id|page
op_assign
id|virt_to_page
c_func
(paren
id|pg_addr
)paren
suffix:semicolon
id|set_bit
c_func
(paren
id|PG_arch_1
comma
op_amp
id|page-&gt;flags
)paren
suffix:semicolon
id|pg_addr
op_add_assign
id|PAGE_SIZE
suffix:semicolon
)brace
)brace
multiline_comment|/*&n; * Unmap a single streaming mode DMA translation.  The dma_addr and size must match what&n; * was provided for in a previous swiotlb_map_single call.  All other usages are&n; * undefined.&n; *&n; * After this call, reads by the cpu to the buffer are guarenteed to see whatever the&n; * device wrote there.&n; */
r_void
DECL|function|swiotlb_unmap_single
id|swiotlb_unmap_single
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
id|dma_addr_t
id|pci_addr
comma
r_int
id|size
comma
r_int
id|direction
)paren
(brace
r_char
op_star
id|dma_addr
op_assign
id|phys_to_virt
c_func
(paren
id|pci_addr
)paren
suffix:semicolon
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_NONE
)paren
id|BUG
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|dma_addr
op_ge
id|io_tlb_start
op_logical_and
id|dma_addr
OL
id|io_tlb_end
)paren
id|unmap_single
c_func
(paren
id|hwdev
comma
id|dma_addr
comma
id|size
comma
id|direction
)paren
suffix:semicolon
r_else
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_FROMDEVICE
)paren
id|mark_clean
c_func
(paren
id|dma_addr
comma
id|size
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Make physical memory consistent for a single streaming mode DMA translation after a&n; * transfer.&n; *&n; * If you perform a swiotlb_map_single() but wish to interrogate the buffer using the cpu,&n; * yet do not wish to teardown the PCI dma mapping, you must call this function before&n; * doing so.  At the next point you give the PCI dma address back to the card, the device&n; * again owns the buffer.&n; */
r_void
DECL|function|swiotlb_sync_single
id|swiotlb_sync_single
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
id|dma_addr_t
id|pci_addr
comma
r_int
id|size
comma
r_int
id|direction
)paren
(brace
r_char
op_star
id|dma_addr
op_assign
id|phys_to_virt
c_func
(paren
id|pci_addr
)paren
suffix:semicolon
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_NONE
)paren
id|BUG
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|dma_addr
op_ge
id|io_tlb_start
op_logical_and
id|dma_addr
OL
id|io_tlb_end
)paren
id|sync_single
c_func
(paren
id|hwdev
comma
id|dma_addr
comma
id|size
comma
id|direction
)paren
suffix:semicolon
r_else
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_FROMDEVICE
)paren
id|mark_clean
c_func
(paren
id|dma_addr
comma
id|size
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Map a set of buffers described by scatterlist in streaming mode for DMA.  This is the&n; * scather-gather version of the above swiotlb_map_single interface.  Here the scatter&n; * gather list elements are each tagged with the appropriate dma address and length.  They&n; * are obtained via sg_dma_{address,length}(SG).&n; *&n; * NOTE: An implementation may be able to use a smaller number of&n; *       DMA address/length pairs than there are SG table elements.&n; *       (for example via virtual mapping capabilities)&n; *       The routine returns the number of addr/length pairs actually&n; *       used, at most nents.&n; *&n; * Device ownership issues as mentioned above for swiotlb_map_single are the same here.&n; */
r_int
DECL|function|swiotlb_map_sg
id|swiotlb_map_sg
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
r_struct
id|scatterlist
op_star
id|sg
comma
r_int
id|nelems
comma
r_int
id|direction
)paren
(brace
r_void
op_star
id|addr
suffix:semicolon
r_int
r_int
id|pci_addr
suffix:semicolon
r_int
id|i
suffix:semicolon
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_NONE
)paren
id|BUG
c_func
(paren
)paren
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
id|nelems
suffix:semicolon
id|i
op_increment
comma
id|sg
op_increment
)paren
(brace
id|addr
op_assign
id|SG_ENT_VIRT_ADDRESS
c_func
(paren
id|sg
)paren
suffix:semicolon
id|pci_addr
op_assign
id|virt_to_phys
c_func
(paren
id|addr
)paren
suffix:semicolon
r_if
c_cond
(paren
(paren
id|pci_addr
op_amp
op_complement
id|hwdev-&gt;dma_mask
)paren
op_ne
l_int|0
)paren
id|sg-&gt;dma_address
op_assign
(paren
id|dma_addr_t
)paren
id|map_single
c_func
(paren
id|hwdev
comma
id|addr
comma
id|sg-&gt;length
comma
id|direction
)paren
suffix:semicolon
r_else
id|sg-&gt;dma_address
op_assign
id|pci_addr
suffix:semicolon
id|sg-&gt;dma_length
op_assign
id|sg-&gt;length
suffix:semicolon
)brace
r_return
id|nelems
suffix:semicolon
)brace
multiline_comment|/*&n; * Unmap a set of streaming mode DMA translations.  Again, cpu read rules concerning calls&n; * here are the same as for swiotlb_unmap_single() above.&n; */
r_void
DECL|function|swiotlb_unmap_sg
id|swiotlb_unmap_sg
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
r_struct
id|scatterlist
op_star
id|sg
comma
r_int
id|nelems
comma
r_int
id|direction
)paren
(brace
r_int
id|i
suffix:semicolon
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_NONE
)paren
id|BUG
c_func
(paren
)paren
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
id|nelems
suffix:semicolon
id|i
op_increment
comma
id|sg
op_increment
)paren
r_if
c_cond
(paren
id|sg-&gt;dma_address
op_ne
id|SG_ENT_PHYS_ADDRESS
c_func
(paren
id|sg
)paren
)paren
id|unmap_single
c_func
(paren
id|hwdev
comma
(paren
r_void
op_star
)paren
id|sg-&gt;dma_address
comma
id|sg-&gt;dma_length
comma
id|direction
)paren
suffix:semicolon
r_else
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_FROMDEVICE
)paren
id|mark_clean
c_func
(paren
id|SG_ENT_VIRT_ADDRESS
c_func
(paren
id|sg
)paren
comma
id|sg-&gt;dma_length
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Make physical memory consistent for a set of streaming mode DMA translations after a&n; * transfer.&n; *&n; * The same as swiotlb_dma_sync_single but for a scatter-gather list, same rules and&n; * usage.&n; */
r_void
DECL|function|swiotlb_sync_sg
id|swiotlb_sync_sg
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
r_struct
id|scatterlist
op_star
id|sg
comma
r_int
id|nelems
comma
r_int
id|direction
)paren
(brace
r_int
id|i
suffix:semicolon
r_if
c_cond
(paren
id|direction
op_eq
id|PCI_DMA_NONE
)paren
id|BUG
c_func
(paren
)paren
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
id|nelems
suffix:semicolon
id|i
op_increment
comma
id|sg
op_increment
)paren
r_if
c_cond
(paren
id|sg-&gt;dma_address
op_ne
id|SG_ENT_PHYS_ADDRESS
c_func
(paren
id|sg
)paren
)paren
id|sync_single
c_func
(paren
id|hwdev
comma
(paren
r_void
op_star
)paren
id|sg-&gt;dma_address
comma
id|sg-&gt;dma_length
comma
id|direction
)paren
suffix:semicolon
)brace
r_int
r_int
DECL|function|swiotlb_dma_address
id|swiotlb_dma_address
(paren
r_struct
id|scatterlist
op_star
id|sg
)paren
(brace
r_return
id|sg-&gt;dma_address
suffix:semicolon
)brace
multiline_comment|/*&n; * Return whether the given PCI device DMA address mask can be supported properly.  For&n; * example, if your device can only drive the low 24-bits during PCI bus mastering, then&n; * you would pass 0x00ffffff as the mask to this function.&n; */
r_int
DECL|function|swiotlb_pci_dma_supported
id|swiotlb_pci_dma_supported
(paren
r_struct
id|pci_dev
op_star
id|hwdev
comma
id|u64
id|mask
)paren
(brace
r_return
l_int|1
suffix:semicolon
)brace
DECL|variable|swiotlb_init
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_init
)paren
suffix:semicolon
DECL|variable|swiotlb_map_single
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_map_single
)paren
suffix:semicolon
DECL|variable|swiotlb_unmap_single
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_unmap_single
)paren
suffix:semicolon
DECL|variable|swiotlb_map_sg
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_map_sg
)paren
suffix:semicolon
DECL|variable|swiotlb_unmap_sg
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_unmap_sg
)paren
suffix:semicolon
DECL|variable|swiotlb_sync_single
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_sync_single
)paren
suffix:semicolon
DECL|variable|swiotlb_sync_sg
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_sync_sg
)paren
suffix:semicolon
DECL|variable|swiotlb_dma_address
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_dma_address
)paren
suffix:semicolon
DECL|variable|swiotlb_alloc_consistent
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_alloc_consistent
)paren
suffix:semicolon
DECL|variable|swiotlb_free_consistent
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_free_consistent
)paren
suffix:semicolon
DECL|variable|swiotlb_pci_dma_supported
id|EXPORT_SYMBOL
c_func
(paren
id|swiotlb_pci_dma_supported
)paren
suffix:semicolon
eof
