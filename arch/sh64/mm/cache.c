multiline_comment|/*&n; * This file is subject to the terms and conditions of the GNU General Public&n; * License.  See the file &quot;COPYING&quot; in the main directory of this archive&n; * for more details.&n; *&n; * arch/sh64/mm/cache.c&n; *&n; * Original version Copyright (C) 2000, 2001  Paolo Alberelli&n; * Second version Copyright (C) benedict.gaster@superh.com 2002&n; * Third version Copyright Richard.Curnow@superh.com 2003&n; * Hacks to third version Copyright (C) 2003 Paul Mundt&n; */
multiline_comment|/****************************************************************************/
macro_line|#include &lt;linux/config.h&gt;
macro_line|#include &lt;linux/init.h&gt;
macro_line|#include &lt;linux/mman.h&gt;
macro_line|#include &lt;linux/mm.h&gt;
macro_line|#include &lt;linux/threads.h&gt;
macro_line|#include &lt;asm/page.h&gt;
macro_line|#include &lt;asm/pgtable.h&gt;
macro_line|#include &lt;asm/processor.h&gt;
macro_line|#include &lt;asm/cache.h&gt;
macro_line|#include &lt;asm/tlb.h&gt;
macro_line|#include &lt;asm/io.h&gt;
macro_line|#include &lt;asm/uaccess.h&gt;
macro_line|#include &lt;asm/mmu_context.h&gt;
macro_line|#include &lt;asm/pgalloc.h&gt; /* for flush_itlb_range */
macro_line|#include &lt;linux/proc_fs.h&gt;
multiline_comment|/* This function is in entry.S */
r_extern
r_int
r_int
id|switch_and_save_asid
c_func
(paren
r_int
r_int
id|new_asid
)paren
suffix:semicolon
multiline_comment|/* Wired TLB entry for the D-cache */
DECL|variable|dtlb_cache_slot
r_static
r_int
r_int
r_int
id|dtlb_cache_slot
suffix:semicolon
multiline_comment|/**&n; * sh64_cache_init()&n; *&n; * This is pretty much just a straightforward clone of the SH&n; * detect_cpu_and_cache_system().&n; *&n; * This function is responsible for setting up all of the cache&n; * info dynamically as well as taking care of CPU probing and&n; * setting up the relevant subtype data.&n; *&n; * FIXME: For the time being, we only really support the SH5-101&n; * out of the box, and don&squot;t support dynamic probing for things&n; * like the SH5-103 or even cut2 of the SH5-101. Implement this&n; * later!&n; */
DECL|function|sh64_cache_init
r_int
id|__init
id|sh64_cache_init
c_func
(paren
r_void
)paren
(brace
multiline_comment|/*&n;&t; * First, setup some sane values for the I-cache.&n;&t; */
id|cpu_data-&gt;icache.ways
op_assign
l_int|4
suffix:semicolon
id|cpu_data-&gt;icache.sets
op_assign
l_int|256
suffix:semicolon
id|cpu_data-&gt;icache.linesz
op_assign
id|L1_CACHE_BYTES
suffix:semicolon
multiline_comment|/*&n;&t; * FIXME: This can probably be cleaned up a bit as well.. for example,&n;&t; * do we really need the way shift _and_ the way_step_shift ?? Judging&n;&t; * by the existing code, I would guess no.. is there any valid reason&n;&t; * why we need to be tracking this around?&n;&t; */
id|cpu_data-&gt;icache.way_shift
op_assign
l_int|13
suffix:semicolon
id|cpu_data-&gt;icache.entry_shift
op_assign
l_int|5
suffix:semicolon
id|cpu_data-&gt;icache.set_shift
op_assign
l_int|4
suffix:semicolon
id|cpu_data-&gt;icache.way_step_shift
op_assign
l_int|16
suffix:semicolon
id|cpu_data-&gt;icache.asid_shift
op_assign
l_int|2
suffix:semicolon
multiline_comment|/*&n;&t; * way offset = cache size / associativity, so just don&squot;t factor in&n;&t; * associativity in the first place..&n;&t; */
id|cpu_data-&gt;icache.way_ofs
op_assign
id|cpu_data-&gt;icache.sets
op_star
id|cpu_data-&gt;icache.linesz
suffix:semicolon
id|cpu_data-&gt;icache.asid_mask
op_assign
l_int|0x3fc
suffix:semicolon
id|cpu_data-&gt;icache.idx_mask
op_assign
l_int|0x1fe0
suffix:semicolon
id|cpu_data-&gt;icache.epn_mask
op_assign
l_int|0xffffe000
suffix:semicolon
id|cpu_data-&gt;icache.flags
op_assign
l_int|0
suffix:semicolon
multiline_comment|/*&n;&t; * Next, setup some sane values for the D-cache.&n;&t; *&n;&t; * On the SH5, these are pretty consistent with the I-cache settings,&n;&t; * so we just copy over the existing definitions.. these can be fixed&n;&t; * up later, especially if we add runtime CPU probing.&n;&t; *&n;&t; * Though in the meantime it saves us from having to duplicate all of&n;&t; * the above definitions..&n;&t; */
id|cpu_data-&gt;dcache
op_assign
id|cpu_data-&gt;icache
suffix:semicolon
multiline_comment|/*&n;&t; * Setup any cache-related flags here&n;&t; */
macro_line|#if defined(CONFIG_DCACHE_WRITE_THROUGH)
id|set_bit
c_func
(paren
id|SH_CACHE_MODE_WT
comma
op_amp
(paren
id|cpu_data-&gt;dcache.flags
)paren
)paren
suffix:semicolon
macro_line|#elif defined(CONFIG_DCACHE_WRITE_BACK)
id|set_bit
c_func
(paren
id|SH_CACHE_MODE_WB
comma
op_amp
(paren
id|cpu_data-&gt;dcache.flags
)paren
)paren
suffix:semicolon
macro_line|#endif
multiline_comment|/*&n;&t; * We also need to reserve a slot for the D-cache in the DTLB, so we&n;&t; * do this now ..&n;&t; */
id|dtlb_cache_slot
op_assign
id|sh64_get_wired_dtlb_entry
c_func
(paren
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_DCACHE_DISABLED
DECL|macro|sh64_dcache_purge_all
mdefine_line|#define sh64_dcache_purge_all()&t;&t;&t;&t;&t;do { } while (0)
DECL|macro|sh64_dcache_purge_coloured_phy_page
mdefine_line|#define sh64_dcache_purge_coloured_phy_page(paddr, eaddr)&t;do { } while (0)
DECL|macro|sh64_dcache_purge_user_range
mdefine_line|#define sh64_dcache_purge_user_range(mm, start, end)&t;&t;do { } while (0)
DECL|macro|sh64_dcache_purge_phy_page
mdefine_line|#define sh64_dcache_purge_phy_page(paddr)&t;&t;&t;do { } while (0)
DECL|macro|sh64_dcache_purge_virt_page
mdefine_line|#define sh64_dcache_purge_virt_page(mm, eaddr)&t;&t;&t;do { } while (0)
DECL|macro|sh64_dcache_purge_kernel_range
mdefine_line|#define sh64_dcache_purge_kernel_range(start, end)&t;&t;do { } while (0)
DECL|macro|sh64_dcache_wback_current_user_range
mdefine_line|#define sh64_dcache_wback_current_user_range(start, end)&t;do { } while (0)
macro_line|#endif
multiline_comment|/*##########################################################################*/
multiline_comment|/* From here onwards, a rewrite of the implementation,&n;   by Richard.Curnow@superh.com.&n;&n;   The major changes in this compared to the old version are;&n;   1. use more selective purging through OCBP instead of using ALLOCO to purge&n;      by natural replacement.  This avoids purging out unrelated cache lines&n;      that happen to be in the same set.&n;   2. exploit the APIs copy_user_page and clear_user_page better&n;   3. be more selective about I-cache purging, in particular use invalidate_all&n;      more sparingly.&n;&n;   */
multiline_comment|/*##########################################################################&n;&t;&t;&t;       SUPPORT FUNCTIONS&n;  ##########################################################################*/
multiline_comment|/****************************************************************************/
multiline_comment|/* The following group of functions deal with mapping and unmapping a temporary&n;   page into the DTLB slot that have been set aside for our exclusive use. */
multiline_comment|/* In order to accomplish this, we use the generic interface for adding and&n;   removing a wired slot entry as defined in arch/sh64/mm/tlb.c */
multiline_comment|/****************************************************************************/
DECL|variable|slot_own_flags
r_static
r_int
r_int
id|slot_own_flags
suffix:semicolon
DECL|function|sh64_setup_dtlb_cache_slot
r_static
r_inline
r_void
id|sh64_setup_dtlb_cache_slot
c_func
(paren
r_int
r_int
id|eaddr
comma
r_int
r_int
id|asid
comma
r_int
r_int
id|paddr
)paren
(brace
id|local_irq_save
c_func
(paren
id|slot_own_flags
)paren
suffix:semicolon
id|sh64_setup_tlb_slot
c_func
(paren
id|dtlb_cache_slot
comma
id|eaddr
comma
id|asid
comma
id|paddr
)paren
suffix:semicolon
)brace
DECL|function|sh64_teardown_dtlb_cache_slot
r_static
r_inline
r_void
id|sh64_teardown_dtlb_cache_slot
c_func
(paren
r_void
)paren
(brace
id|sh64_teardown_tlb_slot
c_func
(paren
id|dtlb_cache_slot
)paren
suffix:semicolon
id|local_irq_restore
c_func
(paren
id|slot_own_flags
)paren
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
macro_line|#ifndef CONFIG_ICACHE_DISABLED
DECL|function|sh64_icache_inv_all
r_static
r_void
id|__inline__
id|sh64_icache_inv_all
c_func
(paren
r_void
)paren
(brace
r_int
r_int
r_int
id|addr
comma
id|flag
comma
id|data
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
id|addr
op_assign
id|ICCR0
suffix:semicolon
id|flag
op_assign
id|ICCR0_ICI
suffix:semicolon
id|data
op_assign
l_int|0
suffix:semicolon
multiline_comment|/* Make this a critical section for safety (probably not strictly necessary.) */
id|local_irq_save
c_func
(paren
id|flags
)paren
suffix:semicolon
multiline_comment|/* Without %1 it gets unexplicably wrong */
id|asm
r_volatile
(paren
l_string|&quot;getcfg&t;%3, 0, %0&bslash;n&bslash;t&quot;
l_string|&quot;or&t;%0, %2, %0&bslash;n&bslash;t&quot;
l_string|&quot;putcfg&t;%3, 0, %0&bslash;n&bslash;t&quot;
l_string|&quot;synci&quot;
suffix:colon
l_string|&quot;=&amp;r&quot;
(paren
id|data
)paren
suffix:colon
l_string|&quot;0&quot;
(paren
id|data
)paren
comma
l_string|&quot;r&quot;
(paren
id|flag
)paren
comma
l_string|&quot;r&quot;
(paren
id|addr
)paren
)paren
suffix:semicolon
id|local_irq_restore
c_func
(paren
id|flags
)paren
suffix:semicolon
)brace
DECL|function|sh64_icache_inv_kernel_range
r_static
r_void
id|sh64_icache_inv_kernel_range
c_func
(paren
r_int
r_int
id|start
comma
r_int
r_int
id|end
)paren
(brace
multiline_comment|/* Invalidate range of addresses [start,end] from the I-cache, where&n;&t; * the addresses lie in the kernel superpage. */
r_int
r_int
r_int
id|ullend
comma
id|addr
comma
id|aligned_start
suffix:semicolon
macro_line|#if (NEFF == 32)
id|aligned_start
op_assign
(paren
r_int
r_int
r_int
)paren
(paren
r_int
r_int
r_int
)paren
(paren
r_int
r_int
)paren
id|start
suffix:semicolon
macro_line|#else
macro_line|#error &quot;NEFF != 32&quot;
macro_line|#endif
id|aligned_start
op_and_assign
id|L1_CACHE_ALIGN_MASK
suffix:semicolon
id|addr
op_assign
id|aligned_start
suffix:semicolon
macro_line|#if (NEFF == 32)
id|ullend
op_assign
(paren
r_int
r_int
r_int
)paren
(paren
r_int
r_int
r_int
)paren
(paren
r_int
r_int
)paren
id|end
suffix:semicolon
macro_line|#else
macro_line|#error &quot;NEFF != 32&quot;
macro_line|#endif
r_while
c_loop
(paren
id|addr
op_le
id|ullend
)paren
(brace
id|asm
id|__volatile__
(paren
l_string|&quot;icbi %0, 0&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|addr
)paren
)paren
suffix:semicolon
id|addr
op_add_assign
id|L1_CACHE_BYTES
suffix:semicolon
)brace
)brace
DECL|function|sh64_icache_inv_user_page
r_static
r_void
id|sh64_icache_inv_user_page
c_func
(paren
r_struct
id|vm_area_struct
op_star
id|vma
comma
r_int
r_int
id|eaddr
)paren
(brace
multiline_comment|/* If we get called, we know that vma-&gt;vm_flags contains VM_EXEC.&n;&t;   Also, eaddr is page-aligned. */
r_int
r_int
r_int
id|addr
comma
id|end_addr
suffix:semicolon
r_int
r_int
id|flags
op_assign
l_int|0
suffix:semicolon
r_int
r_int
id|running_asid
comma
id|vma_asid
suffix:semicolon
id|addr
op_assign
id|eaddr
suffix:semicolon
id|end_addr
op_assign
id|addr
op_plus
id|PAGE_SIZE
suffix:semicolon
multiline_comment|/* Check whether we can use the current ASID for the I-cache&n;&t;   invalidation.  For example, if we&squot;re called via&n;&t;   access_process_vm-&gt;flush_cache_page-&gt;here, (e.g. when reading from&n;&t;   /proc), &squot;running_asid&squot; will be that of the reader, not of the&n;&t;   victim.&n;&n;&t;   Also, note the risk that we might get pre-empted between the ASID&n;&t;   compare and blocking IRQs, and before we regain control, the&n;&t;   pid-&gt;ASID mapping changes.  However, the whole cache will get&n;&t;   invalidated when the mapping is renewed, so the worst that can&n;&t;   happen is that the loop below ends up invalidating somebody else&squot;s&n;&t;   cache entries.&n;&t;*/
id|running_asid
op_assign
id|get_asid
c_func
(paren
)paren
suffix:semicolon
id|vma_asid
op_assign
(paren
id|vma-&gt;vm_mm-&gt;context
op_amp
id|MMU_CONTEXT_ASID_MASK
)paren
suffix:semicolon
r_if
c_cond
(paren
id|running_asid
op_ne
id|vma_asid
)paren
(brace
id|local_irq_save
c_func
(paren
id|flags
)paren
suffix:semicolon
id|switch_and_save_asid
c_func
(paren
id|vma_asid
)paren
suffix:semicolon
)brace
r_while
c_loop
(paren
id|addr
OL
id|end_addr
)paren
(brace
multiline_comment|/* Worth unrolling a little */
id|asm
id|__volatile__
c_func
(paren
l_string|&quot;icbi %0,  0&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|addr
)paren
)paren
suffix:semicolon
id|asm
id|__volatile__
c_func
(paren
l_string|&quot;icbi %0, 32&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|addr
)paren
)paren
suffix:semicolon
id|asm
id|__volatile__
c_func
(paren
l_string|&quot;icbi %0, 64&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|addr
)paren
)paren
suffix:semicolon
id|asm
id|__volatile__
c_func
(paren
l_string|&quot;icbi %0, 96&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|addr
)paren
)paren
suffix:semicolon
id|addr
op_add_assign
l_int|128
suffix:semicolon
)brace
r_if
c_cond
(paren
id|running_asid
op_ne
id|vma_asid
)paren
(brace
id|switch_and_save_asid
c_func
(paren
id|running_asid
)paren
suffix:semicolon
id|local_irq_restore
c_func
(paren
id|flags
)paren
suffix:semicolon
)brace
)brace
multiline_comment|/****************************************************************************/
DECL|function|sh64_icache_inv_user_page_range
r_static
r_void
id|sh64_icache_inv_user_page_range
c_func
(paren
r_struct
id|mm_struct
op_star
id|mm
comma
r_int
r_int
id|start
comma
r_int
r_int
id|end
)paren
(brace
multiline_comment|/* Used for invalidating big chunks of I-cache, i.e. assume the range&n;&t;   is whole pages.  If &squot;start&squot; or &squot;end&squot; is not page aligned, the code&n;&t;   is conservative and invalidates to the ends of the enclosing pages.&n;&t;   This is functionally OK, just a performance loss. */
multiline_comment|/* See the comments below in sh64_dcache_purge_user_range() regarding&n;&t;   the choice of algorithm.  However, for the I-cache option (2) isn&squot;t&n;&t;   available because there are no physical tags so aliases can&squot;t be&n;&t;   resolved.  The icbi instruction has to be used through the user&n;&t;   mapping.   Because icbi is cheaper than ocbp on a cache hit, it&n;&t;   would be cheaper to use the selective code for a large range than is&n;&t;   possible with the D-cache.  Just assume 64 for now as a working&n;&t;   figure.&n;&t;   */
r_int
id|n_pages
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|mm
)paren
r_return
suffix:semicolon
id|n_pages
op_assign
(paren
(paren
id|end
op_minus
id|start
)paren
op_rshift
id|PAGE_SHIFT
)paren
suffix:semicolon
r_if
c_cond
(paren
id|n_pages
op_ge
l_int|64
)paren
(brace
id|sh64_icache_inv_all
c_func
(paren
)paren
suffix:semicolon
)brace
r_else
(brace
r_int
r_int
id|aligned_start
suffix:semicolon
r_int
r_int
id|eaddr
suffix:semicolon
r_int
r_int
id|after_last_page_start
suffix:semicolon
r_int
r_int
id|mm_asid
comma
id|current_asid
suffix:semicolon
r_int
r_int
r_int
id|flags
op_assign
l_int|0ULL
suffix:semicolon
id|mm_asid
op_assign
id|mm-&gt;context
op_amp
id|MMU_CONTEXT_ASID_MASK
suffix:semicolon
id|current_asid
op_assign
id|get_asid
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|mm_asid
op_ne
id|current_asid
)paren
(brace
multiline_comment|/* Switch ASID and run the invalidate loop under cli */
id|local_irq_save
c_func
(paren
id|flags
)paren
suffix:semicolon
id|switch_and_save_asid
c_func
(paren
id|mm_asid
)paren
suffix:semicolon
)brace
id|aligned_start
op_assign
id|start
op_amp
id|PAGE_MASK
suffix:semicolon
id|after_last_page_start
op_assign
id|PAGE_SIZE
op_plus
(paren
(paren
id|end
op_minus
l_int|1
)paren
op_amp
id|PAGE_MASK
)paren
suffix:semicolon
r_while
c_loop
(paren
id|aligned_start
OL
id|after_last_page_start
)paren
(brace
r_struct
id|vm_area_struct
op_star
id|vma
suffix:semicolon
r_int
r_int
id|vma_end
suffix:semicolon
id|vma
op_assign
id|find_vma
c_func
(paren
id|mm
comma
id|aligned_start
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|vma
op_logical_or
(paren
id|aligned_start
op_le
id|vma-&gt;vm_end
)paren
)paren
(brace
multiline_comment|/* Avoid getting stuck in an error condition */
id|aligned_start
op_add_assign
id|PAGE_SIZE
suffix:semicolon
r_continue
suffix:semicolon
)brace
id|vma_end
op_assign
id|vma-&gt;vm_end
suffix:semicolon
r_if
c_cond
(paren
id|vma-&gt;vm_flags
op_amp
id|VM_EXEC
)paren
(brace
multiline_comment|/* Executable */
id|eaddr
op_assign
id|aligned_start
suffix:semicolon
r_while
c_loop
(paren
id|eaddr
OL
id|vma_end
)paren
(brace
id|sh64_icache_inv_user_page
c_func
(paren
id|vma
comma
id|eaddr
)paren
suffix:semicolon
id|eaddr
op_add_assign
id|PAGE_SIZE
suffix:semicolon
)brace
)brace
id|aligned_start
op_assign
id|vma-&gt;vm_end
suffix:semicolon
multiline_comment|/* Skip to start of next region */
)brace
r_if
c_cond
(paren
id|mm_asid
op_ne
id|current_asid
)paren
(brace
id|switch_and_save_asid
c_func
(paren
id|current_asid
)paren
suffix:semicolon
id|local_irq_restore
c_func
(paren
id|flags
)paren
suffix:semicolon
)brace
)brace
)brace
DECL|function|sh64_icache_inv_user_small_range
r_static
r_void
id|sh64_icache_inv_user_small_range
c_func
(paren
r_struct
id|mm_struct
op_star
id|mm
comma
r_int
r_int
id|start
comma
r_int
id|len
)paren
(brace
multiline_comment|/* Invalidate a small range of user context I-cache, not necessarily&n;&t;   page (or even cache-line) aligned. */
r_int
r_int
r_int
id|eaddr
op_assign
id|start
suffix:semicolon
r_int
r_int
r_int
id|eaddr_end
op_assign
id|start
op_plus
id|len
suffix:semicolon
r_int
r_int
id|current_asid
comma
id|mm_asid
suffix:semicolon
r_int
r_int
r_int
id|flags
suffix:semicolon
r_int
r_int
r_int
id|epage_start
suffix:semicolon
multiline_comment|/* Since this is used inside ptrace, the ASID in the mm context&n;&t;   typically won&squot;t match current_asid.  We&squot;ll have to switch ASID to do&n;&t;   this.  For safety, and given that the range will be small, do all&n;&t;   this under cli.&n;&n;&t;   Note, there is a hazard that the ASID in mm-&gt;context is no longer&n;&t;   actually associated with mm, i.e. if the mm-&gt;context has started a&n;&t;   new cycle since mm was last active.  However, this is just a&n;&t;   performance issue: all that happens is that we invalidate lines&n;&t;   belonging to another mm, so the owning process has to refill them&n;&t;   when that mm goes live again.  mm itself can&squot;t have any cache&n;&t;   entries because there will have been a flush_cache_all when the new&n;&t;   mm-&gt;context cycle started. */
multiline_comment|/* Align to start of cache line.  Otherwise, suppose len==8 and start&n;&t;   was at 32N+28 : the last 4 bytes wouldn&squot;t get invalidated. */
id|eaddr
op_assign
id|start
op_amp
id|L1_CACHE_ALIGN_MASK
suffix:semicolon
id|eaddr_end
op_assign
id|start
op_plus
id|len
suffix:semicolon
id|local_irq_save
c_func
(paren
id|flags
)paren
suffix:semicolon
id|mm_asid
op_assign
id|mm-&gt;context
op_amp
id|MMU_CONTEXT_ASID_MASK
suffix:semicolon
id|current_asid
op_assign
id|switch_and_save_asid
c_func
(paren
id|mm_asid
)paren
suffix:semicolon
id|epage_start
op_assign
id|eaddr
op_amp
id|PAGE_MASK
suffix:semicolon
r_while
c_loop
(paren
id|eaddr
OL
id|eaddr_end
)paren
(brace
id|asm
id|__volatile__
c_func
(paren
l_string|&quot;icbi %0, 0&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|eaddr
)paren
)paren
suffix:semicolon
id|eaddr
op_add_assign
id|L1_CACHE_BYTES
suffix:semicolon
)brace
id|switch_and_save_asid
c_func
(paren
id|current_asid
)paren
suffix:semicolon
id|local_irq_restore
c_func
(paren
id|flags
)paren
suffix:semicolon
)brace
DECL|function|sh64_icache_inv_current_user_range
r_static
r_void
id|sh64_icache_inv_current_user_range
c_func
(paren
r_int
r_int
id|start
comma
r_int
r_int
id|end
)paren
(brace
multiline_comment|/* The icbi instruction never raises ITLBMISS.  i.e. if there&squot;s not a&n;&t;   cache hit on the virtual tag the instruction ends there, without a&n;&t;   TLB lookup. */
r_int
r_int
r_int
id|aligned_start
suffix:semicolon
r_int
r_int
r_int
id|ull_end
suffix:semicolon
r_int
r_int
r_int
id|addr
suffix:semicolon
id|ull_end
op_assign
id|end
suffix:semicolon
multiline_comment|/* Just invalidate over the range using the natural addresses.  TLB&n;&t;   miss handling will be OK (TBC).  Since it&squot;s for the current process,&n;&t;   either we&squot;re already in the right ASID context, or the ASIDs have&n;&t;   been recycled since we were last active in which case we might just&n;&t;   invalidate another processes I-cache entries : no worries, just a&n;&t;   performance drop for him. */
id|aligned_start
op_assign
id|start
op_amp
id|L1_CACHE_ALIGN_MASK
suffix:semicolon
id|addr
op_assign
id|aligned_start
suffix:semicolon
r_while
c_loop
(paren
id|addr
OL
id|ull_end
)paren
(brace
id|asm
id|__volatile__
(paren
l_string|&quot;icbi %0, 0&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|addr
)paren
)paren
suffix:semicolon
id|asm
id|__volatile__
(paren
l_string|&quot;nop&quot;
)paren
suffix:semicolon
id|asm
id|__volatile__
(paren
l_string|&quot;nop&quot;
)paren
suffix:semicolon
id|addr
op_add_assign
id|L1_CACHE_BYTES
suffix:semicolon
)brace
)brace
macro_line|#endif /* !CONFIG_ICACHE_DISABLED */
multiline_comment|/****************************************************************************/
macro_line|#ifndef CONFIG_DCACHE_DISABLED
multiline_comment|/* Buffer used as the target of alloco instructions to purge data from cache&n;   sets by natural eviction. -- RPC */
DECL|macro|DUMMY_ALLOCO_AREA_SIZE
mdefine_line|#define DUMMY_ALLOCO_AREA_SIZE L1_CACHE_SIZE_BYTES + (1024 * 4)
DECL|variable|__cacheline_aligned
r_static
r_int
r_char
id|dummy_alloco_area
(braket
id|DUMMY_ALLOCO_AREA_SIZE
)braket
id|__cacheline_aligned
op_assign
(brace
l_int|0
comma
)brace
suffix:semicolon
multiline_comment|/****************************************************************************/
DECL|function|sh64_dcache_purge_sets
r_static
r_void
id|__inline__
id|sh64_dcache_purge_sets
c_func
(paren
r_int
id|sets_to_purge_base
comma
r_int
id|n_sets
)paren
(brace
multiline_comment|/* Purge all ways in a particular block of sets, specified by the base&n;&t;   set number and number of sets.  Can handle wrap-around, if that&squot;s&n;&t;   needed.  */
r_int
id|dummy_buffer_base_set
suffix:semicolon
r_int
r_int
r_int
id|eaddr
comma
id|eaddr0
comma
id|eaddr1
suffix:semicolon
r_int
id|j
suffix:semicolon
r_int
id|set_offset
suffix:semicolon
id|dummy_buffer_base_set
op_assign
(paren
(paren
r_int
)paren
op_amp
id|dummy_alloco_area
op_amp
id|cpu_data-&gt;dcache.idx_mask
)paren
op_rshift
id|cpu_data-&gt;dcache.entry_shift
suffix:semicolon
id|set_offset
op_assign
id|sets_to_purge_base
op_minus
id|dummy_buffer_base_set
suffix:semicolon
r_for
c_loop
(paren
id|j
op_assign
l_int|0
suffix:semicolon
id|j
OL
id|n_sets
suffix:semicolon
id|j
op_increment
comma
id|set_offset
op_increment
)paren
(brace
id|set_offset
op_and_assign
(paren
id|cpu_data-&gt;dcache.sets
op_minus
l_int|1
)paren
suffix:semicolon
id|eaddr0
op_assign
(paren
r_int
r_int
r_int
)paren
id|dummy_alloco_area
op_plus
(paren
id|set_offset
op_lshift
id|cpu_data-&gt;dcache.entry_shift
)paren
suffix:semicolon
multiline_comment|/* Do one alloco which hits the required set per cache way.  For&n;&t;&t;   write-back mode, this will purge the #ways resident lines.   There&squot;s&n;&t;&t;   little point unrolling this loop because the allocos stall more if&n;&t;&t;   they&squot;re too close together. */
id|eaddr1
op_assign
id|eaddr0
op_plus
id|cpu_data-&gt;dcache.way_ofs
op_star
id|cpu_data-&gt;dcache.ways
suffix:semicolon
r_for
c_loop
(paren
id|eaddr
op_assign
id|eaddr0
suffix:semicolon
id|eaddr
OL
id|eaddr1
suffix:semicolon
id|eaddr
op_add_assign
id|cpu_data-&gt;dcache.way_ofs
)paren
(brace
id|asm
id|__volatile__
(paren
l_string|&quot;alloco %0, 0&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|eaddr
)paren
)paren
suffix:semicolon
id|asm
id|__volatile__
(paren
l_string|&quot;synco&quot;
)paren
suffix:semicolon
multiline_comment|/* TAKum03020 */
)brace
id|eaddr1
op_assign
id|eaddr0
op_plus
id|cpu_data-&gt;dcache.way_ofs
op_star
id|cpu_data-&gt;dcache.ways
suffix:semicolon
r_for
c_loop
(paren
id|eaddr
op_assign
id|eaddr0
suffix:semicolon
id|eaddr
OL
id|eaddr1
suffix:semicolon
id|eaddr
op_add_assign
id|cpu_data-&gt;dcache.way_ofs
)paren
(brace
multiline_comment|/* Load from each address.  Required because alloco is a NOP if&n;&t;&t;&t;   the cache is write-through.  Write-through is a config option. */
r_if
c_cond
(paren
id|test_bit
c_func
(paren
id|SH_CACHE_MODE_WT
comma
op_amp
(paren
id|cpu_data-&gt;dcache.flags
)paren
)paren
)paren
op_star
(paren
r_volatile
r_int
r_char
op_star
)paren
(paren
r_int
)paren
id|eaddr
suffix:semicolon
)brace
)brace
multiline_comment|/* Don&squot;t use OCBI to invalidate the lines.  That costs cycles directly.&n;&t;   If the dummy block is just left resident, it will naturally get&n;&t;   evicted as required.  */
r_return
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
DECL|function|sh64_dcache_purge_all
r_static
r_void
id|sh64_dcache_purge_all
c_func
(paren
r_void
)paren
(brace
multiline_comment|/* Purge the entire contents of the dcache.  The most efficient way to&n;&t;   achieve this is to use alloco instructions on a region of unused&n;&t;   memory equal in size to the cache, thereby causing the current&n;&t;   contents to be discarded by natural eviction.  The alternative,&n;&t;   namely reading every tag, setting up a mapping for the corresponding&n;&t;   page and doing an OCBP for the line, would be much more expensive.&n;&t;   */
id|sh64_dcache_purge_sets
c_func
(paren
l_int|0
comma
id|cpu_data-&gt;dcache.sets
)paren
suffix:semicolon
r_return
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
DECL|function|sh64_dcache_purge_kernel_range
r_static
r_void
id|sh64_dcache_purge_kernel_range
c_func
(paren
r_int
r_int
id|start
comma
r_int
r_int
id|end
)paren
(brace
multiline_comment|/* Purge the range of addresses [start,end] from the D-cache.  The&n;&t;   addresses lie in the superpage mapping.  There&squot;s no harm if we&n;&t;   overpurge at either end - just a small performance loss. */
r_int
r_int
r_int
id|ullend
comma
id|addr
comma
id|aligned_start
suffix:semicolon
macro_line|#if (NEFF == 32)
id|aligned_start
op_assign
(paren
r_int
r_int
r_int
)paren
(paren
r_int
r_int
r_int
)paren
(paren
r_int
r_int
)paren
id|start
suffix:semicolon
macro_line|#else
macro_line|#error &quot;NEFF != 32&quot;
macro_line|#endif
id|aligned_start
op_and_assign
id|L1_CACHE_ALIGN_MASK
suffix:semicolon
id|addr
op_assign
id|aligned_start
suffix:semicolon
macro_line|#if (NEFF == 32)
id|ullend
op_assign
(paren
r_int
r_int
r_int
)paren
(paren
r_int
r_int
r_int
)paren
(paren
r_int
r_int
)paren
id|end
suffix:semicolon
macro_line|#else
macro_line|#error &quot;NEFF != 32&quot;
macro_line|#endif
r_while
c_loop
(paren
id|addr
op_le
id|ullend
)paren
(brace
id|asm
id|__volatile__
(paren
l_string|&quot;ocbp %0, 0&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|addr
)paren
)paren
suffix:semicolon
id|addr
op_add_assign
id|L1_CACHE_BYTES
suffix:semicolon
)brace
r_return
suffix:semicolon
)brace
multiline_comment|/* Assumes this address (+ (2**n_synbits) pages up from it) aren&squot;t used for&n;   anything else in the kernel */
DECL|macro|MAGIC_PAGE0_START
mdefine_line|#define MAGIC_PAGE0_START 0xffffffffec000000ULL
DECL|function|sh64_dcache_purge_coloured_phy_page
r_static
r_void
id|sh64_dcache_purge_coloured_phy_page
c_func
(paren
r_int
r_int
id|paddr
comma
r_int
r_int
id|eaddr
)paren
(brace
multiline_comment|/* Purge the physical page &squot;paddr&squot; from the cache.  It&squot;s known that any&n;&t;   cache lines requiring attention have the same page colour as the the&n;&t;   address &squot;eaddr&squot;.&n;&n;&t;   This relies on the fact that the D-cache matches on physical tags&n;&t;   when no virtual tag matches.  So we create an alias for the original&n;&t;   page and purge through that.  (Alternatively, we could have done&n;&t;   this by switching ASID to match the original mapping and purged&n;&t;   through that, but that involves ASID switching cost + probably a&n;&t;   TLBMISS + refill anyway.)&n;&t;   */
r_int
r_int
r_int
id|magic_page_start
suffix:semicolon
r_int
r_int
r_int
id|magic_eaddr
comma
id|magic_eaddr_end
suffix:semicolon
id|magic_page_start
op_assign
id|MAGIC_PAGE0_START
op_plus
(paren
id|eaddr
op_amp
id|CACHE_OC_SYN_MASK
)paren
suffix:semicolon
multiline_comment|/* As long as the kernel is not pre-emptible, this doesn&squot;t need to be&n;&t;   under cli/sti. */
id|sh64_setup_dtlb_cache_slot
c_func
(paren
id|magic_page_start
comma
id|get_asid
c_func
(paren
)paren
comma
id|paddr
)paren
suffix:semicolon
id|magic_eaddr
op_assign
id|magic_page_start
suffix:semicolon
id|magic_eaddr_end
op_assign
id|magic_eaddr
op_plus
id|PAGE_SIZE
suffix:semicolon
r_while
c_loop
(paren
id|magic_eaddr
OL
id|magic_eaddr_end
)paren
(brace
multiline_comment|/* Little point in unrolling this loop - the OCBPs are blocking&n;&t;&t;   and won&squot;t go any quicker (i.e. the loop overhead is parallel&n;&t;&t;   to part of the OCBP execution.) */
id|asm
id|__volatile__
(paren
l_string|&quot;ocbp %0, 0&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|magic_eaddr
)paren
)paren
suffix:semicolon
id|magic_eaddr
op_add_assign
id|L1_CACHE_BYTES
suffix:semicolon
)brace
id|sh64_teardown_dtlb_cache_slot
c_func
(paren
)paren
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
DECL|function|sh64_dcache_purge_phy_page
r_static
r_void
id|sh64_dcache_purge_phy_page
c_func
(paren
r_int
r_int
id|paddr
)paren
(brace
multiline_comment|/* Pure a page given its physical start address, by creating a&n;&t;   temporary 1 page mapping and purging across that.  Even if we know&n;&t;   the virtual address (&amp; vma or mm) of the page, the method here is&n;&t;   more elegant because it avoids issues of coping with page faults on&n;&t;   the purge instructions (i.e. no special-case code required in the&n;&t;   critical path in the TLB miss handling). */
r_int
r_int
r_int
id|eaddr_start
comma
id|eaddr
comma
id|eaddr_end
suffix:semicolon
r_int
id|i
suffix:semicolon
multiline_comment|/* As long as the kernel is not pre-emptible, this doesn&squot;t need to be&n;&t;   under cli/sti. */
id|eaddr_start
op_assign
id|MAGIC_PAGE0_START
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
(paren
l_int|1
op_lshift
id|CACHE_OC_N_SYNBITS
)paren
suffix:semicolon
id|i
op_increment
)paren
(brace
id|sh64_setup_dtlb_cache_slot
c_func
(paren
id|eaddr_start
comma
id|get_asid
c_func
(paren
)paren
comma
id|paddr
)paren
suffix:semicolon
id|eaddr
op_assign
id|eaddr_start
suffix:semicolon
id|eaddr_end
op_assign
id|eaddr
op_plus
id|PAGE_SIZE
suffix:semicolon
r_while
c_loop
(paren
id|eaddr
OL
id|eaddr_end
)paren
(brace
id|asm
id|__volatile__
(paren
l_string|&quot;ocbp %0, 0&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|eaddr
)paren
)paren
suffix:semicolon
id|eaddr
op_add_assign
id|L1_CACHE_BYTES
suffix:semicolon
)brace
id|sh64_teardown_dtlb_cache_slot
c_func
(paren
)paren
suffix:semicolon
id|eaddr_start
op_add_assign
id|PAGE_SIZE
suffix:semicolon
)brace
)brace
DECL|function|sh64_dcache_purge_user_page
r_static
r_void
id|sh64_dcache_purge_user_page
c_func
(paren
r_struct
id|mm_struct
op_star
id|mm
comma
r_int
r_int
id|eaddr
)paren
(brace
id|pgd_t
op_star
id|pgd
suffix:semicolon
id|pmd_t
op_star
id|pmd
suffix:semicolon
id|pte_t
op_star
id|pte
suffix:semicolon
id|pte_t
id|entry
suffix:semicolon
r_int
r_int
id|paddr
suffix:semicolon
multiline_comment|/* NOTE : all the callers of this have mm-&gt;page_table_lock held, so the&n;&t;   following page table traversal is safe even on SMP/pre-emptible. */
r_if
c_cond
(paren
op_logical_neg
id|mm
)paren
r_return
suffix:semicolon
multiline_comment|/* No way to find physical address of page */
id|pgd
op_assign
id|pgd_offset
c_func
(paren
id|mm
comma
id|eaddr
)paren
suffix:semicolon
r_if
c_cond
(paren
id|pgd_bad
c_func
(paren
op_star
id|pgd
)paren
)paren
r_return
suffix:semicolon
id|pmd
op_assign
id|pmd_offset
c_func
(paren
id|pgd
comma
id|eaddr
)paren
suffix:semicolon
r_if
c_cond
(paren
id|pmd_none
c_func
(paren
op_star
id|pmd
)paren
op_logical_or
id|pmd_bad
c_func
(paren
op_star
id|pmd
)paren
)paren
r_return
suffix:semicolon
id|pte
op_assign
id|pte_offset_kernel
c_func
(paren
id|pmd
comma
id|eaddr
)paren
suffix:semicolon
id|entry
op_assign
op_star
id|pte
suffix:semicolon
r_if
c_cond
(paren
id|pte_none
c_func
(paren
id|entry
)paren
op_logical_or
op_logical_neg
id|pte_present
c_func
(paren
id|entry
)paren
)paren
r_return
suffix:semicolon
id|paddr
op_assign
id|pte_val
c_func
(paren
id|entry
)paren
op_amp
id|PAGE_MASK
suffix:semicolon
id|sh64_dcache_purge_coloured_phy_page
c_func
(paren
id|paddr
comma
id|eaddr
)paren
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
DECL|function|sh64_dcache_purge_user_range
r_static
r_void
id|sh64_dcache_purge_user_range
c_func
(paren
r_struct
id|mm_struct
op_star
id|mm
comma
r_int
r_int
id|start
comma
r_int
r_int
id|end
)paren
(brace
multiline_comment|/* There are at least 5 choices for the implementation of this, with&n;&t;   pros (+), cons(-), comments(*):&n;&n;&t;   1. ocbp each line in the range through the original user&squot;s ASID&n;&t;      + no lines spuriously evicted&n;&t;      - tlbmiss handling (must either handle faults on demand =&gt; extra&n;&t;&t;special-case code in tlbmiss critical path), or map the page in&n;&t;&t;advance (=&gt; flush_tlb_range in advance to avoid multiple hits)&n;&t;      - ASID switching&n;&t;      - expensive for large ranges&n;&n;&t;   2. temporarily map each page in the range to a special effective&n;&t;      address and ocbp through the temporary mapping; relies on the&n;&t;      fact that SH-5 OCB* always do TLB lookup and match on ptags (they&n;&t;      never look at the etags)&n;&t;      + no spurious evictions&n;&t;      - expensive for large ranges&n;&t;      * surely cheaper than (1)&n;&n;&t;   3. walk all the lines in the cache, check the tags, if a match&n;&t;      occurs create a page mapping to ocbp the line through&n;&t;      + no spurious evictions&n;&t;      - tag inspection overhead&n;&t;      - (especially for small ranges)&n;&t;      - potential cost of setting up/tearing down page mapping for&n;&t;&t;every line that matches the range&n;&t;      * cost partly independent of range size&n;&n;&t;   4. walk all the lines in the cache, check the tags, if a match&n;&t;      occurs use 4 * alloco to purge the line (+3 other probably&n;&t;      innocent victims) by natural eviction&n;&t;      + no tlb mapping overheads&n;&t;      - spurious evictions&n;&t;      - tag inspection overhead&n;&n;&t;   5. implement like flush_cache_all&n;&t;      + no tag inspection overhead&n;&t;      - spurious evictions&n;&t;      - bad for small ranges&n;&n;&t;   (1) can be ruled out as more expensive than (2).  (2) appears best&n;&t;   for small ranges.  The choice between (3), (4) and (5) for large&n;&t;   ranges and the range size for the large/small boundary need&n;&t;   benchmarking to determine.&n;&n;&t;   For now use approach (2) for small ranges and (5) for large ones.&n;&n;&t;   */
r_int
id|n_pages
suffix:semicolon
id|n_pages
op_assign
(paren
(paren
id|end
op_minus
id|start
)paren
op_rshift
id|PAGE_SHIFT
)paren
suffix:semicolon
r_if
c_cond
(paren
id|n_pages
op_ge
l_int|64
)paren
(brace
macro_line|#if 1
id|sh64_dcache_purge_all
c_func
(paren
)paren
suffix:semicolon
macro_line|#else
r_int
r_int
r_int
id|set
comma
id|way
suffix:semicolon
r_int
r_int
id|mm_asid
op_assign
id|mm-&gt;context
op_amp
id|MMU_CONTEXT_ASID_MASK
suffix:semicolon
r_for
c_loop
(paren
id|set
op_assign
l_int|0
suffix:semicolon
id|set
OL
id|cpu_data-&gt;dcache.sets
suffix:semicolon
id|set
op_increment
)paren
(brace
r_int
r_int
r_int
id|set_base_config_addr
op_assign
id|CACHE_OC_ADDRESS_ARRAY
op_plus
(paren
id|set
op_lshift
id|cpu_data-&gt;dcache.set_shift
)paren
suffix:semicolon
r_for
c_loop
(paren
id|way
op_assign
l_int|0
suffix:semicolon
id|way
OL
id|cpu_data-&gt;dcache.ways
suffix:semicolon
id|way
op_increment
)paren
(brace
r_int
r_int
r_int
id|config_addr
op_assign
id|set_base_config_addr
op_plus
(paren
id|way
op_lshift
id|cpu_data-&gt;dcache.way_step_shift
)paren
suffix:semicolon
r_int
r_int
r_int
id|tag0
suffix:semicolon
r_int
r_int
id|line_valid
suffix:semicolon
id|asm
id|__volatile__
c_func
(paren
l_string|&quot;getcfg %1, 0, %0&quot;
suffix:colon
l_string|&quot;=r&quot;
(paren
id|tag0
)paren
suffix:colon
l_string|&quot;r&quot;
(paren
id|config_addr
)paren
)paren
suffix:semicolon
id|line_valid
op_assign
id|tag0
op_amp
id|SH_CACHE_VALID
suffix:semicolon
r_if
c_cond
(paren
id|line_valid
)paren
(brace
r_int
r_int
id|cache_asid
suffix:semicolon
r_int
r_int
id|epn
suffix:semicolon
id|cache_asid
op_assign
(paren
id|tag0
op_amp
id|cpu_data-&gt;dcache.asid_mask
)paren
op_rshift
id|cpu_data-&gt;dcache.asid_shift
suffix:semicolon
multiline_comment|/* The next line needs some&n;&t;&t;&t;&t;&t;   explanation.  The virtual tags&n;&t;&t;&t;&t;&t;   encode bits [31:13] of the virtual&n;&t;&t;&t;&t;&t;   address, bit [12] of the &squot;tag&squot; being&n;&t;&t;&t;&t;&t;   implied by the cache set index. */
id|epn
op_assign
(paren
id|tag0
op_amp
id|cpu_data-&gt;dcache.epn_mask
)paren
op_or
(paren
(paren
id|set
op_amp
l_int|0x80
)paren
op_lshift
id|cpu_data-&gt;dcache.entry_shift
)paren
suffix:semicolon
r_if
c_cond
(paren
(paren
id|cache_asid
op_eq
id|mm_asid
)paren
op_logical_and
(paren
id|start
op_le
id|epn
)paren
op_logical_and
(paren
id|epn
OL
id|end
)paren
)paren
(brace
multiline_comment|/* TODO : could optimise this&n;&t;&t;&t;&t;&t;&t;   call by batching multiple&n;&t;&t;&t;&t;&t;&t;   adjacent sets together. */
id|sh64_dcache_purge_sets
c_func
(paren
id|set
comma
l_int|1
)paren
suffix:semicolon
r_break
suffix:semicolon
multiline_comment|/* Don&squot;t waste time inspecting other ways for this set */
)brace
)brace
)brace
)brace
macro_line|#endif
)brace
r_else
(brace
multiline_comment|/* &squot;Small&squot; range */
r_int
r_int
id|aligned_start
suffix:semicolon
r_int
r_int
id|eaddr
suffix:semicolon
r_int
r_int
id|last_page_start
suffix:semicolon
id|aligned_start
op_assign
id|start
op_amp
id|PAGE_MASK
suffix:semicolon
multiline_comment|/* &squot;end&squot; is 1 byte beyond the end of the range */
id|last_page_start
op_assign
(paren
id|end
op_minus
l_int|1
)paren
op_amp
id|PAGE_MASK
suffix:semicolon
id|eaddr
op_assign
id|aligned_start
suffix:semicolon
r_while
c_loop
(paren
id|eaddr
op_le
id|last_page_start
)paren
(brace
id|sh64_dcache_purge_user_page
c_func
(paren
id|mm
comma
id|eaddr
)paren
suffix:semicolon
id|eaddr
op_add_assign
id|PAGE_SIZE
suffix:semicolon
)brace
)brace
r_return
suffix:semicolon
)brace
DECL|function|sh64_dcache_wback_current_user_range
r_static
r_void
id|sh64_dcache_wback_current_user_range
c_func
(paren
r_int
r_int
id|start
comma
r_int
r_int
id|end
)paren
(brace
r_int
r_int
r_int
id|aligned_start
suffix:semicolon
r_int
r_int
r_int
id|ull_end
suffix:semicolon
r_int
r_int
r_int
id|addr
suffix:semicolon
id|ull_end
op_assign
id|end
suffix:semicolon
multiline_comment|/* Just wback over the range using the natural addresses.  TLB miss&n;&t;   handling will be OK (TBC) : the range has just been written to by&n;&t;   the signal frame setup code, so the PTEs must exist.&n;&n;&t;   Note, if we have CONFIG_PREEMPT and get preempted inside this loop,&n;&t;   it doesn&squot;t matter, even if the pid-&gt;ASID mapping changes whilst&n;&t;   we&squot;re away.  In that case the cache will have been flushed when the&n;&t;   mapping was renewed.  So the writebacks below will be nugatory (and&n;&t;   we&squot;ll doubtless have to fault the TLB entry/ies in again with the&n;&t;   new ASID), but it&squot;s a rare case.&n;&t;   */
id|aligned_start
op_assign
id|start
op_amp
id|L1_CACHE_ALIGN_MASK
suffix:semicolon
id|addr
op_assign
id|aligned_start
suffix:semicolon
r_while
c_loop
(paren
id|addr
OL
id|ull_end
)paren
(brace
id|asm
id|__volatile__
(paren
l_string|&quot;ocbwb %0, 0&quot;
suffix:colon
suffix:colon
l_string|&quot;r&quot;
(paren
id|addr
)paren
)paren
suffix:semicolon
id|addr
op_add_assign
id|L1_CACHE_BYTES
suffix:semicolon
)brace
)brace
multiline_comment|/****************************************************************************/
multiline_comment|/* These *MUST* lie in an area of virtual address space that&squot;s otherwise unused. */
DECL|macro|UNIQUE_EADDR_START
mdefine_line|#define UNIQUE_EADDR_START 0xe0000000UL
DECL|macro|UNIQUE_EADDR_END
mdefine_line|#define UNIQUE_EADDR_END   0xe8000000UL
DECL|function|sh64_make_unique_eaddr
r_static
r_int
r_int
id|sh64_make_unique_eaddr
c_func
(paren
r_int
r_int
id|user_eaddr
comma
r_int
r_int
id|paddr
)paren
(brace
multiline_comment|/* Given a physical address paddr, and a user virtual address&n;&t;   user_eaddr which will eventually be mapped to it, create a one-off&n;&t;   kernel-private eaddr mapped to the same paddr.  This is used for&n;&t;   creating special destination pages for copy_user_page and&n;&t;   clear_user_page */
r_static
r_int
r_int
id|current_pointer
op_assign
id|UNIQUE_EADDR_START
suffix:semicolon
r_int
r_int
id|coloured_pointer
suffix:semicolon
r_if
c_cond
(paren
id|current_pointer
op_eq
id|UNIQUE_EADDR_END
)paren
(brace
id|sh64_dcache_purge_all
c_func
(paren
)paren
suffix:semicolon
id|current_pointer
op_assign
id|UNIQUE_EADDR_START
suffix:semicolon
)brace
id|coloured_pointer
op_assign
(paren
id|current_pointer
op_amp
op_complement
id|CACHE_OC_SYN_MASK
)paren
op_or
(paren
id|user_eaddr
op_amp
id|CACHE_OC_SYN_MASK
)paren
suffix:semicolon
id|sh64_setup_dtlb_cache_slot
c_func
(paren
id|coloured_pointer
comma
id|get_asid
c_func
(paren
)paren
comma
id|paddr
)paren
suffix:semicolon
id|current_pointer
op_add_assign
(paren
id|PAGE_SIZE
op_lshift
id|CACHE_OC_N_SYNBITS
)paren
suffix:semicolon
r_return
id|coloured_pointer
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
DECL|function|sh64_copy_user_page_coloured
r_static
r_void
id|sh64_copy_user_page_coloured
c_func
(paren
r_void
op_star
id|to
comma
r_void
op_star
id|from
comma
r_int
r_int
id|address
)paren
(brace
r_void
op_star
id|coloured_to
suffix:semicolon
multiline_comment|/* Discard any existing cache entries of the wrong colour.  These are&n;&t;   present quite often, if the kernel has recently used the page&n;&t;   internally, then given it up, then it&squot;s been allocated to the user.&n;&t;   */
id|sh64_dcache_purge_coloured_phy_page
c_func
(paren
id|__pa
c_func
(paren
id|to
)paren
comma
(paren
r_int
r_int
)paren
id|to
)paren
suffix:semicolon
id|coloured_to
op_assign
(paren
r_void
op_star
)paren
id|sh64_make_unique_eaddr
c_func
(paren
id|address
comma
id|__pa
c_func
(paren
id|to
)paren
)paren
suffix:semicolon
id|sh64_page_copy
c_func
(paren
id|from
comma
id|coloured_to
)paren
suffix:semicolon
id|sh64_teardown_dtlb_cache_slot
c_func
(paren
)paren
suffix:semicolon
)brace
DECL|function|sh64_clear_user_page_coloured
r_static
r_void
id|sh64_clear_user_page_coloured
c_func
(paren
r_void
op_star
id|to
comma
r_int
r_int
id|address
)paren
(brace
r_void
op_star
id|coloured_to
suffix:semicolon
multiline_comment|/* Discard any existing kernel-originated lines of the wrong colour (as&n;&t;   above) */
id|sh64_dcache_purge_coloured_phy_page
c_func
(paren
id|__pa
c_func
(paren
id|to
)paren
comma
(paren
r_int
r_int
)paren
id|to
)paren
suffix:semicolon
id|coloured_to
op_assign
(paren
r_void
op_star
)paren
id|sh64_make_unique_eaddr
c_func
(paren
id|address
comma
id|__pa
c_func
(paren
id|to
)paren
)paren
suffix:semicolon
id|sh64_page_clear
c_func
(paren
id|coloured_to
)paren
suffix:semicolon
id|sh64_teardown_dtlb_cache_slot
c_func
(paren
)paren
suffix:semicolon
)brace
macro_line|#endif /* !CONFIG_DCACHE_DISABLED */
multiline_comment|/****************************************************************************/
multiline_comment|/*##########################################################################&n;&t;&t;&t;    EXTERNALLY CALLABLE API.&n;  ##########################################################################*/
multiline_comment|/* These functions are described in Documentation/cachetlb.txt.&n;   Each one of these functions varies in behaviour depending on whether the&n;   I-cache and/or D-cache are configured out.&n;&n;   Note that the Linux term &squot;flush&squot; corresponds to what is termed &squot;purge&squot; in&n;   the sh/sh64 jargon for the D-cache, i.e. write back dirty data then&n;   invalidate the cache lines, and &squot;invalidate&squot; for the I-cache.&n;   */
DECL|macro|FLUSH_TRACE
macro_line|#undef FLUSH_TRACE
DECL|function|flush_cache_all
r_void
id|flush_cache_all
c_func
(paren
r_void
)paren
(brace
multiline_comment|/* Invalidate the entire contents of both caches, after writing back to&n;&t;   memory any dirty data from the D-cache. */
id|sh64_dcache_purge_all
c_func
(paren
)paren
suffix:semicolon
id|sh64_icache_inv_all
c_func
(paren
)paren
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
DECL|function|flush_cache_mm
r_void
id|flush_cache_mm
c_func
(paren
r_struct
id|mm_struct
op_star
id|mm
)paren
(brace
multiline_comment|/* Invalidate an entire user-address space from both caches, after&n;&t;   writing back dirty data (e.g. for shared mmap etc). */
multiline_comment|/* This could be coded selectively by inspecting all the tags then&n;&t;   doing 4*alloco on any set containing a match (as for&n;&t;   flush_cache_range), but fork/exit/execve (where this is called from)&n;&t;   are expensive anyway. */
multiline_comment|/* Have to do a purge here, despite the comments re I-cache below.&n;&t;   There could be odd-coloured dirty data associated with the mm still&n;&t;   in the cache - if this gets written out through natural eviction&n;&t;   after the kernel has reused the page there will be chaos.&n;&t;   */
id|sh64_dcache_purge_all
c_func
(paren
)paren
suffix:semicolon
multiline_comment|/* The mm being torn down won&squot;t ever be active again, so any Icache&n;&t;   lines tagged with its ASID won&squot;t be visible for the rest of the&n;&t;   lifetime of this ASID cycle.  Before the ASID gets reused, there&n;&t;   will be a flush_cache_all.  Hence we don&squot;t need to touch the&n;&t;   I-cache.  This is similar to the lack of action needed in&n;&t;   flush_tlb_mm - see fault.c. */
)brace
multiline_comment|/****************************************************************************/
DECL|function|flush_cache_range
r_void
id|flush_cache_range
c_func
(paren
r_struct
id|vm_area_struct
op_star
id|vma
comma
r_int
r_int
id|start
comma
r_int
r_int
id|end
)paren
(brace
r_struct
id|mm_struct
op_star
id|mm
op_assign
id|vma-&gt;vm_mm
suffix:semicolon
multiline_comment|/* Invalidate (from both caches) the range [start,end) of virtual&n;&t;   addresses from the user address space specified by mm, after writing&n;&t;   back any dirty data.&n;&n;&t;   Note(1), &squot;end&squot; is 1 byte beyond the end of the range to flush.&n;&n;&t;   Note(2), this is called with mm-&gt;page_table_lock held.*/
id|sh64_dcache_purge_user_range
c_func
(paren
id|mm
comma
id|start
comma
id|end
)paren
suffix:semicolon
id|sh64_icache_inv_user_page_range
c_func
(paren
id|mm
comma
id|start
comma
id|end
)paren
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
DECL|function|flush_cache_page
r_void
id|flush_cache_page
c_func
(paren
r_struct
id|vm_area_struct
op_star
id|vma
comma
r_int
r_int
id|eaddr
comma
r_int
r_int
id|pfn
)paren
(brace
multiline_comment|/* Invalidate any entries in either cache for the vma within the user&n;&t;   address space vma-&gt;vm_mm for the page starting at virtual address&n;&t;   &squot;eaddr&squot;.   This seems to be used primarily in breaking COW.  Note,&n;&t;   the I-cache must be searched too in case the page in question is&n;&t;   both writable and being executed from (e.g. stack trampolines.)&n;&n;&t;   Note(1), this is called with mm-&gt;page_table_lock held.&n;&t;   */
id|sh64_dcache_purge_phy_page
c_func
(paren
id|pfn
op_lshift
id|PAGE_SHIFT
)paren
suffix:semicolon
r_if
c_cond
(paren
id|vma-&gt;vm_flags
op_amp
id|VM_EXEC
)paren
(brace
id|sh64_icache_inv_user_page
c_func
(paren
id|vma
comma
id|eaddr
)paren
suffix:semicolon
)brace
)brace
multiline_comment|/****************************************************************************/
macro_line|#ifndef CONFIG_DCACHE_DISABLED
DECL|function|copy_user_page
r_void
id|copy_user_page
c_func
(paren
r_void
op_star
id|to
comma
r_void
op_star
id|from
comma
r_int
r_int
id|address
comma
r_struct
id|page
op_star
id|page
)paren
(brace
multiline_comment|/* &squot;from&squot; and &squot;to&squot; are kernel virtual addresses (within the superpage&n;&t;   mapping of the physical RAM).  &squot;address&squot; is the user virtual address&n;&t;   where the copy &squot;to&squot; will be mapped after.  This allows a custom&n;&t;   mapping to be used to ensure that the new copy is placed in the&n;&t;   right cache sets for the user to see it without having to bounce it&n;&t;   out via memory.  Note however : the call to flush_page_to_ram in&n;&t;   (generic)/mm/memory.c:(break_cow) undoes all this good work in that one&n;&t;   very important case!&n;&n;&t;   TBD : can we guarantee that on every call, any cache entries for&n;&t;   &squot;from&squot; are in the same colour sets as &squot;address&squot; also?  i.e. is this&n;&t;   always used just to deal with COW?  (I suspect not). */
multiline_comment|/* There are two possibilities here for when the page &squot;from&squot; was last accessed:&n;&t;   * by the kernel : this is OK, no purge required.&n;&t;   * by the/a user (e.g. for break_COW) : need to purge.&n;&n;&t;   If the potential user mapping at &squot;address&squot; is the same colour as&n;&t;   &squot;from&squot; there is no need to purge any cache lines from the &squot;from&squot;&n;&t;   page mapped into cache sets of colour &squot;address&squot;.  (The copy will be&n;&t;   accessing the page through &squot;from&squot;).&n;&t;   */
r_if
c_cond
(paren
(paren
(paren
id|address
op_xor
(paren
r_int
r_int
)paren
id|from
)paren
op_amp
id|CACHE_OC_SYN_MASK
)paren
op_ne
l_int|0
)paren
(brace
id|sh64_dcache_purge_coloured_phy_page
c_func
(paren
id|__pa
c_func
(paren
id|from
)paren
comma
id|address
)paren
suffix:semicolon
)brace
r_if
c_cond
(paren
(paren
(paren
id|address
op_xor
(paren
r_int
r_int
)paren
id|to
)paren
op_amp
id|CACHE_OC_SYN_MASK
)paren
op_eq
l_int|0
)paren
(brace
multiline_comment|/* No synonym problem on destination */
id|sh64_page_copy
c_func
(paren
id|from
comma
id|to
)paren
suffix:semicolon
)brace
r_else
(brace
id|sh64_copy_user_page_coloured
c_func
(paren
id|to
comma
id|from
comma
id|address
)paren
suffix:semicolon
)brace
multiline_comment|/* Note, don&squot;t need to flush &squot;from&squot; page from the cache again - it&squot;s&n;&t;   done anyway by the generic code */
)brace
DECL|function|clear_user_page
r_void
id|clear_user_page
c_func
(paren
r_void
op_star
id|to
comma
r_int
r_int
id|address
comma
r_struct
id|page
op_star
id|page
)paren
(brace
multiline_comment|/* &squot;to&squot; is a kernel virtual address (within the superpage&n;&t;   mapping of the physical RAM).  &squot;address&squot; is the user virtual address&n;&t;   where the &squot;to&squot; page will be mapped after.  This allows a custom&n;&t;   mapping to be used to ensure that the new copy is placed in the&n;&t;   right cache sets for the user to see it without having to bounce it&n;&t;   out via memory.&n;&t;*/
r_if
c_cond
(paren
(paren
(paren
id|address
op_xor
(paren
r_int
r_int
)paren
id|to
)paren
op_amp
id|CACHE_OC_SYN_MASK
)paren
op_eq
l_int|0
)paren
(brace
multiline_comment|/* No synonym problem on destination */
id|sh64_page_clear
c_func
(paren
id|to
)paren
suffix:semicolon
)brace
r_else
(brace
id|sh64_clear_user_page_coloured
c_func
(paren
id|to
comma
id|address
)paren
suffix:semicolon
)brace
)brace
macro_line|#endif /* !CONFIG_DCACHE_DISABLED */
multiline_comment|/****************************************************************************/
DECL|function|flush_dcache_page
r_void
id|flush_dcache_page
c_func
(paren
r_struct
id|page
op_star
id|page
)paren
(brace
id|sh64_dcache_purge_phy_page
c_func
(paren
id|page_to_phys
c_func
(paren
id|page
)paren
)paren
suffix:semicolon
id|wmb
c_func
(paren
)paren
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
DECL|function|flush_icache_range
r_void
id|flush_icache_range
c_func
(paren
r_int
r_int
id|start
comma
r_int
r_int
id|end
)paren
(brace
multiline_comment|/* Flush the range [start,end] of kernel virtual adddress space from&n;&t;   the I-cache.  The corresponding range must be purged from the&n;&t;   D-cache also because the SH-5 doesn&squot;t have cache snooping between&n;&t;   the caches.  The addresses will be visible through the superpage&n;&t;   mapping, therefore it&squot;s guaranteed that there no cache entries for&n;&t;   the range in cache sets of the wrong colour.&n;&n;&t;   Primarily used for cohering the I-cache after a module has&n;&t;   been loaded.  */
multiline_comment|/* We also make sure to purge the same range from the D-cache since&n;&t;   flush_page_to_ram() won&squot;t be doing this for us! */
id|sh64_dcache_purge_kernel_range
c_func
(paren
id|start
comma
id|end
)paren
suffix:semicolon
id|wmb
c_func
(paren
)paren
suffix:semicolon
id|sh64_icache_inv_kernel_range
c_func
(paren
id|start
comma
id|end
)paren
suffix:semicolon
)brace
multiline_comment|/****************************************************************************/
DECL|function|flush_icache_user_range
r_void
id|flush_icache_user_range
c_func
(paren
r_struct
id|vm_area_struct
op_star
id|vma
comma
r_struct
id|page
op_star
id|page
comma
r_int
r_int
id|addr
comma
r_int
id|len
)paren
(brace
multiline_comment|/* Flush the range of user (defined by vma-&gt;vm_mm) address space&n;&t;   starting at &squot;addr&squot; for &squot;len&squot; bytes from the cache.  The range does&n;&t;   not straddle a page boundary, the unique physical page containing&n;&t;   the range is &squot;page&squot;.  This seems to be used mainly for invalidating&n;&t;   an address range following a poke into the program text through the&n;&t;   ptrace() call from another process (e.g. for BRK instruction&n;&t;   insertion). */
id|sh64_dcache_purge_coloured_phy_page
c_func
(paren
id|page_to_phys
c_func
(paren
id|page
)paren
comma
id|addr
)paren
suffix:semicolon
id|mb
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|vma-&gt;vm_flags
op_amp
id|VM_EXEC
)paren
(brace
id|sh64_icache_inv_user_small_range
c_func
(paren
id|vma-&gt;vm_mm
comma
id|addr
comma
id|len
)paren
suffix:semicolon
)brace
)brace
multiline_comment|/*##########################################################################&n;&t;&t;&t;ARCH/SH64 PRIVATE CALLABLE API.&n;  ##########################################################################*/
DECL|function|flush_cache_sigtramp
r_void
id|flush_cache_sigtramp
c_func
(paren
r_int
r_int
id|start
comma
r_int
r_int
id|end
)paren
(brace
multiline_comment|/* For the address range [start,end), write back the data from the&n;&t;   D-cache and invalidate the corresponding region of the I-cache for&n;&t;   the current process.  Used to flush signal trampolines on the stack&n;&t;   to make them executable. */
id|sh64_dcache_wback_current_user_range
c_func
(paren
id|start
comma
id|end
)paren
suffix:semicolon
id|wmb
c_func
(paren
)paren
suffix:semicolon
id|sh64_icache_inv_current_user_range
c_func
(paren
id|start
comma
id|end
)paren
suffix:semicolon
)brace
eof
