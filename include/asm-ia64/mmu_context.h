macro_line|#ifndef _ASM_IA64_MMU_CONTEXT_H
DECL|macro|_ASM_IA64_MMU_CONTEXT_H
mdefine_line|#define _ASM_IA64_MMU_CONTEXT_H
multiline_comment|/*&n; * Copyright (C) 1998-2002 Hewlett-Packard Co&n; *&t;David Mosberger-Tang &lt;davidm@hpl.hp.com&gt;&n; */
multiline_comment|/*&n; * Routines to manage the allocation of task context numbers.  Task context numbers are&n; * used to reduce or eliminate the need to perform TLB flushes due to context switches.&n; * Context numbers are implemented using ia-64 region ids.  Since the IA-64 TLB does not&n; * consider the region number when performing a TLB lookup, we need to assign a unique&n; * region id to each region in a process.  We use the least significant three bits in a&n; * region id for this purpose.&n; */
DECL|macro|IA64_REGION_ID_KERNEL
mdefine_line|#define IA64_REGION_ID_KERNEL&t;0 /* the kernel&squot;s region id (tlb.c depends on this being 0) */
DECL|macro|ia64_rid
mdefine_line|#define ia64_rid(ctx,addr)&t;(((ctx) &lt;&lt; 3) | (addr &gt;&gt; 61))
macro_line|# ifndef __ASSEMBLY__
macro_line|#include &lt;linux/compiler.h&gt;
macro_line|#include &lt;linux/percpu.h&gt;
macro_line|#include &lt;linux/sched.h&gt;
macro_line|#include &lt;linux/spinlock.h&gt;
macro_line|#include &lt;asm/processor.h&gt;
DECL|macro|MMU_CONTEXT_DEBUG
mdefine_line|#define MMU_CONTEXT_DEBUG&t;0
macro_line|#if MMU_CONTEXT_DEBUG
macro_line|#include &lt;ia64intrin.h&gt;
DECL|struct|mmu_trace_entry
r_extern
r_struct
id|mmu_trace_entry
(brace
DECL|member|op
r_char
id|op
suffix:semicolon
DECL|member|cpu
id|u8
id|cpu
suffix:semicolon
DECL|member|context
id|u32
id|context
suffix:semicolon
DECL|member|mm
r_void
op_star
id|mm
suffix:semicolon
)brace
id|mmu_tbuf
(braket
l_int|1024
)braket
suffix:semicolon
r_extern
r_volatile
r_int
id|mmu_tbuf_index
suffix:semicolon
DECL|macro|MMU_TRACE
macro_line|# define MMU_TRACE(_op,_cpu,_mm,_ctx)&t;&t;&t;&t;&t;&t;&t;&bslash;&n;do {&t;&t;&t;&t;&t;&t;&t;&t;&t;&t;&t;&bslash;&n;&t;int i = __sync_fetch_and_add(&amp;mmu_tbuf_index, 1) % ARRAY_SIZE(mmu_tbuf);&t;&bslash;&n;&t;struct mmu_trace_entry e;&t;&t;&t;&t;&t;&t;&t;&bslash;&n;&t;e.op = (_op);&t;&t;&t;&t;&t;&t;&t;&t;&t;&bslash;&n;&t;e.cpu = (_cpu);&t;&t;&t;&t;&t;&t;&t;&t;&t;&bslash;&n;&t;e.mm = (_mm);&t;&t;&t;&t;&t;&t;&t;&t;&t;&bslash;&n;&t;e.context = (_ctx);&t;&t;&t;&t;&t;&t;&t;&t;&bslash;&n;&t;mmu_tbuf[i] = e;&t;&t;&t;&t;&t;&t;&t;&t;&bslash;&n;} while (0)
macro_line|#else
DECL|macro|MMU_TRACE
macro_line|# define MMU_TRACE(op,cpu,mm,ctx)&t;do { ; } while (0)
macro_line|#endif
DECL|struct|ia64_ctx
r_struct
id|ia64_ctx
(brace
DECL|member|lock
id|spinlock_t
id|lock
suffix:semicolon
DECL|member|next
r_int
r_int
id|next
suffix:semicolon
multiline_comment|/* next context number to use */
DECL|member|limit
r_int
r_int
id|limit
suffix:semicolon
multiline_comment|/* next &gt;= limit =&gt; must call wrap_mmu_context() */
DECL|member|max_ctx
r_int
r_int
id|max_ctx
suffix:semicolon
multiline_comment|/* max. context value supported by all CPUs */
)brace
suffix:semicolon
r_extern
r_struct
id|ia64_ctx
id|ia64_ctx
suffix:semicolon
id|DECLARE_PER_CPU
c_func
(paren
id|u8
comma
id|ia64_need_tlb_flush
)paren
suffix:semicolon
r_extern
r_void
id|wrap_mmu_context
(paren
r_struct
id|mm_struct
op_star
id|mm
)paren
suffix:semicolon
r_static
r_inline
r_void
DECL|function|enter_lazy_tlb
id|enter_lazy_tlb
(paren
r_struct
id|mm_struct
op_star
id|mm
comma
r_struct
id|task_struct
op_star
id|tsk
)paren
(brace
)brace
multiline_comment|/*&n; * When the context counter wraps around all TLBs need to be flushed because an old&n; * context number might have been reused. This is signalled by the ia64_need_tlb_flush&n; * per-CPU variable, which is checked in the routine below. Called by activate_mm().&n; * &lt;efocht@ess.nec.de&gt;&n; */
r_static
r_inline
r_void
DECL|function|delayed_tlb_flush
id|delayed_tlb_flush
(paren
r_void
)paren
(brace
r_extern
r_void
id|local_flush_tlb_all
(paren
r_void
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|__ia64_per_cpu_var
c_func
(paren
id|ia64_need_tlb_flush
)paren
)paren
)paren
(brace
id|local_flush_tlb_all
c_func
(paren
)paren
suffix:semicolon
id|__ia64_per_cpu_var
c_func
(paren
id|ia64_need_tlb_flush
)paren
op_assign
l_int|0
suffix:semicolon
)brace
)brace
r_static
r_inline
id|mm_context_t
DECL|function|get_mmu_context
id|get_mmu_context
(paren
r_struct
id|mm_struct
op_star
id|mm
)paren
(brace
id|mm_context_t
id|context
op_assign
id|mm-&gt;context
suffix:semicolon
r_if
c_cond
(paren
id|context
)paren
r_return
id|context
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|ia64_ctx.lock
)paren
suffix:semicolon
(brace
multiline_comment|/* re-check, now that we&squot;ve got the lock: */
id|context
op_assign
id|mm-&gt;context
suffix:semicolon
r_if
c_cond
(paren
id|context
op_eq
l_int|0
)paren
(brace
r_if
c_cond
(paren
id|ia64_ctx.next
op_ge
id|ia64_ctx.limit
)paren
id|wrap_mmu_context
c_func
(paren
id|mm
)paren
suffix:semicolon
id|mm-&gt;context
op_assign
id|context
op_assign
id|ia64_ctx.next
op_increment
suffix:semicolon
)brace
)brace
id|spin_unlock
c_func
(paren
op_amp
id|ia64_ctx.lock
)paren
suffix:semicolon
r_return
id|context
suffix:semicolon
)brace
multiline_comment|/*&n; * Initialize context number to some sane value.  MM is guaranteed to be a brand-new&n; * address-space, so no TLB flushing is needed, ever.&n; */
r_static
r_inline
r_int
DECL|function|init_new_context
id|init_new_context
(paren
r_struct
id|task_struct
op_star
id|p
comma
r_struct
id|mm_struct
op_star
id|mm
)paren
(brace
id|MMU_TRACE
c_func
(paren
l_char|&squot;N&squot;
comma
id|smp_processor_id
c_func
(paren
)paren
comma
id|mm
comma
l_int|0
)paren
suffix:semicolon
id|mm-&gt;context
op_assign
l_int|0
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
r_static
r_inline
r_void
DECL|function|destroy_context
id|destroy_context
(paren
r_struct
id|mm_struct
op_star
id|mm
)paren
(brace
multiline_comment|/* Nothing to do.  */
id|MMU_TRACE
c_func
(paren
l_char|&squot;D&squot;
comma
id|smp_processor_id
c_func
(paren
)paren
comma
id|mm
comma
id|mm-&gt;context
)paren
suffix:semicolon
)brace
r_static
r_inline
r_void
DECL|function|reload_context
id|reload_context
(paren
id|mm_context_t
id|context
)paren
(brace
r_int
r_int
id|rid
suffix:semicolon
r_int
r_int
id|rid_incr
op_assign
l_int|0
suffix:semicolon
r_int
r_int
id|rr0
comma
id|rr1
comma
id|rr2
comma
id|rr3
comma
id|rr4
suffix:semicolon
id|rid
op_assign
id|context
op_lshift
l_int|3
suffix:semicolon
multiline_comment|/* make space for encoding the region number */
id|rid_incr
op_assign
l_int|1
op_lshift
l_int|8
suffix:semicolon
multiline_comment|/* encode the region id, preferred page size, and VHPT enable bit: */
id|rr0
op_assign
(paren
id|rid
op_lshift
l_int|8
)paren
op_or
(paren
id|PAGE_SHIFT
op_lshift
l_int|2
)paren
op_or
l_int|1
suffix:semicolon
id|rr1
op_assign
id|rr0
op_plus
l_int|1
op_star
id|rid_incr
suffix:semicolon
id|rr2
op_assign
id|rr0
op_plus
l_int|2
op_star
id|rid_incr
suffix:semicolon
id|rr3
op_assign
id|rr0
op_plus
l_int|3
op_star
id|rid_incr
suffix:semicolon
id|rr4
op_assign
id|rr0
op_plus
l_int|4
op_star
id|rid_incr
suffix:semicolon
macro_line|#ifdef  CONFIG_HUGETLB_PAGE
id|rr4
op_assign
(paren
id|rr4
op_amp
(paren
op_complement
(paren
l_int|0xfcUL
)paren
)paren
)paren
op_or
(paren
id|HPAGE_SHIFT
op_lshift
l_int|2
)paren
suffix:semicolon
macro_line|#endif
id|ia64_set_rr
c_func
(paren
l_int|0x0000000000000000
comma
id|rr0
)paren
suffix:semicolon
id|ia64_set_rr
c_func
(paren
l_int|0x2000000000000000
comma
id|rr1
)paren
suffix:semicolon
id|ia64_set_rr
c_func
(paren
l_int|0x4000000000000000
comma
id|rr2
)paren
suffix:semicolon
id|ia64_set_rr
c_func
(paren
l_int|0x6000000000000000
comma
id|rr3
)paren
suffix:semicolon
id|ia64_set_rr
c_func
(paren
l_int|0x8000000000000000
comma
id|rr4
)paren
suffix:semicolon
id|ia64_srlz_i
c_func
(paren
)paren
suffix:semicolon
multiline_comment|/* srlz.i implies srlz.d */
)brace
r_static
r_inline
r_void
DECL|function|activate_context
id|activate_context
(paren
r_struct
id|mm_struct
op_star
id|mm
)paren
(brace
id|mm_context_t
id|context
suffix:semicolon
r_do
(brace
id|context
op_assign
id|get_mmu_context
c_func
(paren
id|mm
)paren
suffix:semicolon
id|MMU_TRACE
c_func
(paren
l_char|&squot;A&squot;
comma
id|smp_processor_id
c_func
(paren
)paren
comma
id|mm
comma
id|context
)paren
suffix:semicolon
id|reload_context
c_func
(paren
id|context
)paren
suffix:semicolon
id|MMU_TRACE
c_func
(paren
l_char|&squot;a&squot;
comma
id|smp_processor_id
c_func
(paren
)paren
comma
id|mm
comma
id|context
)paren
suffix:semicolon
multiline_comment|/* in the unlikely event of a TLB-flush by another thread, redo the load: */
)brace
r_while
c_loop
(paren
id|unlikely
c_func
(paren
id|context
op_ne
id|mm-&gt;context
)paren
)paren
suffix:semicolon
)brace
DECL|macro|deactivate_mm
mdefine_line|#define deactivate_mm(tsk,mm)&t;&t;&t;&t;&t;&bslash;&n;do {&t;&t;&t;&t;&t;&t;&t;&t;&bslash;&n;&t;MMU_TRACE(&squot;d&squot;, smp_processor_id(), mm, mm-&gt;context);&t;&bslash;&n;} while (0)
multiline_comment|/*&n; * Switch from address space PREV to address space NEXT.&n; */
r_static
r_inline
r_void
DECL|function|activate_mm
id|activate_mm
(paren
r_struct
id|mm_struct
op_star
id|prev
comma
r_struct
id|mm_struct
op_star
id|next
)paren
(brace
id|delayed_tlb_flush
c_func
(paren
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * We may get interrupts here, but that&squot;s OK because interrupt handlers cannot&n;&t; * touch user-space.&n;&t; */
id|ia64_set_kr
c_func
(paren
id|IA64_KR_PT_BASE
comma
id|__pa
c_func
(paren
id|next-&gt;pgd
)paren
)paren
suffix:semicolon
id|activate_context
c_func
(paren
id|next
)paren
suffix:semicolon
)brace
DECL|macro|switch_mm
mdefine_line|#define switch_mm(prev_mm,next_mm,next_task)&t;activate_mm(prev_mm, next_mm)
macro_line|# endif /* ! __ASSEMBLY__ */
macro_line|#endif /* _ASM_IA64_MMU_CONTEXT_H */
eof
