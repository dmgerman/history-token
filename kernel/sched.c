multiline_comment|/*&n; *  kernel/sched.c&n; *&n; *  Kernel scheduler and related syscalls&n; *&n; *  Copyright (C) 1991-2002  Linus Torvalds&n; *&n; *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and&n; *&t;&t;make semaphores SMP safe&n; *  1998-11-19&t;Implemented schedule_timeout() and related stuff&n; *&t;&t;by Andrea Arcangeli&n; *  2002-01-04&t;New ultra-scalable O(1) scheduler by Ingo Molnar:&n; *&t;&t;hybrid priority-list and round-robin design with&n; *&t;&t;an array-switch method of distributing timeslices&n; *&t;&t;and per-CPU runqueues.  Cleanups and useful suggestions&n; *&t;&t;by Davide Libenzi, preemptible kernel bits by Robert Love.&n; *  2003-09-03&t;Interactivity tuning by Con Kolivas.&n; *  2004-04-02&t;Scheduler domains code by Nick Piggin&n; */
macro_line|#include &lt;linux/mm.h&gt;
macro_line|#include &lt;linux/module.h&gt;
macro_line|#include &lt;linux/nmi.h&gt;
macro_line|#include &lt;linux/init.h&gt;
macro_line|#include &lt;asm/uaccess.h&gt;
macro_line|#include &lt;linux/highmem.h&gt;
macro_line|#include &lt;linux/smp_lock.h&gt;
macro_line|#include &lt;asm/mmu_context.h&gt;
macro_line|#include &lt;linux/interrupt.h&gt;
macro_line|#include &lt;linux/completion.h&gt;
macro_line|#include &lt;linux/kernel_stat.h&gt;
macro_line|#include &lt;linux/security.h&gt;
macro_line|#include &lt;linux/notifier.h&gt;
macro_line|#include &lt;linux/profile.h&gt;
macro_line|#include &lt;linux/suspend.h&gt;
macro_line|#include &lt;linux/blkdev.h&gt;
macro_line|#include &lt;linux/delay.h&gt;
macro_line|#include &lt;linux/smp.h&gt;
macro_line|#include &lt;linux/threads.h&gt;
macro_line|#include &lt;linux/timer.h&gt;
macro_line|#include &lt;linux/rcupdate.h&gt;
macro_line|#include &lt;linux/cpu.h&gt;
macro_line|#include &lt;linux/percpu.h&gt;
macro_line|#include &lt;linux/kthread.h&gt;
macro_line|#include &lt;linux/seq_file.h&gt;
macro_line|#include &lt;linux/syscalls.h&gt;
macro_line|#include &lt;linux/times.h&gt;
macro_line|#include &lt;asm/tlb.h&gt;
macro_line|#include &lt;asm/unistd.h&gt;
multiline_comment|/*&n; * Convert user-nice values [ -20 ... 0 ... 19 ]&n; * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],&n; * and back.&n; */
DECL|macro|NICE_TO_PRIO
mdefine_line|#define NICE_TO_PRIO(nice)&t;(MAX_RT_PRIO + (nice) + 20)
DECL|macro|PRIO_TO_NICE
mdefine_line|#define PRIO_TO_NICE(prio)&t;((prio) - MAX_RT_PRIO - 20)
DECL|macro|TASK_NICE
mdefine_line|#define TASK_NICE(p)&t;&t;PRIO_TO_NICE((p)-&gt;static_prio)
multiline_comment|/*&n; * &squot;User priority&squot; is the nice value converted to something we&n; * can work with better when scaling various scheduler parameters,&n; * it&squot;s a [ 0 ... 39 ] range.&n; */
DECL|macro|USER_PRIO
mdefine_line|#define USER_PRIO(p)&t;&t;((p)-MAX_RT_PRIO)
DECL|macro|TASK_USER_PRIO
mdefine_line|#define TASK_USER_PRIO(p)&t;USER_PRIO((p)-&gt;static_prio)
DECL|macro|MAX_USER_PRIO
mdefine_line|#define MAX_USER_PRIO&t;&t;(USER_PRIO(MAX_PRIO))
multiline_comment|/*&n; * Some helpers for converting nanosecond timing to jiffy resolution&n; */
DECL|macro|NS_TO_JIFFIES
mdefine_line|#define NS_TO_JIFFIES(TIME)&t;((TIME) / (1000000000 / HZ))
DECL|macro|JIFFIES_TO_NS
mdefine_line|#define JIFFIES_TO_NS(TIME)&t;((TIME) * (1000000000 / HZ))
multiline_comment|/*&n; * These are the &squot;tuning knobs&squot; of the scheduler:&n; *&n; * Minimum timeslice is 5 msecs (or 1 jiffy, whichever is larger),&n; * default timeslice is 100 msecs, maximum timeslice is 800 msecs.&n; * Timeslices get refilled after they expire.&n; */
DECL|macro|MIN_TIMESLICE
mdefine_line|#define MIN_TIMESLICE&t;&t;max(5 * HZ / 1000, 1)
DECL|macro|DEF_TIMESLICE
mdefine_line|#define DEF_TIMESLICE&t;&t;(100 * HZ / 1000)
DECL|macro|ON_RUNQUEUE_WEIGHT
mdefine_line|#define ON_RUNQUEUE_WEIGHT&t; 30
DECL|macro|CHILD_PENALTY
mdefine_line|#define CHILD_PENALTY&t;&t; 95
DECL|macro|PARENT_PENALTY
mdefine_line|#define PARENT_PENALTY&t;&t;100
DECL|macro|EXIT_WEIGHT
mdefine_line|#define EXIT_WEIGHT&t;&t;  3
DECL|macro|PRIO_BONUS_RATIO
mdefine_line|#define PRIO_BONUS_RATIO&t; 25
DECL|macro|MAX_BONUS
mdefine_line|#define MAX_BONUS&t;&t;(MAX_USER_PRIO * PRIO_BONUS_RATIO / 100)
DECL|macro|INTERACTIVE_DELTA
mdefine_line|#define INTERACTIVE_DELTA&t;  2
DECL|macro|MAX_SLEEP_AVG
mdefine_line|#define MAX_SLEEP_AVG&t;&t;(DEF_TIMESLICE * MAX_BONUS)
DECL|macro|STARVATION_LIMIT
mdefine_line|#define STARVATION_LIMIT&t;(MAX_SLEEP_AVG)
DECL|macro|NS_MAX_SLEEP_AVG
mdefine_line|#define NS_MAX_SLEEP_AVG&t;(JIFFIES_TO_NS(MAX_SLEEP_AVG))
multiline_comment|/*&n; * If a task is &squot;interactive&squot; then we reinsert it in the active&n; * array after it has expired its current timeslice. (it will not&n; * continue to run immediately, it will still roundrobin with&n; * other interactive tasks.)&n; *&n; * This part scales the interactivity limit depending on niceness.&n; *&n; * We scale it linearly, offset by the INTERACTIVE_DELTA delta.&n; * Here are a few examples of different nice levels:&n; *&n; *  TASK_INTERACTIVE(-20): [1,1,1,1,1,1,1,1,1,0,0]&n; *  TASK_INTERACTIVE(-10): [1,1,1,1,1,1,1,0,0,0,0]&n; *  TASK_INTERACTIVE(  0): [1,1,1,1,0,0,0,0,0,0,0]&n; *  TASK_INTERACTIVE( 10): [1,1,0,0,0,0,0,0,0,0,0]&n; *  TASK_INTERACTIVE( 19): [0,0,0,0,0,0,0,0,0,0,0]&n; *&n; * (the X axis represents the possible -5 ... 0 ... +5 dynamic&n; *  priority range a task can explore, a value of &squot;1&squot; means the&n; *  task is rated interactive.)&n; *&n; * Ie. nice +19 tasks can never get &squot;interactive&squot; enough to be&n; * reinserted into the active array. And only heavily CPU-hog nice -20&n; * tasks will be expired. Default nice 0 tasks are somewhere between,&n; * it takes some effort for them to get interactive, but it&squot;s not&n; * too hard.&n; */
DECL|macro|CURRENT_BONUS
mdefine_line|#define CURRENT_BONUS(p) &bslash;&n;&t;(NS_TO_JIFFIES((p)-&gt;sleep_avg) * MAX_BONUS / &bslash;&n;&t;&t;MAX_SLEEP_AVG)
DECL|macro|GRANULARITY
mdefine_line|#define GRANULARITY&t;(10 * HZ / 1000 ? : 1)
macro_line|#ifdef CONFIG_SMP
DECL|macro|TIMESLICE_GRANULARITY
mdefine_line|#define TIMESLICE_GRANULARITY(p)&t;(GRANULARITY * &bslash;&n;&t;&t;(1 &lt;&lt; (((MAX_BONUS - CURRENT_BONUS(p)) ? : 1) - 1)) * &bslash;&n;&t;&t;&t;num_online_cpus())
macro_line|#else
DECL|macro|TIMESLICE_GRANULARITY
mdefine_line|#define TIMESLICE_GRANULARITY(p)&t;(GRANULARITY * &bslash;&n;&t;&t;(1 &lt;&lt; (((MAX_BONUS - CURRENT_BONUS(p)) ? : 1) - 1)))
macro_line|#endif
DECL|macro|SCALE
mdefine_line|#define SCALE(v1,v1_max,v2_max) &bslash;&n;&t;(v1) * (v2_max) / (v1_max)
DECL|macro|DELTA
mdefine_line|#define DELTA(p) &bslash;&n;&t;(SCALE(TASK_NICE(p), 40, MAX_BONUS) + INTERACTIVE_DELTA)
DECL|macro|TASK_INTERACTIVE
mdefine_line|#define TASK_INTERACTIVE(p) &bslash;&n;&t;((p)-&gt;prio &lt;= (p)-&gt;static_prio - DELTA(p))
DECL|macro|INTERACTIVE_SLEEP
mdefine_line|#define INTERACTIVE_SLEEP(p) &bslash;&n;&t;(JIFFIES_TO_NS(MAX_SLEEP_AVG * &bslash;&n;&t;&t;(MAX_BONUS / 2 + DELTA((p)) + 1) / MAX_BONUS - 1))
DECL|macro|TASK_PREEMPTS_CURR
mdefine_line|#define TASK_PREEMPTS_CURR(p, rq) &bslash;&n;&t;((p)-&gt;prio &lt; (rq)-&gt;curr-&gt;prio)
multiline_comment|/*&n; * task_timeslice() scales user-nice values [ -20 ... 0 ... 19 ]&n; * to time slice values: [800ms ... 100ms ... 5ms]&n; *&n; * The higher a thread&squot;s priority, the bigger timeslices&n; * it gets during one round of execution. But even the lowest&n; * priority thread gets MIN_TIMESLICE worth of execution time.&n; */
DECL|macro|SCALE_PRIO
mdefine_line|#define SCALE_PRIO(x, prio) &bslash;&n;&t;max(x * (MAX_PRIO - prio) / (MAX_USER_PRIO/2), MIN_TIMESLICE)
DECL|function|task_timeslice
r_static
r_int
r_int
id|task_timeslice
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
r_if
c_cond
(paren
id|p-&gt;static_prio
OL
id|NICE_TO_PRIO
c_func
(paren
l_int|0
)paren
)paren
r_return
id|SCALE_PRIO
c_func
(paren
id|DEF_TIMESLICE
op_star
l_int|4
comma
id|p-&gt;static_prio
)paren
suffix:semicolon
r_else
r_return
id|SCALE_PRIO
c_func
(paren
id|DEF_TIMESLICE
comma
id|p-&gt;static_prio
)paren
suffix:semicolon
)brace
DECL|macro|task_hot
mdefine_line|#define task_hot(p, now, sd) ((long long) ((now) - (p)-&gt;last_ran)&t;&bslash;&n;&t;&t;&t;&t;&lt; (long long) (sd)-&gt;cache_hot_time)
multiline_comment|/*&n; * These are the runqueue data structures:&n; */
DECL|macro|BITMAP_SIZE
mdefine_line|#define BITMAP_SIZE ((((MAX_PRIO+1+7)/8)+sizeof(long)-1)/sizeof(long))
DECL|typedef|runqueue_t
r_typedef
r_struct
id|runqueue
id|runqueue_t
suffix:semicolon
DECL|struct|prio_array
r_struct
id|prio_array
(brace
DECL|member|nr_active
r_int
r_int
id|nr_active
suffix:semicolon
DECL|member|bitmap
r_int
r_int
id|bitmap
(braket
id|BITMAP_SIZE
)braket
suffix:semicolon
DECL|member|queue
r_struct
id|list_head
id|queue
(braket
id|MAX_PRIO
)braket
suffix:semicolon
)brace
suffix:semicolon
multiline_comment|/*&n; * This is the main, per-CPU runqueue data structure.&n; *&n; * Locking rule: those places that want to lock multiple runqueues&n; * (such as the load balancing or the thread migration code), lock&n; * acquire operations must be ordered by ascending &amp;runqueue.&n; */
DECL|struct|runqueue
r_struct
id|runqueue
(brace
DECL|member|lock
id|spinlock_t
id|lock
suffix:semicolon
multiline_comment|/*&n;&t; * nr_running and cpu_load should be in the same cacheline because&n;&t; * remote CPUs use both these fields when doing load calculation.&n;&t; */
DECL|member|nr_running
r_int
r_int
id|nr_running
suffix:semicolon
macro_line|#ifdef CONFIG_SMP
DECL|member|cpu_load
r_int
r_int
id|cpu_load
suffix:semicolon
macro_line|#endif
DECL|member|nr_switches
r_int
r_int
r_int
id|nr_switches
suffix:semicolon
multiline_comment|/*&n;&t; * This is part of a global counter where only the total sum&n;&t; * over all CPUs matters. A task can increase this counter on&n;&t; * one CPU and if it got migrated afterwards it may decrease&n;&t; * it on another CPU. Always updated under the runqueue lock:&n;&t; */
DECL|member|nr_uninterruptible
r_int
r_int
id|nr_uninterruptible
suffix:semicolon
DECL|member|expired_timestamp
r_int
r_int
id|expired_timestamp
suffix:semicolon
DECL|member|timestamp_last_tick
r_int
r_int
r_int
id|timestamp_last_tick
suffix:semicolon
DECL|member|curr
DECL|member|idle
id|task_t
op_star
id|curr
comma
op_star
id|idle
suffix:semicolon
DECL|member|prev_mm
r_struct
id|mm_struct
op_star
id|prev_mm
suffix:semicolon
DECL|member|active
DECL|member|expired
DECL|member|arrays
id|prio_array_t
op_star
id|active
comma
op_star
id|expired
comma
id|arrays
(braket
l_int|2
)braket
suffix:semicolon
DECL|member|best_expired_prio
r_int
id|best_expired_prio
suffix:semicolon
DECL|member|nr_iowait
id|atomic_t
id|nr_iowait
suffix:semicolon
macro_line|#ifdef CONFIG_SMP
DECL|member|sd
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
multiline_comment|/* For active balancing */
DECL|member|active_balance
r_int
id|active_balance
suffix:semicolon
DECL|member|push_cpu
r_int
id|push_cpu
suffix:semicolon
DECL|member|migration_thread
id|task_t
op_star
id|migration_thread
suffix:semicolon
DECL|member|migration_queue
r_struct
id|list_head
id|migration_queue
suffix:semicolon
macro_line|#endif
macro_line|#ifdef CONFIG_SCHEDSTATS
multiline_comment|/* latency stats */
DECL|member|rq_sched_info
r_struct
id|sched_info
id|rq_sched_info
suffix:semicolon
multiline_comment|/* sys_sched_yield() stats */
DECL|member|yld_exp_empty
r_int
r_int
id|yld_exp_empty
suffix:semicolon
DECL|member|yld_act_empty
r_int
r_int
id|yld_act_empty
suffix:semicolon
DECL|member|yld_both_empty
r_int
r_int
id|yld_both_empty
suffix:semicolon
DECL|member|yld_cnt
r_int
r_int
id|yld_cnt
suffix:semicolon
multiline_comment|/* schedule() stats */
DECL|member|sched_noswitch
r_int
r_int
id|sched_noswitch
suffix:semicolon
DECL|member|sched_switch
r_int
r_int
id|sched_switch
suffix:semicolon
DECL|member|sched_cnt
r_int
r_int
id|sched_cnt
suffix:semicolon
DECL|member|sched_goidle
r_int
r_int
id|sched_goidle
suffix:semicolon
multiline_comment|/* pull_task() stats */
DECL|member|pt_gained
r_int
r_int
id|pt_gained
(braket
id|MAX_IDLE_TYPES
)braket
suffix:semicolon
DECL|member|pt_lost
r_int
r_int
id|pt_lost
(braket
id|MAX_IDLE_TYPES
)braket
suffix:semicolon
multiline_comment|/* active_load_balance() stats */
DECL|member|alb_cnt
r_int
r_int
id|alb_cnt
suffix:semicolon
DECL|member|alb_lost
r_int
r_int
id|alb_lost
suffix:semicolon
DECL|member|alb_gained
r_int
r_int
id|alb_gained
suffix:semicolon
DECL|member|alb_failed
r_int
r_int
id|alb_failed
suffix:semicolon
multiline_comment|/* try_to_wake_up() stats */
DECL|member|ttwu_cnt
r_int
r_int
id|ttwu_cnt
suffix:semicolon
DECL|member|ttwu_attempts
r_int
r_int
id|ttwu_attempts
suffix:semicolon
DECL|member|ttwu_moved
r_int
r_int
id|ttwu_moved
suffix:semicolon
multiline_comment|/* wake_up_new_task() stats */
DECL|member|wunt_cnt
r_int
r_int
id|wunt_cnt
suffix:semicolon
DECL|member|wunt_moved
r_int
r_int
id|wunt_moved
suffix:semicolon
multiline_comment|/* sched_migrate_task() stats */
DECL|member|smt_cnt
r_int
r_int
id|smt_cnt
suffix:semicolon
multiline_comment|/* sched_balance_exec() stats */
DECL|member|sbe_cnt
r_int
r_int
id|sbe_cnt
suffix:semicolon
macro_line|#endif
)brace
suffix:semicolon
r_static
id|DEFINE_PER_CPU
c_func
(paren
r_struct
id|runqueue
comma
id|runqueues
)paren
suffix:semicolon
DECL|macro|for_each_domain
mdefine_line|#define for_each_domain(cpu, domain) &bslash;&n;&t;for (domain = cpu_rq(cpu)-&gt;sd; domain; domain = domain-&gt;parent)
DECL|macro|cpu_rq
mdefine_line|#define cpu_rq(cpu)&t;&t;(&amp;per_cpu(runqueues, (cpu)))
DECL|macro|this_rq
mdefine_line|#define this_rq()&t;&t;(&amp;__get_cpu_var(runqueues))
DECL|macro|task_rq
mdefine_line|#define task_rq(p)&t;&t;cpu_rq(task_cpu(p))
DECL|macro|cpu_curr
mdefine_line|#define cpu_curr(cpu)&t;&t;(cpu_rq(cpu)-&gt;curr)
multiline_comment|/*&n; * Default context-switch locking:&n; */
macro_line|#ifndef prepare_arch_switch
DECL|macro|prepare_arch_switch
macro_line|# define prepare_arch_switch(rq, next)&t;do { } while (0)
DECL|macro|finish_arch_switch
macro_line|# define finish_arch_switch(rq, next)&t;spin_unlock_irq(&amp;(rq)-&gt;lock)
DECL|macro|task_running
macro_line|# define task_running(rq, p)&t;&t;((rq)-&gt;curr == (p))
macro_line|#endif
multiline_comment|/*&n; * task_rq_lock - lock the runqueue a given task resides on and disable&n; * interrupts.  Note the ordering: we can safely lookup the task_rq without&n; * explicitly disabling preemption.&n; */
r_static
id|runqueue_t
op_star
id|task_rq_lock
c_func
(paren
id|task_t
op_star
id|p
comma
r_int
r_int
op_star
id|flags
)paren
id|__acquires
c_func
(paren
id|rq-&gt;lock
)paren
(brace
r_struct
id|runqueue
op_star
id|rq
suffix:semicolon
id|repeat_lock_task
suffix:colon
id|local_irq_save
c_func
(paren
op_star
id|flags
)paren
suffix:semicolon
id|rq
op_assign
id|task_rq
c_func
(paren
id|p
)paren
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|rq
op_ne
id|task_rq
c_func
(paren
id|p
)paren
)paren
)paren
(brace
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|rq-&gt;lock
comma
op_star
id|flags
)paren
suffix:semicolon
r_goto
id|repeat_lock_task
suffix:semicolon
)brace
r_return
id|rq
suffix:semicolon
)brace
r_static
r_inline
r_void
id|task_rq_unlock
c_func
(paren
id|runqueue_t
op_star
id|rq
comma
r_int
r_int
op_star
id|flags
)paren
id|__releases
c_func
(paren
id|rq-&gt;lock
)paren
(brace
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|rq-&gt;lock
comma
op_star
id|flags
)paren
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_SCHEDSTATS
multiline_comment|/*&n; * bump this up when changing the output format or the meaning of an existing&n; * format, so that tools can adapt (or abort)&n; */
DECL|macro|SCHEDSTAT_VERSION
mdefine_line|#define SCHEDSTAT_VERSION 10
DECL|function|show_schedstat
r_static
r_int
id|show_schedstat
c_func
(paren
r_struct
id|seq_file
op_star
id|seq
comma
r_void
op_star
id|v
)paren
(brace
r_int
id|cpu
suffix:semicolon
r_enum
id|idle_type
id|itype
suffix:semicolon
id|seq_printf
c_func
(paren
id|seq
comma
l_string|&quot;version %d&bslash;n&quot;
comma
id|SCHEDSTAT_VERSION
)paren
suffix:semicolon
id|seq_printf
c_func
(paren
id|seq
comma
l_string|&quot;timestamp %lu&bslash;n&quot;
comma
id|jiffies
)paren
suffix:semicolon
id|for_each_online_cpu
c_func
(paren
id|cpu
)paren
(brace
id|runqueue_t
op_star
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|cpu
)paren
suffix:semicolon
macro_line|#ifdef CONFIG_SMP
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
r_int
id|dcnt
op_assign
l_int|0
suffix:semicolon
macro_line|#endif
multiline_comment|/* runqueue-specific stats */
id|seq_printf
c_func
(paren
id|seq
comma
l_string|&quot;cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu &quot;
l_string|&quot;%lu %lu %lu %lu %lu %lu %lu %lu %lu %lu&quot;
comma
id|cpu
comma
id|rq-&gt;yld_both_empty
comma
id|rq-&gt;yld_act_empty
comma
id|rq-&gt;yld_exp_empty
comma
id|rq-&gt;yld_cnt
comma
id|rq-&gt;sched_noswitch
comma
id|rq-&gt;sched_switch
comma
id|rq-&gt;sched_cnt
comma
id|rq-&gt;sched_goidle
comma
id|rq-&gt;alb_cnt
comma
id|rq-&gt;alb_gained
comma
id|rq-&gt;alb_lost
comma
id|rq-&gt;alb_failed
comma
id|rq-&gt;ttwu_cnt
comma
id|rq-&gt;ttwu_moved
comma
id|rq-&gt;ttwu_attempts
comma
id|rq-&gt;wunt_cnt
comma
id|rq-&gt;wunt_moved
comma
id|rq-&gt;smt_cnt
comma
id|rq-&gt;sbe_cnt
comma
id|rq-&gt;rq_sched_info.cpu_time
comma
id|rq-&gt;rq_sched_info.run_delay
comma
id|rq-&gt;rq_sched_info.pcnt
)paren
suffix:semicolon
r_for
c_loop
(paren
id|itype
op_assign
id|SCHED_IDLE
suffix:semicolon
id|itype
OL
id|MAX_IDLE_TYPES
suffix:semicolon
id|itype
op_increment
)paren
id|seq_printf
c_func
(paren
id|seq
comma
l_string|&quot; %lu %lu&quot;
comma
id|rq-&gt;pt_gained
(braket
id|itype
)braket
comma
id|rq-&gt;pt_lost
(braket
id|itype
)braket
)paren
suffix:semicolon
id|seq_printf
c_func
(paren
id|seq
comma
l_string|&quot;&bslash;n&quot;
)paren
suffix:semicolon
macro_line|#ifdef CONFIG_SMP
multiline_comment|/* domain-specific stats */
id|for_each_domain
c_func
(paren
id|cpu
comma
id|sd
)paren
(brace
r_char
id|mask_str
(braket
id|NR_CPUS
)braket
suffix:semicolon
id|cpumask_scnprintf
c_func
(paren
id|mask_str
comma
id|NR_CPUS
comma
id|sd-&gt;span
)paren
suffix:semicolon
id|seq_printf
c_func
(paren
id|seq
comma
l_string|&quot;domain%d %s&quot;
comma
id|dcnt
op_increment
comma
id|mask_str
)paren
suffix:semicolon
r_for
c_loop
(paren
id|itype
op_assign
id|SCHED_IDLE
suffix:semicolon
id|itype
OL
id|MAX_IDLE_TYPES
suffix:semicolon
id|itype
op_increment
)paren
(brace
id|seq_printf
c_func
(paren
id|seq
comma
l_string|&quot; %lu %lu %lu %lu %lu&quot;
comma
id|sd-&gt;lb_cnt
(braket
id|itype
)braket
comma
id|sd-&gt;lb_failed
(braket
id|itype
)braket
comma
id|sd-&gt;lb_imbalance
(braket
id|itype
)braket
comma
id|sd-&gt;lb_nobusyq
(braket
id|itype
)braket
comma
id|sd-&gt;lb_nobusyg
(braket
id|itype
)braket
)paren
suffix:semicolon
)brace
id|seq_printf
c_func
(paren
id|seq
comma
l_string|&quot; %lu %lu %lu %lu&bslash;n&quot;
comma
id|sd-&gt;sbe_pushed
comma
id|sd-&gt;sbe_attempts
comma
id|sd-&gt;ttwu_wake_affine
comma
id|sd-&gt;ttwu_wake_balance
)paren
suffix:semicolon
)brace
macro_line|#endif
)brace
r_return
l_int|0
suffix:semicolon
)brace
DECL|function|schedstat_open
r_static
r_int
id|schedstat_open
c_func
(paren
r_struct
id|inode
op_star
id|inode
comma
r_struct
id|file
op_star
id|file
)paren
(brace
r_int
r_int
id|size
op_assign
id|PAGE_SIZE
op_star
(paren
l_int|1
op_plus
id|num_online_cpus
c_func
(paren
)paren
op_div
l_int|32
)paren
suffix:semicolon
r_char
op_star
id|buf
op_assign
id|kmalloc
c_func
(paren
id|size
comma
id|GFP_KERNEL
)paren
suffix:semicolon
r_struct
id|seq_file
op_star
id|m
suffix:semicolon
r_int
id|res
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|buf
)paren
r_return
op_minus
id|ENOMEM
suffix:semicolon
id|res
op_assign
id|single_open
c_func
(paren
id|file
comma
id|show_schedstat
comma
l_int|NULL
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|res
)paren
(brace
id|m
op_assign
id|file-&gt;private_data
suffix:semicolon
id|m-&gt;buf
op_assign
id|buf
suffix:semicolon
id|m-&gt;size
op_assign
id|size
suffix:semicolon
)brace
r_else
id|kfree
c_func
(paren
id|buf
)paren
suffix:semicolon
r_return
id|res
suffix:semicolon
)brace
DECL|variable|proc_schedstat_operations
r_struct
id|file_operations
id|proc_schedstat_operations
op_assign
(brace
dot
id|open
op_assign
id|schedstat_open
comma
dot
id|read
op_assign
id|seq_read
comma
dot
id|llseek
op_assign
id|seq_lseek
comma
dot
id|release
op_assign
id|single_release
comma
)brace
suffix:semicolon
DECL|macro|schedstat_inc
macro_line|# define schedstat_inc(rq, field)&t;do { (rq)-&gt;field++; } while (0)
DECL|macro|schedstat_add
macro_line|# define schedstat_add(rq, field, amt)&t;do { (rq)-&gt;field += (amt); } while (0)
macro_line|#else /* !CONFIG_SCHEDSTATS */
DECL|macro|schedstat_inc
macro_line|# define schedstat_inc(rq, field)&t;do { } while (0)
DECL|macro|schedstat_add
macro_line|# define schedstat_add(rq, field, amt)&t;do { } while (0)
macro_line|#endif
multiline_comment|/*&n; * rq_lock - lock a given runqueue and disable interrupts.&n; */
r_static
id|runqueue_t
op_star
id|this_rq_lock
c_func
(paren
r_void
)paren
id|__acquires
c_func
(paren
id|rq-&gt;lock
)paren
(brace
id|runqueue_t
op_star
id|rq
suffix:semicolon
id|local_irq_disable
c_func
(paren
)paren
suffix:semicolon
id|rq
op_assign
id|this_rq
c_func
(paren
)paren
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
r_return
id|rq
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_SCHED_SMT
DECL|function|cpu_and_siblings_are_idle
r_static
r_int
id|cpu_and_siblings_are_idle
c_func
(paren
r_int
id|cpu
)paren
(brace
r_int
id|sib
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|sib
comma
id|cpu_sibling_map
(braket
id|cpu
)braket
)paren
(brace
r_if
c_cond
(paren
id|idle_cpu
c_func
(paren
id|sib
)paren
)paren
r_continue
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
r_return
l_int|1
suffix:semicolon
)brace
macro_line|#else
DECL|macro|cpu_and_siblings_are_idle
mdefine_line|#define cpu_and_siblings_are_idle(A) idle_cpu(A)
macro_line|#endif
macro_line|#ifdef CONFIG_SCHEDSTATS
multiline_comment|/*&n; * Called when a process is dequeued from the active array and given&n; * the cpu.  We should note that with the exception of interactive&n; * tasks, the expired queue will become the active queue after the active&n; * queue is empty, without explicitly dequeuing and requeuing tasks in the&n; * expired queue.  (Interactive tasks may be requeued directly to the&n; * active queue, thus delaying tasks in the expired queue from running;&n; * see scheduler_tick()).&n; *&n; * This function is only called from sched_info_arrive(), rather than&n; * dequeue_task(). Even though a task may be queued and dequeued multiple&n; * times as it is shuffled about, we&squot;re really interested in knowing how&n; * long it was from the *first* time it was queued to the time that it&n; * finally hit a cpu.&n; */
DECL|function|sched_info_dequeued
r_static
r_inline
r_void
id|sched_info_dequeued
c_func
(paren
id|task_t
op_star
id|t
)paren
(brace
id|t-&gt;sched_info.last_queued
op_assign
l_int|0
suffix:semicolon
)brace
multiline_comment|/*&n; * Called when a task finally hits the cpu.  We can now calculate how&n; * long it was waiting to run.  We also note when it began so that we&n; * can keep stats on how long its timeslice is.&n; */
DECL|function|sched_info_arrive
r_static
r_inline
r_void
id|sched_info_arrive
c_func
(paren
id|task_t
op_star
id|t
)paren
(brace
r_int
r_int
id|now
op_assign
id|jiffies
comma
id|diff
op_assign
l_int|0
suffix:semicolon
r_struct
id|runqueue
op_star
id|rq
op_assign
id|task_rq
c_func
(paren
id|t
)paren
suffix:semicolon
r_if
c_cond
(paren
id|t-&gt;sched_info.last_queued
)paren
id|diff
op_assign
id|now
op_minus
id|t-&gt;sched_info.last_queued
suffix:semicolon
id|sched_info_dequeued
c_func
(paren
id|t
)paren
suffix:semicolon
id|t-&gt;sched_info.run_delay
op_add_assign
id|diff
suffix:semicolon
id|t-&gt;sched_info.last_arrival
op_assign
id|now
suffix:semicolon
id|t-&gt;sched_info.pcnt
op_increment
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|rq
)paren
r_return
suffix:semicolon
id|rq-&gt;rq_sched_info.run_delay
op_add_assign
id|diff
suffix:semicolon
id|rq-&gt;rq_sched_info.pcnt
op_increment
suffix:semicolon
)brace
multiline_comment|/*&n; * Called when a process is queued into either the active or expired&n; * array.  The time is noted and later used to determine how long we&n; * had to wait for us to reach the cpu.  Since the expired queue will&n; * become the active queue after active queue is empty, without dequeuing&n; * and requeuing any tasks, we are interested in queuing to either. It&n; * is unusual but not impossible for tasks to be dequeued and immediately&n; * requeued in the same or another array: this can happen in sched_yield(),&n; * set_user_nice(), and even load_balance() as it moves tasks from runqueue&n; * to runqueue.&n; *&n; * This function is only called from enqueue_task(), but also only updates&n; * the timestamp if it is already not set.  It&squot;s assumed that&n; * sched_info_dequeued() will clear that stamp when appropriate.&n; */
DECL|function|sched_info_queued
r_static
r_inline
r_void
id|sched_info_queued
c_func
(paren
id|task_t
op_star
id|t
)paren
(brace
r_if
c_cond
(paren
op_logical_neg
id|t-&gt;sched_info.last_queued
)paren
id|t-&gt;sched_info.last_queued
op_assign
id|jiffies
suffix:semicolon
)brace
multiline_comment|/*&n; * Called when a process ceases being the active-running process, either&n; * voluntarily or involuntarily.  Now we can calculate how long we ran.&n; */
DECL|function|sched_info_depart
r_static
r_inline
r_void
id|sched_info_depart
c_func
(paren
id|task_t
op_star
id|t
)paren
(brace
r_struct
id|runqueue
op_star
id|rq
op_assign
id|task_rq
c_func
(paren
id|t
)paren
suffix:semicolon
r_int
r_int
id|diff
op_assign
id|jiffies
op_minus
id|t-&gt;sched_info.last_arrival
suffix:semicolon
id|t-&gt;sched_info.cpu_time
op_add_assign
id|diff
suffix:semicolon
r_if
c_cond
(paren
id|rq
)paren
id|rq-&gt;rq_sched_info.cpu_time
op_add_assign
id|diff
suffix:semicolon
)brace
multiline_comment|/*&n; * Called when tasks are switched involuntarily due, typically, to expiring&n; * their time slice.  (This may also be called when switching to or from&n; * the idle task.)  We are only called when prev != next.&n; */
DECL|function|sched_info_switch
r_static
r_inline
r_void
id|sched_info_switch
c_func
(paren
id|task_t
op_star
id|prev
comma
id|task_t
op_star
id|next
)paren
(brace
r_struct
id|runqueue
op_star
id|rq
op_assign
id|task_rq
c_func
(paren
id|prev
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * prev now departs the cpu.  It&squot;s not interesting to record&n;&t; * stats about how efficient we were at scheduling the idle&n;&t; * process, however.&n;&t; */
r_if
c_cond
(paren
id|prev
op_ne
id|rq-&gt;idle
)paren
id|sched_info_depart
c_func
(paren
id|prev
)paren
suffix:semicolon
r_if
c_cond
(paren
id|next
op_ne
id|rq-&gt;idle
)paren
id|sched_info_arrive
c_func
(paren
id|next
)paren
suffix:semicolon
)brace
macro_line|#else
DECL|macro|sched_info_queued
mdefine_line|#define sched_info_queued(t)&t;&t;do { } while (0)
DECL|macro|sched_info_switch
mdefine_line|#define sched_info_switch(t, next)&t;do { } while (0)
macro_line|#endif /* CONFIG_SCHEDSTATS */
multiline_comment|/*&n; * Adding/removing a task to/from a priority array:&n; */
DECL|function|dequeue_task
r_static
r_void
id|dequeue_task
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|prio_array_t
op_star
id|array
)paren
(brace
id|array-&gt;nr_active
op_decrement
suffix:semicolon
id|list_del
c_func
(paren
op_amp
id|p-&gt;run_list
)paren
suffix:semicolon
r_if
c_cond
(paren
id|list_empty
c_func
(paren
id|array-&gt;queue
op_plus
id|p-&gt;prio
)paren
)paren
id|__clear_bit
c_func
(paren
id|p-&gt;prio
comma
id|array-&gt;bitmap
)paren
suffix:semicolon
)brace
DECL|function|enqueue_task
r_static
r_void
id|enqueue_task
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|prio_array_t
op_star
id|array
)paren
(brace
id|sched_info_queued
c_func
(paren
id|p
)paren
suffix:semicolon
id|list_add_tail
c_func
(paren
op_amp
id|p-&gt;run_list
comma
id|array-&gt;queue
op_plus
id|p-&gt;prio
)paren
suffix:semicolon
id|__set_bit
c_func
(paren
id|p-&gt;prio
comma
id|array-&gt;bitmap
)paren
suffix:semicolon
id|array-&gt;nr_active
op_increment
suffix:semicolon
id|p-&gt;array
op_assign
id|array
suffix:semicolon
)brace
multiline_comment|/*&n; * Put task to the end of the run list without the overhead of dequeue&n; * followed by enqueue.&n; */
DECL|function|requeue_task
r_static
r_void
id|requeue_task
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|prio_array_t
op_star
id|array
)paren
(brace
id|list_move_tail
c_func
(paren
op_amp
id|p-&gt;run_list
comma
id|array-&gt;queue
op_plus
id|p-&gt;prio
)paren
suffix:semicolon
)brace
DECL|function|enqueue_task_head
r_static
r_inline
r_void
id|enqueue_task_head
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|prio_array_t
op_star
id|array
)paren
(brace
id|list_add
c_func
(paren
op_amp
id|p-&gt;run_list
comma
id|array-&gt;queue
op_plus
id|p-&gt;prio
)paren
suffix:semicolon
id|__set_bit
c_func
(paren
id|p-&gt;prio
comma
id|array-&gt;bitmap
)paren
suffix:semicolon
id|array-&gt;nr_active
op_increment
suffix:semicolon
id|p-&gt;array
op_assign
id|array
suffix:semicolon
)brace
multiline_comment|/*&n; * effective_prio - return the priority that is based on the static&n; * priority but is modified by bonuses/penalties.&n; *&n; * We scale the actual sleep average [0 .... MAX_SLEEP_AVG]&n; * into the -5 ... 0 ... +5 bonus/penalty range.&n; *&n; * We use 25% of the full 0...39 priority range so that:&n; *&n; * 1) nice +19 interactive tasks do not preempt nice 0 CPU hogs.&n; * 2) nice -20 CPU hogs do not get preempted by nice 0 tasks.&n; *&n; * Both properties are important to certain workloads.&n; */
DECL|function|effective_prio
r_static
r_int
id|effective_prio
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
r_int
id|bonus
comma
id|prio
suffix:semicolon
r_if
c_cond
(paren
id|rt_task
c_func
(paren
id|p
)paren
)paren
r_return
id|p-&gt;prio
suffix:semicolon
id|bonus
op_assign
id|CURRENT_BONUS
c_func
(paren
id|p
)paren
op_minus
id|MAX_BONUS
op_div
l_int|2
suffix:semicolon
id|prio
op_assign
id|p-&gt;static_prio
op_minus
id|bonus
suffix:semicolon
r_if
c_cond
(paren
id|prio
OL
id|MAX_RT_PRIO
)paren
id|prio
op_assign
id|MAX_RT_PRIO
suffix:semicolon
r_if
c_cond
(paren
id|prio
OG
id|MAX_PRIO
op_minus
l_int|1
)paren
id|prio
op_assign
id|MAX_PRIO
op_minus
l_int|1
suffix:semicolon
r_return
id|prio
suffix:semicolon
)brace
multiline_comment|/*&n; * __activate_task - move a task to the runqueue.&n; */
DECL|function|__activate_task
r_static
r_inline
r_void
id|__activate_task
c_func
(paren
id|task_t
op_star
id|p
comma
id|runqueue_t
op_star
id|rq
)paren
(brace
id|enqueue_task
c_func
(paren
id|p
comma
id|rq-&gt;active
)paren
suffix:semicolon
id|rq-&gt;nr_running
op_increment
suffix:semicolon
)brace
multiline_comment|/*&n; * __activate_idle_task - move idle task to the _front_ of runqueue.&n; */
DECL|function|__activate_idle_task
r_static
r_inline
r_void
id|__activate_idle_task
c_func
(paren
id|task_t
op_star
id|p
comma
id|runqueue_t
op_star
id|rq
)paren
(brace
id|enqueue_task_head
c_func
(paren
id|p
comma
id|rq-&gt;active
)paren
suffix:semicolon
id|rq-&gt;nr_running
op_increment
suffix:semicolon
)brace
DECL|function|recalc_task_prio
r_static
r_void
id|recalc_task_prio
c_func
(paren
id|task_t
op_star
id|p
comma
r_int
r_int
r_int
id|now
)paren
(brace
r_int
r_int
r_int
id|__sleep_time
op_assign
id|now
op_minus
id|p-&gt;timestamp
suffix:semicolon
r_int
r_int
id|sleep_time
suffix:semicolon
r_if
c_cond
(paren
id|__sleep_time
OG
id|NS_MAX_SLEEP_AVG
)paren
id|sleep_time
op_assign
id|NS_MAX_SLEEP_AVG
suffix:semicolon
r_else
id|sleep_time
op_assign
(paren
r_int
r_int
)paren
id|__sleep_time
suffix:semicolon
r_if
c_cond
(paren
id|likely
c_func
(paren
id|sleep_time
OG
l_int|0
)paren
)paren
(brace
multiline_comment|/*&n;&t;&t; * User tasks that sleep a long time are categorised as&n;&t;&t; * idle and will get just interactive status to stay active &amp;&n;&t;&t; * prevent them suddenly becoming cpu hogs and starving&n;&t;&t; * other processes.&n;&t;&t; */
r_if
c_cond
(paren
id|p-&gt;mm
op_logical_and
id|p-&gt;activated
op_ne
op_minus
l_int|1
op_logical_and
id|sleep_time
OG
id|INTERACTIVE_SLEEP
c_func
(paren
id|p
)paren
)paren
(brace
id|p-&gt;sleep_avg
op_assign
id|JIFFIES_TO_NS
c_func
(paren
id|MAX_SLEEP_AVG
op_minus
id|DEF_TIMESLICE
)paren
suffix:semicolon
)brace
r_else
(brace
multiline_comment|/*&n;&t;&t;&t; * The lower the sleep avg a task has the more&n;&t;&t;&t; * rapidly it will rise with sleep time.&n;&t;&t;&t; */
id|sleep_time
op_mul_assign
(paren
id|MAX_BONUS
op_minus
id|CURRENT_BONUS
c_func
(paren
id|p
)paren
)paren
ques
c_cond
suffix:colon
l_int|1
suffix:semicolon
multiline_comment|/*&n;&t;&t;&t; * Tasks waking from uninterruptible sleep are&n;&t;&t;&t; * limited in their sleep_avg rise as they&n;&t;&t;&t; * are likely to be waiting on I/O&n;&t;&t;&t; */
r_if
c_cond
(paren
id|p-&gt;activated
op_eq
op_minus
l_int|1
op_logical_and
id|p-&gt;mm
)paren
(brace
r_if
c_cond
(paren
id|p-&gt;sleep_avg
op_ge
id|INTERACTIVE_SLEEP
c_func
(paren
id|p
)paren
)paren
id|sleep_time
op_assign
l_int|0
suffix:semicolon
r_else
r_if
c_cond
(paren
id|p-&gt;sleep_avg
op_plus
id|sleep_time
op_ge
id|INTERACTIVE_SLEEP
c_func
(paren
id|p
)paren
)paren
(brace
id|p-&gt;sleep_avg
op_assign
id|INTERACTIVE_SLEEP
c_func
(paren
id|p
)paren
suffix:semicolon
id|sleep_time
op_assign
l_int|0
suffix:semicolon
)brace
)brace
multiline_comment|/*&n;&t;&t;&t; * This code gives a bonus to interactive tasks.&n;&t;&t;&t; *&n;&t;&t;&t; * The boost works by updating the &squot;average sleep time&squot;&n;&t;&t;&t; * value here, based on -&gt;timestamp. The more time a&n;&t;&t;&t; * task spends sleeping, the higher the average gets -&n;&t;&t;&t; * and the higher the priority boost gets as well.&n;&t;&t;&t; */
id|p-&gt;sleep_avg
op_add_assign
id|sleep_time
suffix:semicolon
r_if
c_cond
(paren
id|p-&gt;sleep_avg
OG
id|NS_MAX_SLEEP_AVG
)paren
id|p-&gt;sleep_avg
op_assign
id|NS_MAX_SLEEP_AVG
suffix:semicolon
)brace
)brace
id|p-&gt;prio
op_assign
id|effective_prio
c_func
(paren
id|p
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * activate_task - move a task to the runqueue and do priority recalculation&n; *&n; * Update all the scheduling statistics stuff. (sleep average&n; * calculation, priority modifiers, etc.)&n; */
DECL|function|activate_task
r_static
r_void
id|activate_task
c_func
(paren
id|task_t
op_star
id|p
comma
id|runqueue_t
op_star
id|rq
comma
r_int
id|local
)paren
(brace
r_int
r_int
r_int
id|now
suffix:semicolon
id|now
op_assign
id|sched_clock
c_func
(paren
)paren
suffix:semicolon
macro_line|#ifdef CONFIG_SMP
r_if
c_cond
(paren
op_logical_neg
id|local
)paren
(brace
multiline_comment|/* Compensate for drifting sched_clock */
id|runqueue_t
op_star
id|this_rq
op_assign
id|this_rq
c_func
(paren
)paren
suffix:semicolon
id|now
op_assign
(paren
id|now
op_minus
id|this_rq-&gt;timestamp_last_tick
)paren
op_plus
id|rq-&gt;timestamp_last_tick
suffix:semicolon
)brace
macro_line|#endif
id|recalc_task_prio
c_func
(paren
id|p
comma
id|now
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * This checks to make sure it&squot;s not an uninterruptible task&n;&t; * that is now waking up.&n;&t; */
r_if
c_cond
(paren
op_logical_neg
id|p-&gt;activated
)paren
(brace
multiline_comment|/*&n;&t;&t; * Tasks which were woken up by interrupts (ie. hw events)&n;&t;&t; * are most likely of interactive nature. So we give them&n;&t;&t; * the credit of extending their sleep time to the period&n;&t;&t; * of time they spend on the runqueue, waiting for execution&n;&t;&t; * on a CPU, first time around:&n;&t;&t; */
r_if
c_cond
(paren
id|in_interrupt
c_func
(paren
)paren
)paren
id|p-&gt;activated
op_assign
l_int|2
suffix:semicolon
r_else
(brace
multiline_comment|/*&n;&t;&t;&t; * Normal first-time wakeups get a credit too for&n;&t;&t;&t; * on-runqueue time, but it will be weighted down:&n;&t;&t;&t; */
id|p-&gt;activated
op_assign
l_int|1
suffix:semicolon
)brace
)brace
id|p-&gt;timestamp
op_assign
id|now
suffix:semicolon
id|__activate_task
c_func
(paren
id|p
comma
id|rq
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * deactivate_task - remove a task from the runqueue.&n; */
DECL|function|deactivate_task
r_static
r_void
id|deactivate_task
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|runqueue_t
op_star
id|rq
)paren
(brace
id|rq-&gt;nr_running
op_decrement
suffix:semicolon
id|dequeue_task
c_func
(paren
id|p
comma
id|p-&gt;array
)paren
suffix:semicolon
id|p-&gt;array
op_assign
l_int|NULL
suffix:semicolon
)brace
multiline_comment|/*&n; * resched_task - mark a task &squot;to be rescheduled now&squot;.&n; *&n; * On UP this means the setting of the need_resched flag, on SMP it&n; * might also involve a cross-CPU call to trigger the scheduler on&n; * the target CPU.&n; */
macro_line|#ifdef CONFIG_SMP
DECL|function|resched_task
r_static
r_void
id|resched_task
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
r_int
id|need_resched
comma
id|nrpolling
suffix:semicolon
id|BUG_ON
c_func
(paren
op_logical_neg
id|spin_is_locked
c_func
(paren
op_amp
id|task_rq
c_func
(paren
id|p
)paren
op_member_access_from_pointer
id|lock
)paren
)paren
suffix:semicolon
multiline_comment|/* minimise the chance of sending an interrupt to poll_idle() */
id|nrpolling
op_assign
id|test_tsk_thread_flag
c_func
(paren
id|p
comma
id|TIF_POLLING_NRFLAG
)paren
suffix:semicolon
id|need_resched
op_assign
id|test_and_set_tsk_thread_flag
c_func
(paren
id|p
comma
id|TIF_NEED_RESCHED
)paren
suffix:semicolon
id|nrpolling
op_or_assign
id|test_tsk_thread_flag
c_func
(paren
id|p
comma
id|TIF_POLLING_NRFLAG
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|need_resched
op_logical_and
op_logical_neg
id|nrpolling
op_logical_and
(paren
id|task_cpu
c_func
(paren
id|p
)paren
op_ne
id|smp_processor_id
c_func
(paren
)paren
)paren
)paren
id|smp_send_reschedule
c_func
(paren
id|task_cpu
c_func
(paren
id|p
)paren
)paren
suffix:semicolon
)brace
macro_line|#else
DECL|function|resched_task
r_static
r_inline
r_void
id|resched_task
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
id|set_tsk_need_resched
c_func
(paren
id|p
)paren
suffix:semicolon
)brace
macro_line|#endif
multiline_comment|/**&n; * task_curr - is this task currently executing on a CPU?&n; * @p: the task in question.&n; */
DECL|function|task_curr
r_inline
r_int
id|task_curr
c_func
(paren
r_const
id|task_t
op_star
id|p
)paren
(brace
r_return
id|cpu_curr
c_func
(paren
id|task_cpu
c_func
(paren
id|p
)paren
)paren
op_eq
id|p
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_SMP
DECL|enum|request_type
r_enum
id|request_type
(brace
DECL|enumerator|REQ_MOVE_TASK
id|REQ_MOVE_TASK
comma
DECL|enumerator|REQ_SET_DOMAIN
id|REQ_SET_DOMAIN
comma
)brace
suffix:semicolon
r_typedef
r_struct
(brace
DECL|member|list
r_struct
id|list_head
id|list
suffix:semicolon
DECL|member|type
r_enum
id|request_type
id|type
suffix:semicolon
multiline_comment|/* For REQ_MOVE_TASK */
DECL|member|task
id|task_t
op_star
id|task
suffix:semicolon
DECL|member|dest_cpu
r_int
id|dest_cpu
suffix:semicolon
multiline_comment|/* For REQ_SET_DOMAIN */
DECL|member|sd
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
DECL|member|done
r_struct
id|completion
id|done
suffix:semicolon
DECL|typedef|migration_req_t
)brace
id|migration_req_t
suffix:semicolon
multiline_comment|/*&n; * The task&squot;s runqueue lock must be held.&n; * Returns true if you have to wait for migration thread.&n; */
DECL|function|migrate_task
r_static
r_int
id|migrate_task
c_func
(paren
id|task_t
op_star
id|p
comma
r_int
id|dest_cpu
comma
id|migration_req_t
op_star
id|req
)paren
(brace
id|runqueue_t
op_star
id|rq
op_assign
id|task_rq
c_func
(paren
id|p
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * If the task is not on a runqueue (and not running), then&n;&t; * it is sufficient to simply update the task&squot;s cpu field.&n;&t; */
r_if
c_cond
(paren
op_logical_neg
id|p-&gt;array
op_logical_and
op_logical_neg
id|task_running
c_func
(paren
id|rq
comma
id|p
)paren
)paren
(brace
id|set_task_cpu
c_func
(paren
id|p
comma
id|dest_cpu
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
id|init_completion
c_func
(paren
op_amp
id|req-&gt;done
)paren
suffix:semicolon
id|req-&gt;type
op_assign
id|REQ_MOVE_TASK
suffix:semicolon
id|req-&gt;task
op_assign
id|p
suffix:semicolon
id|req-&gt;dest_cpu
op_assign
id|dest_cpu
suffix:semicolon
id|list_add
c_func
(paren
op_amp
id|req-&gt;list
comma
op_amp
id|rq-&gt;migration_queue
)paren
suffix:semicolon
r_return
l_int|1
suffix:semicolon
)brace
multiline_comment|/*&n; * wait_task_inactive - wait for a thread to unschedule.&n; *&n; * The caller must ensure that the task *will* unschedule sometime soon,&n; * else this function might spin for a *long* time. This function can&squot;t&n; * be called with interrupts off, or it may introduce deadlock with&n; * smp_call_function() if an IPI is sent by the same process we are&n; * waiting to become inactive.&n; */
DECL|function|wait_task_inactive
r_void
id|wait_task_inactive
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
id|runqueue_t
op_star
id|rq
suffix:semicolon
r_int
id|preempted
suffix:semicolon
id|repeat
suffix:colon
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
multiline_comment|/* Must be off runqueue entirely, not preempted. */
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|p-&gt;array
)paren
)paren
(brace
multiline_comment|/* If it&squot;s preempted, we yield.  It could be a while. */
id|preempted
op_assign
op_logical_neg
id|task_running
c_func
(paren
id|rq
comma
id|p
)paren
suffix:semicolon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
id|cpu_relax
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|preempted
)paren
id|yield
c_func
(paren
)paren
suffix:semicolon
r_goto
id|repeat
suffix:semicolon
)brace
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
)brace
multiline_comment|/***&n; * kick_process - kick a running thread to enter/exit the kernel&n; * @p: the to-be-kicked thread&n; *&n; * Cause a process which is running on another CPU to enter&n; * kernel-mode, without any delay. (to get signals handled.)&n; */
DECL|function|kick_process
r_void
id|kick_process
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
r_int
id|cpu
suffix:semicolon
id|preempt_disable
c_func
(paren
)paren
suffix:semicolon
id|cpu
op_assign
id|task_cpu
c_func
(paren
id|p
)paren
suffix:semicolon
r_if
c_cond
(paren
(paren
id|cpu
op_ne
id|smp_processor_id
c_func
(paren
)paren
)paren
op_logical_and
id|task_curr
c_func
(paren
id|p
)paren
)paren
id|smp_send_reschedule
c_func
(paren
id|cpu
)paren
suffix:semicolon
id|preempt_enable
c_func
(paren
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Return a low guess at the load of a migration-source cpu.&n; *&n; * We want to under-estimate the load of migration sources, to&n; * balance conservatively.&n; */
DECL|function|source_load
r_static
r_inline
r_int
r_int
id|source_load
c_func
(paren
r_int
id|cpu
)paren
(brace
id|runqueue_t
op_star
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|cpu
)paren
suffix:semicolon
r_int
r_int
id|load_now
op_assign
id|rq-&gt;nr_running
op_star
id|SCHED_LOAD_SCALE
suffix:semicolon
r_return
id|min
c_func
(paren
id|rq-&gt;cpu_load
comma
id|load_now
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Return a high guess at the load of a migration-target cpu&n; */
DECL|function|target_load
r_static
r_inline
r_int
r_int
id|target_load
c_func
(paren
r_int
id|cpu
)paren
(brace
id|runqueue_t
op_star
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|cpu
)paren
suffix:semicolon
r_int
r_int
id|load_now
op_assign
id|rq-&gt;nr_running
op_star
id|SCHED_LOAD_SCALE
suffix:semicolon
r_return
id|max
c_func
(paren
id|rq-&gt;cpu_load
comma
id|load_now
)paren
suffix:semicolon
)brace
macro_line|#endif
multiline_comment|/*&n; * wake_idle() will wake a task on an idle cpu if task-&gt;cpu is&n; * not idle and an idle cpu is available.  The span of cpus to&n; * search starts with cpus closest then further out as needed,&n; * so we always favor a closer, idle cpu.&n; *&n; * Returns the CPU we should wake onto.&n; */
macro_line|#if defined(ARCH_HAS_SCHED_WAKE_IDLE)
DECL|function|wake_idle
r_static
r_int
id|wake_idle
c_func
(paren
r_int
id|cpu
comma
id|task_t
op_star
id|p
)paren
(brace
id|cpumask_t
id|tmp
suffix:semicolon
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
r_int
id|i
suffix:semicolon
r_if
c_cond
(paren
id|idle_cpu
c_func
(paren
id|cpu
)paren
)paren
r_return
id|cpu
suffix:semicolon
id|for_each_domain
c_func
(paren
id|cpu
comma
id|sd
)paren
(brace
r_if
c_cond
(paren
id|sd-&gt;flags
op_amp
id|SD_WAKE_IDLE
)paren
(brace
id|cpus_and
c_func
(paren
id|tmp
comma
id|sd-&gt;span
comma
id|cpu_online_map
)paren
suffix:semicolon
id|cpus_and
c_func
(paren
id|tmp
comma
id|tmp
comma
id|p-&gt;cpus_allowed
)paren
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|tmp
)paren
(brace
r_if
c_cond
(paren
id|idle_cpu
c_func
(paren
id|i
)paren
)paren
r_return
id|i
suffix:semicolon
)brace
)brace
r_else
r_break
suffix:semicolon
)brace
r_return
id|cpu
suffix:semicolon
)brace
macro_line|#else
DECL|function|wake_idle
r_static
r_inline
r_int
id|wake_idle
c_func
(paren
r_int
id|cpu
comma
id|task_t
op_star
id|p
)paren
(brace
r_return
id|cpu
suffix:semicolon
)brace
macro_line|#endif
multiline_comment|/***&n; * try_to_wake_up - wake up a thread&n; * @p: the to-be-woken-up thread&n; * @state: the mask of task states that can be woken&n; * @sync: do a synchronous wakeup?&n; *&n; * Put it on the run-queue if it&squot;s not already there. The &quot;current&quot;&n; * thread is always on the run-queue (except when the actual&n; * re-schedule is in progress), and as such you&squot;re allowed to do&n; * the simpler &quot;current-&gt;state = TASK_RUNNING&quot; to mark yourself&n; * runnable without the overhead of this.&n; *&n; * returns failure only if the task is already active.&n; */
DECL|function|try_to_wake_up
r_static
r_int
id|try_to_wake_up
c_func
(paren
id|task_t
op_star
id|p
comma
r_int
r_int
id|state
comma
r_int
id|sync
)paren
(brace
r_int
id|cpu
comma
id|this_cpu
comma
id|success
op_assign
l_int|0
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
r_int
id|old_state
suffix:semicolon
id|runqueue_t
op_star
id|rq
suffix:semicolon
macro_line|#ifdef CONFIG_SMP
r_int
r_int
id|load
comma
id|this_load
suffix:semicolon
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
r_int
id|new_cpu
suffix:semicolon
macro_line|#endif
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|rq
comma
id|ttwu_cnt
)paren
suffix:semicolon
id|old_state
op_assign
id|p-&gt;state
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
(paren
id|old_state
op_amp
id|state
)paren
)paren
r_goto
id|out
suffix:semicolon
r_if
c_cond
(paren
id|p-&gt;array
)paren
r_goto
id|out_running
suffix:semicolon
id|cpu
op_assign
id|task_cpu
c_func
(paren
id|p
)paren
suffix:semicolon
id|this_cpu
op_assign
id|smp_processor_id
c_func
(paren
)paren
suffix:semicolon
macro_line|#ifdef CONFIG_SMP
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|task_running
c_func
(paren
id|rq
comma
id|p
)paren
)paren
)paren
r_goto
id|out_activate
suffix:semicolon
id|new_cpu
op_assign
id|cpu
suffix:semicolon
r_if
c_cond
(paren
id|cpu
op_eq
id|this_cpu
op_logical_or
id|unlikely
c_func
(paren
op_logical_neg
id|cpu_isset
c_func
(paren
id|this_cpu
comma
id|p-&gt;cpus_allowed
)paren
)paren
)paren
r_goto
id|out_set_cpu
suffix:semicolon
id|load
op_assign
id|source_load
c_func
(paren
id|cpu
)paren
suffix:semicolon
id|this_load
op_assign
id|target_load
c_func
(paren
id|this_cpu
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * If sync wakeup then subtract the (maximum possible) effect of&n;&t; * the currently running task from the load of the current CPU:&n;&t; */
r_if
c_cond
(paren
id|sync
)paren
id|this_load
op_sub_assign
id|SCHED_LOAD_SCALE
suffix:semicolon
multiline_comment|/* Don&squot;t pull the task off an idle CPU to a busy one */
r_if
c_cond
(paren
id|load
template_param
id|SCHED_LOAD_SCALE
op_div
l_int|2
)paren
r_goto
id|out_set_cpu
suffix:semicolon
id|new_cpu
op_assign
id|this_cpu
suffix:semicolon
multiline_comment|/* Wake to this CPU if we can */
multiline_comment|/*&n;&t; * Scan domains for affine wakeup and passive balancing&n;&t; * possibilities.&n;&t; */
id|for_each_domain
c_func
(paren
id|this_cpu
comma
id|sd
)paren
(brace
r_int
r_int
id|imbalance
suffix:semicolon
multiline_comment|/*&n;&t;&t; * Start passive balancing when half the imbalance_pct&n;&t;&t; * limit is reached.&n;&t;&t; */
id|imbalance
op_assign
id|sd-&gt;imbalance_pct
op_plus
(paren
id|sd-&gt;imbalance_pct
op_minus
l_int|100
)paren
op_div
l_int|2
suffix:semicolon
r_if
c_cond
(paren
(paren
id|sd-&gt;flags
op_amp
id|SD_WAKE_AFFINE
)paren
op_logical_and
op_logical_neg
id|task_hot
c_func
(paren
id|p
comma
id|rq-&gt;timestamp_last_tick
comma
id|sd
)paren
)paren
(brace
multiline_comment|/*&n;&t;&t;&t; * This domain has SD_WAKE_AFFINE and p is cache cold&n;&t;&t;&t; * in this domain.&n;&t;&t;&t; */
r_if
c_cond
(paren
id|cpu_isset
c_func
(paren
id|cpu
comma
id|sd-&gt;span
)paren
)paren
(brace
id|schedstat_inc
c_func
(paren
id|sd
comma
id|ttwu_wake_affine
)paren
suffix:semicolon
r_goto
id|out_set_cpu
suffix:semicolon
)brace
)brace
r_else
r_if
c_cond
(paren
(paren
id|sd-&gt;flags
op_amp
id|SD_WAKE_BALANCE
)paren
op_logical_and
id|imbalance
op_star
id|this_load
op_le
l_int|100
op_star
id|load
)paren
(brace
multiline_comment|/*&n;&t;&t;&t; * This domain has SD_WAKE_BALANCE and there is&n;&t;&t;&t; * an imbalance.&n;&t;&t;&t; */
r_if
c_cond
(paren
id|cpu_isset
c_func
(paren
id|cpu
comma
id|sd-&gt;span
)paren
)paren
(brace
id|schedstat_inc
c_func
(paren
id|sd
comma
id|ttwu_wake_balance
)paren
suffix:semicolon
r_goto
id|out_set_cpu
suffix:semicolon
)brace
)brace
)brace
id|new_cpu
op_assign
id|cpu
suffix:semicolon
multiline_comment|/* Could not wake to this_cpu. Wake to cpu instead */
id|out_set_cpu
suffix:colon
id|schedstat_inc
c_func
(paren
id|rq
comma
id|ttwu_attempts
)paren
suffix:semicolon
id|new_cpu
op_assign
id|wake_idle
c_func
(paren
id|new_cpu
comma
id|p
)paren
suffix:semicolon
r_if
c_cond
(paren
id|new_cpu
op_ne
id|cpu
)paren
(brace
id|schedstat_inc
c_func
(paren
id|rq
comma
id|ttwu_moved
)paren
suffix:semicolon
id|set_task_cpu
c_func
(paren
id|p
comma
id|new_cpu
)paren
suffix:semicolon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
multiline_comment|/* might preempt at this point */
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
id|old_state
op_assign
id|p-&gt;state
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
(paren
id|old_state
op_amp
id|state
)paren
)paren
r_goto
id|out
suffix:semicolon
r_if
c_cond
(paren
id|p-&gt;array
)paren
r_goto
id|out_running
suffix:semicolon
id|this_cpu
op_assign
id|smp_processor_id
c_func
(paren
)paren
suffix:semicolon
id|cpu
op_assign
id|task_cpu
c_func
(paren
id|p
)paren
suffix:semicolon
)brace
id|out_activate
suffix:colon
macro_line|#endif /* CONFIG_SMP */
r_if
c_cond
(paren
id|old_state
op_eq
id|TASK_UNINTERRUPTIBLE
)paren
(brace
id|rq-&gt;nr_uninterruptible
op_decrement
suffix:semicolon
multiline_comment|/*&n;&t;&t; * Tasks on involuntary sleep don&squot;t earn&n;&t;&t; * sleep_avg beyond just interactive state.&n;&t;&t; */
id|p-&gt;activated
op_assign
op_minus
l_int|1
suffix:semicolon
)brace
multiline_comment|/*&n;&t; * Sync wakeups (i.e. those types of wakeups where the waker&n;&t; * has indicated that it will leave the CPU in short order)&n;&t; * don&squot;t trigger a preemption, if the woken up task will run on&n;&t; * this cpu. (in this case the &squot;I will reschedule&squot; promise of&n;&t; * the waker guarantees that the freshly woken up task is going&n;&t; * to be considered on this CPU.)&n;&t; */
id|activate_task
c_func
(paren
id|p
comma
id|rq
comma
id|cpu
op_eq
id|this_cpu
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|sync
op_logical_or
id|cpu
op_ne
id|this_cpu
)paren
(brace
r_if
c_cond
(paren
id|TASK_PREEMPTS_CURR
c_func
(paren
id|p
comma
id|rq
)paren
)paren
id|resched_task
c_func
(paren
id|rq-&gt;curr
)paren
suffix:semicolon
)brace
id|success
op_assign
l_int|1
suffix:semicolon
id|out_running
suffix:colon
id|p-&gt;state
op_assign
id|TASK_RUNNING
suffix:semicolon
id|out
suffix:colon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
r_return
id|success
suffix:semicolon
)brace
DECL|function|wake_up_process
r_int
id|fastcall
id|wake_up_process
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
r_return
id|try_to_wake_up
c_func
(paren
id|p
comma
id|TASK_STOPPED
op_or
id|TASK_TRACED
op_or
id|TASK_INTERRUPTIBLE
op_or
id|TASK_UNINTERRUPTIBLE
comma
l_int|0
)paren
suffix:semicolon
)brace
DECL|variable|wake_up_process
id|EXPORT_SYMBOL
c_func
(paren
id|wake_up_process
)paren
suffix:semicolon
DECL|function|wake_up_state
r_int
id|fastcall
id|wake_up_state
c_func
(paren
id|task_t
op_star
id|p
comma
r_int
r_int
id|state
)paren
(brace
r_return
id|try_to_wake_up
c_func
(paren
id|p
comma
id|state
comma
l_int|0
)paren
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_SMP
r_static
r_int
id|find_idlest_cpu
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
r_int
id|this_cpu
comma
r_struct
id|sched_domain
op_star
id|sd
)paren
suffix:semicolon
macro_line|#endif
multiline_comment|/*&n; * Perform scheduler related setup for a newly forked process p.&n; * p is forked by current.&n; */
DECL|function|sched_fork
r_void
id|fastcall
id|sched_fork
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
multiline_comment|/*&n;&t; * We mark the process as running here, but have not actually&n;&t; * inserted it onto the runqueue yet. This guarantees that&n;&t; * nobody will actually run it, and a signal or other external&n;&t; * event cannot wake it up and insert it on the runqueue either.&n;&t; */
id|p-&gt;state
op_assign
id|TASK_RUNNING
suffix:semicolon
id|INIT_LIST_HEAD
c_func
(paren
op_amp
id|p-&gt;run_list
)paren
suffix:semicolon
id|p-&gt;array
op_assign
l_int|NULL
suffix:semicolon
id|spin_lock_init
c_func
(paren
op_amp
id|p-&gt;switch_lock
)paren
suffix:semicolon
macro_line|#ifdef CONFIG_SCHEDSTATS
id|memset
c_func
(paren
op_amp
id|p-&gt;sched_info
comma
l_int|0
comma
r_sizeof
(paren
id|p-&gt;sched_info
)paren
)paren
suffix:semicolon
macro_line|#endif
macro_line|#ifdef CONFIG_PREEMPT
multiline_comment|/*&n;&t; * During context-switch we hold precisely one spinlock, which&n;&t; * schedule_tail drops. (in the common case it&squot;s this_rq()-&gt;lock,&n;&t; * but it also can be p-&gt;switch_lock.) So we compensate with a count&n;&t; * of 1. Also, we want to start with kernel preemption disabled.&n;&t; */
id|p-&gt;thread_info-&gt;preempt_count
op_assign
l_int|1
suffix:semicolon
macro_line|#endif
multiline_comment|/*&n;&t; * Share the timeslice between parent and child, thus the&n;&t; * total amount of pending timeslices in the system doesn&squot;t change,&n;&t; * resulting in more scheduling fairness.&n;&t; */
id|local_irq_disable
c_func
(paren
)paren
suffix:semicolon
id|p-&gt;time_slice
op_assign
(paren
id|current-&gt;time_slice
op_plus
l_int|1
)paren
op_rshift
l_int|1
suffix:semicolon
multiline_comment|/*&n;&t; * The remainder of the first timeslice might be recovered by&n;&t; * the parent if the child exits early enough.&n;&t; */
id|p-&gt;first_time_slice
op_assign
l_int|1
suffix:semicolon
id|current-&gt;time_slice
op_rshift_assign
l_int|1
suffix:semicolon
id|p-&gt;timestamp
op_assign
id|sched_clock
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|current-&gt;time_slice
)paren
)paren
(brace
multiline_comment|/*&n;&t;&t; * This case is rare, it happens when the parent has only&n;&t;&t; * a single jiffy left from its timeslice. Taking the&n;&t;&t; * runqueue lock is not a problem.&n;&t;&t; */
id|current-&gt;time_slice
op_assign
l_int|1
suffix:semicolon
id|preempt_disable
c_func
(paren
)paren
suffix:semicolon
id|scheduler_tick
c_func
(paren
)paren
suffix:semicolon
id|local_irq_enable
c_func
(paren
)paren
suffix:semicolon
id|preempt_enable
c_func
(paren
)paren
suffix:semicolon
)brace
r_else
id|local_irq_enable
c_func
(paren
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * wake_up_new_task - wake up a newly created task for the first time.&n; *&n; * This function will do some initial scheduler statistics housekeeping&n; * that must be done for every newly created context, then puts the task&n; * on the runqueue and wakes it.&n; */
DECL|function|wake_up_new_task
r_void
id|fastcall
id|wake_up_new_task
c_func
(paren
id|task_t
op_star
id|p
comma
r_int
r_int
id|clone_flags
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
r_int
id|this_cpu
comma
id|cpu
suffix:semicolon
id|runqueue_t
op_star
id|rq
comma
op_star
id|this_rq
suffix:semicolon
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
id|cpu
op_assign
id|task_cpu
c_func
(paren
id|p
)paren
suffix:semicolon
id|this_cpu
op_assign
id|smp_processor_id
c_func
(paren
)paren
suffix:semicolon
id|BUG_ON
c_func
(paren
id|p-&gt;state
op_ne
id|TASK_RUNNING
)paren
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|rq
comma
id|wunt_cnt
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * We decrease the sleep average of forking parents&n;&t; * and children as well, to keep max-interactive tasks&n;&t; * from forking tasks that are max-interactive. The parent&n;&t; * (current) is done further down, under its lock.&n;&t; */
id|p-&gt;sleep_avg
op_assign
id|JIFFIES_TO_NS
c_func
(paren
id|CURRENT_BONUS
c_func
(paren
id|p
)paren
op_star
id|CHILD_PENALTY
op_div
l_int|100
op_star
id|MAX_SLEEP_AVG
op_div
id|MAX_BONUS
)paren
suffix:semicolon
id|p-&gt;prio
op_assign
id|effective_prio
c_func
(paren
id|p
)paren
suffix:semicolon
r_if
c_cond
(paren
id|likely
c_func
(paren
id|cpu
op_eq
id|this_cpu
)paren
)paren
(brace
r_if
c_cond
(paren
op_logical_neg
(paren
id|clone_flags
op_amp
id|CLONE_VM
)paren
)paren
(brace
multiline_comment|/*&n;&t;&t;&t; * The VM isn&squot;t cloned, so we&squot;re in a good position to&n;&t;&t;&t; * do child-runs-first in anticipation of an exec. This&n;&t;&t;&t; * usually avoids a lot of COW overhead.&n;&t;&t;&t; */
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|current-&gt;array
)paren
)paren
id|__activate_task
c_func
(paren
id|p
comma
id|rq
)paren
suffix:semicolon
r_else
(brace
id|p-&gt;prio
op_assign
id|current-&gt;prio
suffix:semicolon
id|list_add_tail
c_func
(paren
op_amp
id|p-&gt;run_list
comma
op_amp
id|current-&gt;run_list
)paren
suffix:semicolon
id|p-&gt;array
op_assign
id|current-&gt;array
suffix:semicolon
id|p-&gt;array-&gt;nr_active
op_increment
suffix:semicolon
id|rq-&gt;nr_running
op_increment
suffix:semicolon
)brace
id|set_need_resched
c_func
(paren
)paren
suffix:semicolon
)brace
r_else
multiline_comment|/* Run child last */
id|__activate_task
c_func
(paren
id|p
comma
id|rq
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t; * We skip the following code due to cpu == this_cpu&n;&t; &t; *&n;&t;&t; *   task_rq_unlock(rq, &amp;flags);&n;&t;&t; *   this_rq = task_rq_lock(current, &amp;flags);&n;&t;&t; */
id|this_rq
op_assign
id|rq
suffix:semicolon
)brace
r_else
(brace
id|this_rq
op_assign
id|cpu_rq
c_func
(paren
id|this_cpu
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t; * Not the local CPU - must adjust timestamp. This should&n;&t;&t; * get optimised away in the !CONFIG_SMP case.&n;&t;&t; */
id|p-&gt;timestamp
op_assign
(paren
id|p-&gt;timestamp
op_minus
id|this_rq-&gt;timestamp_last_tick
)paren
op_plus
id|rq-&gt;timestamp_last_tick
suffix:semicolon
id|__activate_task
c_func
(paren
id|p
comma
id|rq
)paren
suffix:semicolon
r_if
c_cond
(paren
id|TASK_PREEMPTS_CURR
c_func
(paren
id|p
comma
id|rq
)paren
)paren
id|resched_task
c_func
(paren
id|rq-&gt;curr
)paren
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|rq
comma
id|wunt_moved
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t; * Parent and child are on different CPUs, now get the&n;&t;&t; * parent runqueue to update the parent&squot;s -&gt;sleep_avg:&n;&t;&t; */
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
id|this_rq
op_assign
id|task_rq_lock
c_func
(paren
id|current
comma
op_amp
id|flags
)paren
suffix:semicolon
)brace
id|current-&gt;sleep_avg
op_assign
id|JIFFIES_TO_NS
c_func
(paren
id|CURRENT_BONUS
c_func
(paren
id|current
)paren
op_star
id|PARENT_PENALTY
op_div
l_int|100
op_star
id|MAX_SLEEP_AVG
op_div
id|MAX_BONUS
)paren
suffix:semicolon
id|task_rq_unlock
c_func
(paren
id|this_rq
comma
op_amp
id|flags
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Potentially available exiting-child timeslices are&n; * retrieved here - this way the parent does not get&n; * penalized for creating too many threads.&n; *&n; * (this cannot be used to &squot;generate&squot; timeslices&n; * artificially, because any timeslice recovered here&n; * was given away by the parent in the first place.)&n; */
DECL|function|sched_exit
r_void
id|fastcall
id|sched_exit
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
id|runqueue_t
op_star
id|rq
suffix:semicolon
multiline_comment|/*&n;&t; * If the child was a (relative-) CPU hog then decrease&n;&t; * the sleep_avg of the parent as well.&n;&t; */
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p-&gt;parent
comma
op_amp
id|flags
)paren
suffix:semicolon
r_if
c_cond
(paren
id|p-&gt;first_time_slice
)paren
(brace
id|p-&gt;parent-&gt;time_slice
op_add_assign
id|p-&gt;time_slice
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|p-&gt;parent-&gt;time_slice
OG
id|task_timeslice
c_func
(paren
id|p
)paren
)paren
)paren
id|p-&gt;parent-&gt;time_slice
op_assign
id|task_timeslice
c_func
(paren
id|p
)paren
suffix:semicolon
)brace
r_if
c_cond
(paren
id|p-&gt;sleep_avg
OL
id|p-&gt;parent-&gt;sleep_avg
)paren
id|p-&gt;parent-&gt;sleep_avg
op_assign
id|p-&gt;parent-&gt;sleep_avg
op_div
(paren
id|EXIT_WEIGHT
op_plus
l_int|1
)paren
op_star
id|EXIT_WEIGHT
op_plus
id|p-&gt;sleep_avg
op_div
(paren
id|EXIT_WEIGHT
op_plus
l_int|1
)paren
suffix:semicolon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
)brace
multiline_comment|/**&n; * finish_task_switch - clean up after a task-switch&n; * @prev: the thread we just switched away from.&n; *&n; * We enter this with the runqueue still locked, and finish_arch_switch()&n; * will unlock it along with doing any other architecture-specific cleanup&n; * actions.&n; *&n; * Note that we may have delayed dropping an mm in context_switch(). If&n; * so, we finish that here outside of the runqueue lock.  (Doing it&n; * with the lock held can cause deadlocks; see schedule() for&n; * details.)&n; */
r_static
r_void
id|finish_task_switch
c_func
(paren
id|task_t
op_star
id|prev
)paren
id|__releases
c_func
(paren
id|rq-&gt;lock
)paren
(brace
id|runqueue_t
op_star
id|rq
op_assign
id|this_rq
c_func
(paren
)paren
suffix:semicolon
r_struct
id|mm_struct
op_star
id|mm
op_assign
id|rq-&gt;prev_mm
suffix:semicolon
r_int
r_int
id|prev_task_flags
suffix:semicolon
id|rq-&gt;prev_mm
op_assign
l_int|NULL
suffix:semicolon
multiline_comment|/*&n;&t; * A task struct has one reference for the use as &quot;current&quot;.&n;&t; * If a task dies, then it sets EXIT_ZOMBIE in tsk-&gt;exit_state and&n;&t; * calls schedule one last time. The schedule call will never return,&n;&t; * and the scheduled task must drop that reference.&n;&t; * The test for EXIT_ZOMBIE must occur while the runqueue locks are&n;&t; * still held, otherwise prev could be scheduled on another cpu, die&n;&t; * there before we look at prev-&gt;state, and then the reference would&n;&t; * be dropped twice.&n;&t; *&t;&t;Manfred Spraul &lt;manfred@colorfullife.com&gt;&n;&t; */
id|prev_task_flags
op_assign
id|prev-&gt;flags
suffix:semicolon
id|finish_arch_switch
c_func
(paren
id|rq
comma
id|prev
)paren
suffix:semicolon
r_if
c_cond
(paren
id|mm
)paren
id|mmdrop
c_func
(paren
id|mm
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|prev_task_flags
op_amp
id|PF_DEAD
)paren
)paren
id|put_task_struct
c_func
(paren
id|prev
)paren
suffix:semicolon
)brace
multiline_comment|/**&n; * schedule_tail - first thing a freshly forked thread must call.&n; * @prev: the thread we just switched away from.&n; */
id|asmlinkage
r_void
id|schedule_tail
c_func
(paren
id|task_t
op_star
id|prev
)paren
id|__releases
c_func
(paren
id|rq-&gt;lock
)paren
(brace
id|finish_task_switch
c_func
(paren
id|prev
)paren
suffix:semicolon
r_if
c_cond
(paren
id|current-&gt;set_child_tid
)paren
id|put_user
c_func
(paren
id|current-&gt;pid
comma
id|current-&gt;set_child_tid
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * context_switch - switch to the new MM and the new&n; * thread&squot;s register state.&n; */
r_static
r_inline
DECL|function|context_switch
id|task_t
op_star
id|context_switch
c_func
(paren
id|runqueue_t
op_star
id|rq
comma
id|task_t
op_star
id|prev
comma
id|task_t
op_star
id|next
)paren
(brace
r_struct
id|mm_struct
op_star
id|mm
op_assign
id|next-&gt;mm
suffix:semicolon
r_struct
id|mm_struct
op_star
id|oldmm
op_assign
id|prev-&gt;active_mm
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|mm
)paren
)paren
(brace
id|next-&gt;active_mm
op_assign
id|oldmm
suffix:semicolon
id|atomic_inc
c_func
(paren
op_amp
id|oldmm-&gt;mm_count
)paren
suffix:semicolon
id|enter_lazy_tlb
c_func
(paren
id|oldmm
comma
id|next
)paren
suffix:semicolon
)brace
r_else
id|switch_mm
c_func
(paren
id|oldmm
comma
id|mm
comma
id|next
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|prev-&gt;mm
)paren
)paren
(brace
id|prev-&gt;active_mm
op_assign
l_int|NULL
suffix:semicolon
id|WARN_ON
c_func
(paren
id|rq-&gt;prev_mm
)paren
suffix:semicolon
id|rq-&gt;prev_mm
op_assign
id|oldmm
suffix:semicolon
)brace
multiline_comment|/* Here we just switch the register state and the stack. */
id|switch_to
c_func
(paren
id|prev
comma
id|next
comma
id|prev
)paren
suffix:semicolon
r_return
id|prev
suffix:semicolon
)brace
multiline_comment|/*&n; * nr_running, nr_uninterruptible and nr_context_switches:&n; *&n; * externally visible scheduler statistics: current number of runnable&n; * threads, current number of uninterruptible-sleeping threads, total&n; * number of context switches performed since bootup.&n; */
DECL|function|nr_running
r_int
r_int
id|nr_running
c_func
(paren
r_void
)paren
(brace
r_int
r_int
id|i
comma
id|sum
op_assign
l_int|0
suffix:semicolon
id|for_each_online_cpu
c_func
(paren
id|i
)paren
id|sum
op_add_assign
id|cpu_rq
c_func
(paren
id|i
)paren
op_member_access_from_pointer
id|nr_running
suffix:semicolon
r_return
id|sum
suffix:semicolon
)brace
DECL|function|nr_uninterruptible
r_int
r_int
id|nr_uninterruptible
c_func
(paren
r_void
)paren
(brace
r_int
r_int
id|i
comma
id|sum
op_assign
l_int|0
suffix:semicolon
id|for_each_cpu
c_func
(paren
id|i
)paren
id|sum
op_add_assign
id|cpu_rq
c_func
(paren
id|i
)paren
op_member_access_from_pointer
id|nr_uninterruptible
suffix:semicolon
multiline_comment|/*&n;&t; * Since we read the counters lockless, it might be slightly&n;&t; * inaccurate. Do not allow it to go below zero though:&n;&t; */
r_if
c_cond
(paren
id|unlikely
c_func
(paren
(paren
r_int
)paren
id|sum
OL
l_int|0
)paren
)paren
id|sum
op_assign
l_int|0
suffix:semicolon
r_return
id|sum
suffix:semicolon
)brace
DECL|function|nr_context_switches
r_int
r_int
r_int
id|nr_context_switches
c_func
(paren
r_void
)paren
(brace
r_int
r_int
r_int
id|i
comma
id|sum
op_assign
l_int|0
suffix:semicolon
id|for_each_cpu
c_func
(paren
id|i
)paren
id|sum
op_add_assign
id|cpu_rq
c_func
(paren
id|i
)paren
op_member_access_from_pointer
id|nr_switches
suffix:semicolon
r_return
id|sum
suffix:semicolon
)brace
DECL|function|nr_iowait
r_int
r_int
id|nr_iowait
c_func
(paren
r_void
)paren
(brace
r_int
r_int
id|i
comma
id|sum
op_assign
l_int|0
suffix:semicolon
id|for_each_cpu
c_func
(paren
id|i
)paren
id|sum
op_add_assign
id|atomic_read
c_func
(paren
op_amp
id|cpu_rq
c_func
(paren
id|i
)paren
op_member_access_from_pointer
id|nr_iowait
)paren
suffix:semicolon
r_return
id|sum
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_SMP
multiline_comment|/*&n; * double_rq_lock - safely lock two runqueues&n; *&n; * Note this does not disable interrupts like task_rq_lock,&n; * you need to do so manually before calling.&n; */
r_static
r_void
id|double_rq_lock
c_func
(paren
id|runqueue_t
op_star
id|rq1
comma
id|runqueue_t
op_star
id|rq2
)paren
id|__acquires
c_func
(paren
id|rq1-&gt;lock
)paren
id|__acquires
c_func
(paren
id|rq2-&gt;lock
)paren
(brace
r_if
c_cond
(paren
id|rq1
op_eq
id|rq2
)paren
(brace
id|spin_lock
c_func
(paren
op_amp
id|rq1-&gt;lock
)paren
suffix:semicolon
id|__acquire
c_func
(paren
id|rq2-&gt;lock
)paren
suffix:semicolon
multiline_comment|/* Fake it out ;) */
)brace
r_else
(brace
r_if
c_cond
(paren
id|rq1
OL
id|rq2
)paren
(brace
id|spin_lock
c_func
(paren
op_amp
id|rq1-&gt;lock
)paren
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|rq2-&gt;lock
)paren
suffix:semicolon
)brace
r_else
(brace
id|spin_lock
c_func
(paren
op_amp
id|rq2-&gt;lock
)paren
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|rq1-&gt;lock
)paren
suffix:semicolon
)brace
)brace
)brace
multiline_comment|/*&n; * double_rq_unlock - safely unlock two runqueues&n; *&n; * Note this does not restore interrupts like task_rq_unlock,&n; * you need to do so manually after calling.&n; */
r_static
r_void
id|double_rq_unlock
c_func
(paren
id|runqueue_t
op_star
id|rq1
comma
id|runqueue_t
op_star
id|rq2
)paren
id|__releases
c_func
(paren
id|rq1-&gt;lock
)paren
id|__releases
c_func
(paren
id|rq2-&gt;lock
)paren
(brace
id|spin_unlock
c_func
(paren
op_amp
id|rq1-&gt;lock
)paren
suffix:semicolon
r_if
c_cond
(paren
id|rq1
op_ne
id|rq2
)paren
id|spin_unlock
c_func
(paren
op_amp
id|rq2-&gt;lock
)paren
suffix:semicolon
r_else
id|__release
c_func
(paren
id|rq2-&gt;lock
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * double_lock_balance - lock the busiest runqueue, this_rq is locked already.&n; */
r_static
r_void
id|double_lock_balance
c_func
(paren
id|runqueue_t
op_star
id|this_rq
comma
id|runqueue_t
op_star
id|busiest
)paren
id|__releases
c_func
(paren
id|this_rq-&gt;lock
)paren
id|__acquires
c_func
(paren
id|busiest-&gt;lock
)paren
id|__acquires
c_func
(paren
id|this_rq-&gt;lock
)paren
(brace
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|spin_trylock
c_func
(paren
op_amp
id|busiest-&gt;lock
)paren
)paren
)paren
(brace
r_if
c_cond
(paren
id|busiest
OL
id|this_rq
)paren
(brace
id|spin_unlock
c_func
(paren
op_amp
id|this_rq-&gt;lock
)paren
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|busiest-&gt;lock
)paren
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|this_rq-&gt;lock
)paren
suffix:semicolon
)brace
r_else
id|spin_lock
c_func
(paren
op_amp
id|busiest-&gt;lock
)paren
suffix:semicolon
)brace
)brace
multiline_comment|/*&n; * find_idlest_cpu - find the least busy runqueue.&n; */
DECL|function|find_idlest_cpu
r_static
r_int
id|find_idlest_cpu
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
r_int
id|this_cpu
comma
r_struct
id|sched_domain
op_star
id|sd
)paren
(brace
r_int
r_int
id|load
comma
id|min_load
comma
id|this_load
suffix:semicolon
r_int
id|i
comma
id|min_cpu
suffix:semicolon
id|cpumask_t
id|mask
suffix:semicolon
id|min_cpu
op_assign
id|UINT_MAX
suffix:semicolon
id|min_load
op_assign
id|ULONG_MAX
suffix:semicolon
id|cpus_and
c_func
(paren
id|mask
comma
id|sd-&gt;span
comma
id|p-&gt;cpus_allowed
)paren
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|mask
)paren
(brace
id|load
op_assign
id|target_load
c_func
(paren
id|i
)paren
suffix:semicolon
r_if
c_cond
(paren
id|load
OL
id|min_load
)paren
(brace
id|min_cpu
op_assign
id|i
suffix:semicolon
id|min_load
op_assign
id|load
suffix:semicolon
multiline_comment|/* break out early on an idle CPU: */
r_if
c_cond
(paren
op_logical_neg
id|min_load
)paren
r_break
suffix:semicolon
)brace
)brace
multiline_comment|/* add +1 to account for the new task */
id|this_load
op_assign
id|source_load
c_func
(paren
id|this_cpu
)paren
op_plus
id|SCHED_LOAD_SCALE
suffix:semicolon
multiline_comment|/*&n;&t; * Would with the addition of the new task to the&n;&t; * current CPU there be an imbalance between this&n;&t; * CPU and the idlest CPU?&n;&t; *&n;&t; * Use half of the balancing threshold - new-context is&n;&t; * a good opportunity to balance.&n;&t; */
r_if
c_cond
(paren
id|min_load
op_star
(paren
l_int|100
op_plus
(paren
id|sd-&gt;imbalance_pct
op_minus
l_int|100
)paren
op_div
l_int|2
)paren
OL
id|this_load
op_star
l_int|100
)paren
r_return
id|min_cpu
suffix:semicolon
r_return
id|this_cpu
suffix:semicolon
)brace
multiline_comment|/*&n; * If dest_cpu is allowed for this process, migrate the task to it.&n; * This is accomplished by forcing the cpu_allowed mask to only&n; * allow dest_cpu, which will force the cpu onto dest_cpu.  Then&n; * the cpu_allowed mask is restored.&n; */
DECL|function|sched_migrate_task
r_static
r_void
id|sched_migrate_task
c_func
(paren
id|task_t
op_star
id|p
comma
r_int
id|dest_cpu
)paren
(brace
id|migration_req_t
id|req
suffix:semicolon
id|runqueue_t
op_star
id|rq
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|cpu_isset
c_func
(paren
id|dest_cpu
comma
id|p-&gt;cpus_allowed
)paren
op_logical_or
id|unlikely
c_func
(paren
id|cpu_is_offline
c_func
(paren
id|dest_cpu
)paren
)paren
)paren
r_goto
id|out
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|rq
comma
id|smt_cnt
)paren
suffix:semicolon
multiline_comment|/* force the process onto the specified CPU */
r_if
c_cond
(paren
id|migrate_task
c_func
(paren
id|p
comma
id|dest_cpu
comma
op_amp
id|req
)paren
)paren
(brace
multiline_comment|/* Need to wait for migration thread (might exit: take ref). */
r_struct
id|task_struct
op_star
id|mt
op_assign
id|rq-&gt;migration_thread
suffix:semicolon
id|get_task_struct
c_func
(paren
id|mt
)paren
suffix:semicolon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
id|wake_up_process
c_func
(paren
id|mt
)paren
suffix:semicolon
id|put_task_struct
c_func
(paren
id|mt
)paren
suffix:semicolon
id|wait_for_completion
c_func
(paren
op_amp
id|req.done
)paren
suffix:semicolon
r_return
suffix:semicolon
)brace
id|out
suffix:colon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * sched_exec(): find the highest-level, exec-balance-capable&n; * domain and try to migrate the task to the least loaded CPU.&n; *&n; * execve() is a valuable balancing opportunity, because at this point&n; * the task has the smallest effective memory and cache footprint.&n; */
DECL|function|sched_exec
r_void
id|sched_exec
c_func
(paren
r_void
)paren
(brace
r_struct
id|sched_domain
op_star
id|tmp
comma
op_star
id|sd
op_assign
l_int|NULL
suffix:semicolon
r_int
id|new_cpu
comma
id|this_cpu
op_assign
id|get_cpu
c_func
(paren
)paren
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|this_rq
c_func
(paren
)paren
comma
id|sbe_cnt
)paren
suffix:semicolon
multiline_comment|/* Prefer the current CPU if there&squot;s only this task running */
r_if
c_cond
(paren
id|this_rq
c_func
(paren
)paren
op_member_access_from_pointer
id|nr_running
op_le
l_int|1
)paren
r_goto
id|out
suffix:semicolon
id|for_each_domain
c_func
(paren
id|this_cpu
comma
id|tmp
)paren
r_if
c_cond
(paren
id|tmp-&gt;flags
op_amp
id|SD_BALANCE_EXEC
)paren
id|sd
op_assign
id|tmp
suffix:semicolon
r_if
c_cond
(paren
id|sd
)paren
(brace
id|schedstat_inc
c_func
(paren
id|sd
comma
id|sbe_attempts
)paren
suffix:semicolon
id|new_cpu
op_assign
id|find_idlest_cpu
c_func
(paren
id|current
comma
id|this_cpu
comma
id|sd
)paren
suffix:semicolon
r_if
c_cond
(paren
id|new_cpu
op_ne
id|this_cpu
)paren
(brace
id|schedstat_inc
c_func
(paren
id|sd
comma
id|sbe_pushed
)paren
suffix:semicolon
id|put_cpu
c_func
(paren
)paren
suffix:semicolon
id|sched_migrate_task
c_func
(paren
id|current
comma
id|new_cpu
)paren
suffix:semicolon
r_return
suffix:semicolon
)brace
)brace
id|out
suffix:colon
id|put_cpu
c_func
(paren
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * pull_task - move a task from a remote runqueue to the local runqueue.&n; * Both runqueues must be locked.&n; */
r_static
r_inline
DECL|function|pull_task
r_void
id|pull_task
c_func
(paren
id|runqueue_t
op_star
id|src_rq
comma
id|prio_array_t
op_star
id|src_array
comma
id|task_t
op_star
id|p
comma
id|runqueue_t
op_star
id|this_rq
comma
id|prio_array_t
op_star
id|this_array
comma
r_int
id|this_cpu
)paren
(brace
id|dequeue_task
c_func
(paren
id|p
comma
id|src_array
)paren
suffix:semicolon
id|src_rq-&gt;nr_running
op_decrement
suffix:semicolon
id|set_task_cpu
c_func
(paren
id|p
comma
id|this_cpu
)paren
suffix:semicolon
id|this_rq-&gt;nr_running
op_increment
suffix:semicolon
id|enqueue_task
c_func
(paren
id|p
comma
id|this_array
)paren
suffix:semicolon
id|p-&gt;timestamp
op_assign
(paren
id|p-&gt;timestamp
op_minus
id|src_rq-&gt;timestamp_last_tick
)paren
op_plus
id|this_rq-&gt;timestamp_last_tick
suffix:semicolon
multiline_comment|/*&n;&t; * Note that idle threads have a prio of MAX_PRIO, for this test&n;&t; * to be always true for them.&n;&t; */
r_if
c_cond
(paren
id|TASK_PREEMPTS_CURR
c_func
(paren
id|p
comma
id|this_rq
)paren
)paren
id|resched_task
c_func
(paren
id|this_rq-&gt;curr
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?&n; */
r_static
r_inline
DECL|function|can_migrate_task
r_int
id|can_migrate_task
c_func
(paren
id|task_t
op_star
id|p
comma
id|runqueue_t
op_star
id|rq
comma
r_int
id|this_cpu
comma
r_struct
id|sched_domain
op_star
id|sd
comma
r_enum
id|idle_type
id|idle
)paren
(brace
multiline_comment|/*&n;&t; * We do not migrate tasks that are:&n;&t; * 1) running (obviously), or&n;&t; * 2) cannot be migrated to this CPU due to cpus_allowed, or&n;&t; * 3) are cache-hot on their current CPU.&n;&t; */
r_if
c_cond
(paren
id|task_running
c_func
(paren
id|rq
comma
id|p
)paren
)paren
r_return
l_int|0
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|cpu_isset
c_func
(paren
id|this_cpu
comma
id|p-&gt;cpus_allowed
)paren
)paren
r_return
l_int|0
suffix:semicolon
multiline_comment|/*&n;&t; * Aggressive migration if:&n;&t; * 1) the [whole] cpu is idle, or&n;&t; * 2) too many balance attempts have failed.&n;&t; */
r_if
c_cond
(paren
id|cpu_and_siblings_are_idle
c_func
(paren
id|this_cpu
)paren
op_logical_or
"&bslash;"
id|sd-&gt;nr_balance_failed
OG
id|sd-&gt;cache_nice_tries
)paren
r_return
l_int|1
suffix:semicolon
r_if
c_cond
(paren
id|task_hot
c_func
(paren
id|p
comma
id|rq-&gt;timestamp_last_tick
comma
id|sd
)paren
)paren
r_return
l_int|0
suffix:semicolon
r_return
l_int|1
suffix:semicolon
)brace
multiline_comment|/*&n; * move_tasks tries to move up to max_nr_move tasks from busiest to this_rq,&n; * as part of a balancing operation within &quot;domain&quot;. Returns the number of&n; * tasks moved.&n; *&n; * Called with both runqueues locked.&n; */
DECL|function|move_tasks
r_static
r_int
id|move_tasks
c_func
(paren
id|runqueue_t
op_star
id|this_rq
comma
r_int
id|this_cpu
comma
id|runqueue_t
op_star
id|busiest
comma
r_int
r_int
id|max_nr_move
comma
r_struct
id|sched_domain
op_star
id|sd
comma
r_enum
id|idle_type
id|idle
)paren
(brace
id|prio_array_t
op_star
id|array
comma
op_star
id|dst_array
suffix:semicolon
r_struct
id|list_head
op_star
id|head
comma
op_star
id|curr
suffix:semicolon
r_int
id|idx
comma
id|pulled
op_assign
l_int|0
suffix:semicolon
id|task_t
op_star
id|tmp
suffix:semicolon
r_if
c_cond
(paren
id|max_nr_move
op_le
l_int|0
op_logical_or
id|busiest-&gt;nr_running
op_le
l_int|1
)paren
r_goto
id|out
suffix:semicolon
multiline_comment|/*&n;&t; * We first consider expired tasks. Those will likely not be&n;&t; * executed in the near future, and they are most likely to&n;&t; * be cache-cold, thus switching CPUs has the least effect&n;&t; * on them.&n;&t; */
r_if
c_cond
(paren
id|busiest-&gt;expired-&gt;nr_active
)paren
(brace
id|array
op_assign
id|busiest-&gt;expired
suffix:semicolon
id|dst_array
op_assign
id|this_rq-&gt;expired
suffix:semicolon
)brace
r_else
(brace
id|array
op_assign
id|busiest-&gt;active
suffix:semicolon
id|dst_array
op_assign
id|this_rq-&gt;active
suffix:semicolon
)brace
id|new_array
suffix:colon
multiline_comment|/* Start searching at priority 0: */
id|idx
op_assign
l_int|0
suffix:semicolon
id|skip_bitmap
suffix:colon
r_if
c_cond
(paren
op_logical_neg
id|idx
)paren
id|idx
op_assign
id|sched_find_first_bit
c_func
(paren
id|array-&gt;bitmap
)paren
suffix:semicolon
r_else
id|idx
op_assign
id|find_next_bit
c_func
(paren
id|array-&gt;bitmap
comma
id|MAX_PRIO
comma
id|idx
)paren
suffix:semicolon
r_if
c_cond
(paren
id|idx
op_ge
id|MAX_PRIO
)paren
(brace
r_if
c_cond
(paren
id|array
op_eq
id|busiest-&gt;expired
op_logical_and
id|busiest-&gt;active-&gt;nr_active
)paren
(brace
id|array
op_assign
id|busiest-&gt;active
suffix:semicolon
id|dst_array
op_assign
id|this_rq-&gt;active
suffix:semicolon
r_goto
id|new_array
suffix:semicolon
)brace
r_goto
id|out
suffix:semicolon
)brace
id|head
op_assign
id|array-&gt;queue
op_plus
id|idx
suffix:semicolon
id|curr
op_assign
id|head-&gt;prev
suffix:semicolon
id|skip_queue
suffix:colon
id|tmp
op_assign
id|list_entry
c_func
(paren
id|curr
comma
id|task_t
comma
id|run_list
)paren
suffix:semicolon
id|curr
op_assign
id|curr-&gt;prev
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|can_migrate_task
c_func
(paren
id|tmp
comma
id|busiest
comma
id|this_cpu
comma
id|sd
comma
id|idle
)paren
)paren
(brace
r_if
c_cond
(paren
id|curr
op_ne
id|head
)paren
r_goto
id|skip_queue
suffix:semicolon
id|idx
op_increment
suffix:semicolon
r_goto
id|skip_bitmap
suffix:semicolon
)brace
multiline_comment|/*&n;&t; * Right now, this is the only place pull_task() is called,&n;&t; * so we can safely collect pull_task() stats here rather than&n;&t; * inside pull_task().&n;&t; */
id|schedstat_inc
c_func
(paren
id|this_rq
comma
id|pt_gained
(braket
id|idle
)braket
)paren
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|busiest
comma
id|pt_lost
(braket
id|idle
)braket
)paren
suffix:semicolon
id|pull_task
c_func
(paren
id|busiest
comma
id|array
comma
id|tmp
comma
id|this_rq
comma
id|dst_array
comma
id|this_cpu
)paren
suffix:semicolon
id|pulled
op_increment
suffix:semicolon
multiline_comment|/* We only want to steal up to the prescribed number of tasks. */
r_if
c_cond
(paren
id|pulled
OL
id|max_nr_move
)paren
(brace
r_if
c_cond
(paren
id|curr
op_ne
id|head
)paren
r_goto
id|skip_queue
suffix:semicolon
id|idx
op_increment
suffix:semicolon
r_goto
id|skip_bitmap
suffix:semicolon
)brace
id|out
suffix:colon
r_return
id|pulled
suffix:semicolon
)brace
multiline_comment|/*&n; * find_busiest_group finds and returns the busiest CPU group within the&n; * domain. It calculates and returns the number of tasks which should be&n; * moved to restore balance via the imbalance parameter.&n; */
r_static
r_struct
id|sched_group
op_star
DECL|function|find_busiest_group
id|find_busiest_group
c_func
(paren
r_struct
id|sched_domain
op_star
id|sd
comma
r_int
id|this_cpu
comma
r_int
r_int
op_star
id|imbalance
comma
r_enum
id|idle_type
id|idle
)paren
(brace
r_struct
id|sched_group
op_star
id|busiest
op_assign
l_int|NULL
comma
op_star
id|this
op_assign
l_int|NULL
comma
op_star
id|group
op_assign
id|sd-&gt;groups
suffix:semicolon
r_int
r_int
id|max_load
comma
id|avg_load
comma
id|total_load
comma
id|this_load
comma
id|total_pwr
suffix:semicolon
id|max_load
op_assign
id|this_load
op_assign
id|total_load
op_assign
id|total_pwr
op_assign
l_int|0
suffix:semicolon
r_do
(brace
r_int
r_int
id|load
suffix:semicolon
r_int
id|local_group
suffix:semicolon
r_int
id|i
comma
id|nr_cpus
op_assign
l_int|0
suffix:semicolon
id|local_group
op_assign
id|cpu_isset
c_func
(paren
id|this_cpu
comma
id|group-&gt;cpumask
)paren
suffix:semicolon
multiline_comment|/* Tally up the load of all CPUs in the group */
id|avg_load
op_assign
l_int|0
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|group-&gt;cpumask
)paren
(brace
multiline_comment|/* Bias balancing toward cpus of our domain */
r_if
c_cond
(paren
id|local_group
)paren
id|load
op_assign
id|target_load
c_func
(paren
id|i
)paren
suffix:semicolon
r_else
id|load
op_assign
id|source_load
c_func
(paren
id|i
)paren
suffix:semicolon
id|nr_cpus
op_increment
suffix:semicolon
id|avg_load
op_add_assign
id|load
suffix:semicolon
)brace
r_if
c_cond
(paren
op_logical_neg
id|nr_cpus
)paren
r_goto
id|nextgroup
suffix:semicolon
id|total_load
op_add_assign
id|avg_load
suffix:semicolon
id|total_pwr
op_add_assign
id|group-&gt;cpu_power
suffix:semicolon
multiline_comment|/* Adjust by relative CPU power of the group */
id|avg_load
op_assign
(paren
id|avg_load
op_star
id|SCHED_LOAD_SCALE
)paren
op_div
id|group-&gt;cpu_power
suffix:semicolon
r_if
c_cond
(paren
id|local_group
)paren
(brace
id|this_load
op_assign
id|avg_load
suffix:semicolon
id|this
op_assign
id|group
suffix:semicolon
r_goto
id|nextgroup
suffix:semicolon
)brace
r_else
r_if
c_cond
(paren
id|avg_load
OG
id|max_load
)paren
(brace
id|max_load
op_assign
id|avg_load
suffix:semicolon
id|busiest
op_assign
id|group
suffix:semicolon
)brace
id|nextgroup
suffix:colon
id|group
op_assign
id|group-&gt;next
suffix:semicolon
)brace
r_while
c_loop
(paren
id|group
op_ne
id|sd-&gt;groups
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|busiest
op_logical_or
id|this_load
op_ge
id|max_load
)paren
r_goto
id|out_balanced
suffix:semicolon
id|avg_load
op_assign
(paren
id|SCHED_LOAD_SCALE
op_star
id|total_load
)paren
op_div
id|total_pwr
suffix:semicolon
r_if
c_cond
(paren
id|this_load
op_ge
id|avg_load
op_logical_or
l_int|100
op_star
id|max_load
op_le
id|sd-&gt;imbalance_pct
op_star
id|this_load
)paren
r_goto
id|out_balanced
suffix:semicolon
multiline_comment|/*&n;&t; * We&squot;re trying to get all the cpus to the average_load, so we don&squot;t&n;&t; * want to push ourselves above the average load, nor do we wish to&n;&t; * reduce the max loaded cpu below the average load, as either of these&n;&t; * actions would just result in more rebalancing later, and ping-pong&n;&t; * tasks around. Thus we look for the minimum possible imbalance.&n;&t; * Negative imbalances (*we* are more loaded than anyone else) will&n;&t; * be counted as no imbalance for these purposes -- we can&squot;t fix that&n;&t; * by pulling tasks to us.  Be careful of negative numbers as they&squot;ll&n;&t; * appear as very large values with unsigned longs.&n;&t; */
op_star
id|imbalance
op_assign
id|min
c_func
(paren
id|max_load
op_minus
id|avg_load
comma
id|avg_load
op_minus
id|this_load
)paren
suffix:semicolon
multiline_comment|/* How much load to actually move to equalise the imbalance */
op_star
id|imbalance
op_assign
(paren
op_star
id|imbalance
op_star
id|min
c_func
(paren
id|busiest-&gt;cpu_power
comma
id|this-&gt;cpu_power
)paren
)paren
op_div
id|SCHED_LOAD_SCALE
suffix:semicolon
r_if
c_cond
(paren
op_star
id|imbalance
OL
id|SCHED_LOAD_SCALE
op_minus
l_int|1
)paren
(brace
r_int
r_int
id|pwr_now
op_assign
l_int|0
comma
id|pwr_move
op_assign
l_int|0
suffix:semicolon
r_int
r_int
id|tmp
suffix:semicolon
r_if
c_cond
(paren
id|max_load
op_minus
id|this_load
op_ge
id|SCHED_LOAD_SCALE
op_star
l_int|2
)paren
(brace
op_star
id|imbalance
op_assign
l_int|1
suffix:semicolon
r_return
id|busiest
suffix:semicolon
)brace
multiline_comment|/*&n;&t;&t; * OK, we don&squot;t have enough imbalance to justify moving tasks,&n;&t;&t; * however we may be able to increase total CPU power used by&n;&t;&t; * moving them.&n;&t;&t; */
id|pwr_now
op_add_assign
id|busiest-&gt;cpu_power
op_star
id|min
c_func
(paren
id|SCHED_LOAD_SCALE
comma
id|max_load
)paren
suffix:semicolon
id|pwr_now
op_add_assign
id|this-&gt;cpu_power
op_star
id|min
c_func
(paren
id|SCHED_LOAD_SCALE
comma
id|this_load
)paren
suffix:semicolon
id|pwr_now
op_div_assign
id|SCHED_LOAD_SCALE
suffix:semicolon
multiline_comment|/* Amount of load we&squot;d subtract */
id|tmp
op_assign
id|SCHED_LOAD_SCALE
op_star
id|SCHED_LOAD_SCALE
op_div
id|busiest-&gt;cpu_power
suffix:semicolon
r_if
c_cond
(paren
id|max_load
OG
id|tmp
)paren
id|pwr_move
op_add_assign
id|busiest-&gt;cpu_power
op_star
id|min
c_func
(paren
id|SCHED_LOAD_SCALE
comma
id|max_load
op_minus
id|tmp
)paren
suffix:semicolon
multiline_comment|/* Amount of load we&squot;d add */
id|tmp
op_assign
id|SCHED_LOAD_SCALE
op_star
id|SCHED_LOAD_SCALE
op_div
id|this-&gt;cpu_power
suffix:semicolon
r_if
c_cond
(paren
id|max_load
OL
id|tmp
)paren
id|tmp
op_assign
id|max_load
suffix:semicolon
id|pwr_move
op_add_assign
id|this-&gt;cpu_power
op_star
id|min
c_func
(paren
id|SCHED_LOAD_SCALE
comma
id|this_load
op_plus
id|tmp
)paren
suffix:semicolon
id|pwr_move
op_div_assign
id|SCHED_LOAD_SCALE
suffix:semicolon
multiline_comment|/* Move if we gain another 8th of a CPU worth of throughput */
r_if
c_cond
(paren
id|pwr_move
OL
id|pwr_now
op_plus
id|SCHED_LOAD_SCALE
op_div
l_int|8
)paren
r_goto
id|out_balanced
suffix:semicolon
op_star
id|imbalance
op_assign
l_int|1
suffix:semicolon
r_return
id|busiest
suffix:semicolon
)brace
multiline_comment|/* Get rid of the scaling factor, rounding down as we divide */
op_star
id|imbalance
op_assign
(paren
op_star
id|imbalance
op_plus
l_int|1
)paren
op_div
id|SCHED_LOAD_SCALE
suffix:semicolon
r_return
id|busiest
suffix:semicolon
id|out_balanced
suffix:colon
r_if
c_cond
(paren
id|busiest
op_logical_and
(paren
id|idle
op_eq
id|NEWLY_IDLE
op_logical_or
(paren
id|idle
op_eq
id|SCHED_IDLE
op_logical_and
id|max_load
OG
id|SCHED_LOAD_SCALE
)paren
)paren
)paren
(brace
op_star
id|imbalance
op_assign
l_int|1
suffix:semicolon
r_return
id|busiest
suffix:semicolon
)brace
op_star
id|imbalance
op_assign
l_int|0
suffix:semicolon
r_return
l_int|NULL
suffix:semicolon
)brace
multiline_comment|/*&n; * find_busiest_queue - find the busiest runqueue among the cpus in group.&n; */
DECL|function|find_busiest_queue
r_static
id|runqueue_t
op_star
id|find_busiest_queue
c_func
(paren
r_struct
id|sched_group
op_star
id|group
)paren
(brace
r_int
r_int
id|load
comma
id|max_load
op_assign
l_int|0
suffix:semicolon
id|runqueue_t
op_star
id|busiest
op_assign
l_int|NULL
suffix:semicolon
r_int
id|i
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|group-&gt;cpumask
)paren
(brace
id|load
op_assign
id|source_load
c_func
(paren
id|i
)paren
suffix:semicolon
r_if
c_cond
(paren
id|load
OG
id|max_load
)paren
(brace
id|max_load
op_assign
id|load
suffix:semicolon
id|busiest
op_assign
id|cpu_rq
c_func
(paren
id|i
)paren
suffix:semicolon
)brace
)brace
r_return
id|busiest
suffix:semicolon
)brace
multiline_comment|/*&n; * Check this_cpu to ensure it is balanced within domain. Attempt to move&n; * tasks if there is an imbalance.&n; *&n; * Called with this_rq unlocked.&n; */
DECL|function|load_balance
r_static
r_int
id|load_balance
c_func
(paren
r_int
id|this_cpu
comma
id|runqueue_t
op_star
id|this_rq
comma
r_struct
id|sched_domain
op_star
id|sd
comma
r_enum
id|idle_type
id|idle
)paren
(brace
r_struct
id|sched_group
op_star
id|group
suffix:semicolon
id|runqueue_t
op_star
id|busiest
suffix:semicolon
r_int
r_int
id|imbalance
suffix:semicolon
r_int
id|nr_moved
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|this_rq-&gt;lock
)paren
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|sd
comma
id|lb_cnt
(braket
id|idle
)braket
)paren
suffix:semicolon
id|group
op_assign
id|find_busiest_group
c_func
(paren
id|sd
comma
id|this_cpu
comma
op_amp
id|imbalance
comma
id|idle
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|group
)paren
(brace
id|schedstat_inc
c_func
(paren
id|sd
comma
id|lb_nobusyg
(braket
id|idle
)braket
)paren
suffix:semicolon
r_goto
id|out_balanced
suffix:semicolon
)brace
id|busiest
op_assign
id|find_busiest_queue
c_func
(paren
id|group
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|busiest
)paren
(brace
id|schedstat_inc
c_func
(paren
id|sd
comma
id|lb_nobusyq
(braket
id|idle
)braket
)paren
suffix:semicolon
r_goto
id|out_balanced
suffix:semicolon
)brace
multiline_comment|/*&n;&t; * This should be &quot;impossible&quot;, but since load&n;&t; * balancing is inherently racy and statistical,&n;&t; * it could happen in theory.&n;&t; */
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|busiest
op_eq
id|this_rq
)paren
)paren
(brace
id|WARN_ON
c_func
(paren
l_int|1
)paren
suffix:semicolon
r_goto
id|out_balanced
suffix:semicolon
)brace
id|schedstat_add
c_func
(paren
id|sd
comma
id|lb_imbalance
(braket
id|idle
)braket
comma
id|imbalance
)paren
suffix:semicolon
id|nr_moved
op_assign
l_int|0
suffix:semicolon
r_if
c_cond
(paren
id|busiest-&gt;nr_running
OG
l_int|1
)paren
(brace
multiline_comment|/*&n;&t;&t; * Attempt to move tasks. If find_busiest_group has found&n;&t;&t; * an imbalance but busiest-&gt;nr_running &lt;= 1, the group is&n;&t;&t; * still unbalanced. nr_moved simply stays zero, so it is&n;&t;&t; * correctly treated as an imbalance.&n;&t;&t; */
id|double_lock_balance
c_func
(paren
id|this_rq
comma
id|busiest
)paren
suffix:semicolon
id|nr_moved
op_assign
id|move_tasks
c_func
(paren
id|this_rq
comma
id|this_cpu
comma
id|busiest
comma
id|imbalance
comma
id|sd
comma
id|idle
)paren
suffix:semicolon
id|spin_unlock
c_func
(paren
op_amp
id|busiest-&gt;lock
)paren
suffix:semicolon
)brace
id|spin_unlock
c_func
(paren
op_amp
id|this_rq-&gt;lock
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|nr_moved
)paren
(brace
id|schedstat_inc
c_func
(paren
id|sd
comma
id|lb_failed
(braket
id|idle
)braket
)paren
suffix:semicolon
id|sd-&gt;nr_balance_failed
op_increment
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|sd-&gt;nr_balance_failed
OG
id|sd-&gt;cache_nice_tries
op_plus
l_int|2
)paren
)paren
(brace
r_int
id|wake
op_assign
l_int|0
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|busiest-&gt;lock
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|busiest-&gt;active_balance
)paren
(brace
id|busiest-&gt;active_balance
op_assign
l_int|1
suffix:semicolon
id|busiest-&gt;push_cpu
op_assign
id|this_cpu
suffix:semicolon
id|wake
op_assign
l_int|1
suffix:semicolon
)brace
id|spin_unlock
c_func
(paren
op_amp
id|busiest-&gt;lock
)paren
suffix:semicolon
r_if
c_cond
(paren
id|wake
)paren
id|wake_up_process
c_func
(paren
id|busiest-&gt;migration_thread
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t;&t; * We&squot;ve kicked active balancing, reset the failure&n;&t;&t;&t; * counter.&n;&t;&t;&t; */
id|sd-&gt;nr_balance_failed
op_assign
id|sd-&gt;cache_nice_tries
suffix:semicolon
)brace
multiline_comment|/*&n;&t;&t; * We were unbalanced, but unsuccessful in move_tasks(),&n;&t;&t; * so bump the balance_interval to lessen the lock contention.&n;&t;&t; */
r_if
c_cond
(paren
id|sd-&gt;balance_interval
OL
id|sd-&gt;max_interval
)paren
id|sd-&gt;balance_interval
op_increment
suffix:semicolon
)brace
r_else
(brace
id|sd-&gt;nr_balance_failed
op_assign
l_int|0
suffix:semicolon
multiline_comment|/* We were unbalanced, so reset the balancing interval */
id|sd-&gt;balance_interval
op_assign
id|sd-&gt;min_interval
suffix:semicolon
)brace
r_return
id|nr_moved
suffix:semicolon
id|out_balanced
suffix:colon
id|spin_unlock
c_func
(paren
op_amp
id|this_rq-&gt;lock
)paren
suffix:semicolon
multiline_comment|/* tune up the balancing interval */
r_if
c_cond
(paren
id|sd-&gt;balance_interval
OL
id|sd-&gt;max_interval
)paren
id|sd-&gt;balance_interval
op_mul_assign
l_int|2
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
multiline_comment|/*&n; * Check this_cpu to ensure it is balanced within domain. Attempt to move&n; * tasks if there is an imbalance.&n; *&n; * Called from schedule when this_rq is about to become idle (NEWLY_IDLE).&n; * this_rq is locked.&n; */
DECL|function|load_balance_newidle
r_static
r_int
id|load_balance_newidle
c_func
(paren
r_int
id|this_cpu
comma
id|runqueue_t
op_star
id|this_rq
comma
r_struct
id|sched_domain
op_star
id|sd
)paren
(brace
r_struct
id|sched_group
op_star
id|group
suffix:semicolon
id|runqueue_t
op_star
id|busiest
op_assign
l_int|NULL
suffix:semicolon
r_int
r_int
id|imbalance
suffix:semicolon
r_int
id|nr_moved
op_assign
l_int|0
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|sd
comma
id|lb_cnt
(braket
id|NEWLY_IDLE
)braket
)paren
suffix:semicolon
id|group
op_assign
id|find_busiest_group
c_func
(paren
id|sd
comma
id|this_cpu
comma
op_amp
id|imbalance
comma
id|NEWLY_IDLE
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|group
)paren
(brace
id|schedstat_inc
c_func
(paren
id|sd
comma
id|lb_nobusyg
(braket
id|NEWLY_IDLE
)braket
)paren
suffix:semicolon
r_goto
id|out
suffix:semicolon
)brace
id|busiest
op_assign
id|find_busiest_queue
c_func
(paren
id|group
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|busiest
op_logical_or
id|busiest
op_eq
id|this_rq
)paren
(brace
id|schedstat_inc
c_func
(paren
id|sd
comma
id|lb_nobusyq
(braket
id|NEWLY_IDLE
)braket
)paren
suffix:semicolon
r_goto
id|out
suffix:semicolon
)brace
multiline_comment|/* Attempt to move tasks */
id|double_lock_balance
c_func
(paren
id|this_rq
comma
id|busiest
)paren
suffix:semicolon
id|schedstat_add
c_func
(paren
id|sd
comma
id|lb_imbalance
(braket
id|NEWLY_IDLE
)braket
comma
id|imbalance
)paren
suffix:semicolon
id|nr_moved
op_assign
id|move_tasks
c_func
(paren
id|this_rq
comma
id|this_cpu
comma
id|busiest
comma
id|imbalance
comma
id|sd
comma
id|NEWLY_IDLE
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|nr_moved
)paren
id|schedstat_inc
c_func
(paren
id|sd
comma
id|lb_failed
(braket
id|NEWLY_IDLE
)braket
)paren
suffix:semicolon
id|spin_unlock
c_func
(paren
op_amp
id|busiest-&gt;lock
)paren
suffix:semicolon
id|out
suffix:colon
r_return
id|nr_moved
suffix:semicolon
)brace
multiline_comment|/*&n; * idle_balance is called by schedule() if this_cpu is about to become&n; * idle. Attempts to pull tasks from other CPUs.&n; */
DECL|function|idle_balance
r_static
r_inline
r_void
id|idle_balance
c_func
(paren
r_int
id|this_cpu
comma
id|runqueue_t
op_star
id|this_rq
)paren
(brace
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
id|for_each_domain
c_func
(paren
id|this_cpu
comma
id|sd
)paren
(brace
r_if
c_cond
(paren
id|sd-&gt;flags
op_amp
id|SD_BALANCE_NEWIDLE
)paren
(brace
r_if
c_cond
(paren
id|load_balance_newidle
c_func
(paren
id|this_cpu
comma
id|this_rq
comma
id|sd
)paren
)paren
(brace
multiline_comment|/* We&squot;ve pulled tasks over so stop searching */
r_break
suffix:semicolon
)brace
)brace
)brace
)brace
multiline_comment|/*&n; * active_load_balance is run by migration threads. It pushes running tasks&n; * off the busiest CPU onto idle CPUs. It requires at least 1 task to be&n; * running on each physical CPU where possible, and avoids physical /&n; * logical imbalances.&n; *&n; * Called with busiest_rq locked.&n; */
DECL|function|active_load_balance
r_static
r_void
id|active_load_balance
c_func
(paren
id|runqueue_t
op_star
id|busiest_rq
comma
r_int
id|busiest_cpu
)paren
(brace
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
r_struct
id|sched_group
op_star
id|cpu_group
suffix:semicolon
id|runqueue_t
op_star
id|target_rq
suffix:semicolon
id|cpumask_t
id|visited_cpus
suffix:semicolon
r_int
id|cpu
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|busiest_rq
comma
id|alb_cnt
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Search for suitable CPUs to push tasks to in successively higher&n;&t; * domains with SD_LOAD_BALANCE set.&n;&t; */
id|visited_cpus
op_assign
id|CPU_MASK_NONE
suffix:semicolon
id|for_each_domain
c_func
(paren
id|busiest_cpu
comma
id|sd
)paren
(brace
r_if
c_cond
(paren
op_logical_neg
(paren
id|sd-&gt;flags
op_amp
id|SD_LOAD_BALANCE
)paren
)paren
multiline_comment|/* no more domains to search */
r_break
suffix:semicolon
id|cpu_group
op_assign
id|sd-&gt;groups
suffix:semicolon
r_do
(brace
id|for_each_cpu_mask
c_func
(paren
id|cpu
comma
id|cpu_group-&gt;cpumask
)paren
(brace
r_if
c_cond
(paren
id|busiest_rq-&gt;nr_running
op_le
l_int|1
)paren
multiline_comment|/* no more tasks left to move */
r_return
suffix:semicolon
r_if
c_cond
(paren
id|cpu_isset
c_func
(paren
id|cpu
comma
id|visited_cpus
)paren
)paren
r_continue
suffix:semicolon
id|cpu_set
c_func
(paren
id|cpu
comma
id|visited_cpus
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|cpu_and_siblings_are_idle
c_func
(paren
id|cpu
)paren
op_logical_or
id|cpu
op_eq
id|busiest_cpu
)paren
r_continue
suffix:semicolon
id|target_rq
op_assign
id|cpu_rq
c_func
(paren
id|cpu
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t;&t;&t; * This condition is &quot;impossible&quot;, if it occurs&n;&t;&t;&t;&t; * we need to fix it.  Originally reported by&n;&t;&t;&t;&t; * Bjorn Helgaas on a 128-cpu setup.&n;&t;&t;&t;&t; */
id|BUG_ON
c_func
(paren
id|busiest_rq
op_eq
id|target_rq
)paren
suffix:semicolon
multiline_comment|/* move a task from busiest_rq to target_rq */
id|double_lock_balance
c_func
(paren
id|busiest_rq
comma
id|target_rq
)paren
suffix:semicolon
r_if
c_cond
(paren
id|move_tasks
c_func
(paren
id|target_rq
comma
id|cpu
comma
id|busiest_rq
comma
l_int|1
comma
id|sd
comma
id|SCHED_IDLE
)paren
)paren
(brace
id|schedstat_inc
c_func
(paren
id|busiest_rq
comma
id|alb_lost
)paren
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|target_rq
comma
id|alb_gained
)paren
suffix:semicolon
)brace
r_else
(brace
id|schedstat_inc
c_func
(paren
id|busiest_rq
comma
id|alb_failed
)paren
suffix:semicolon
)brace
id|spin_unlock
c_func
(paren
op_amp
id|target_rq-&gt;lock
)paren
suffix:semicolon
)brace
id|cpu_group
op_assign
id|cpu_group-&gt;next
suffix:semicolon
)brace
r_while
c_loop
(paren
id|cpu_group
op_ne
id|sd-&gt;groups
)paren
suffix:semicolon
)brace
)brace
multiline_comment|/*&n; * rebalance_tick will get called every timer tick, on every CPU.&n; *&n; * It checks each scheduling domain to see if it is due to be balanced,&n; * and initiates a balancing operation if so.&n; *&n; * Balancing parameters are set up in arch_init_sched_domains.&n; */
multiline_comment|/* Don&squot;t have all balancing operations going off at once */
DECL|macro|CPU_OFFSET
mdefine_line|#define CPU_OFFSET(cpu) (HZ * cpu / NR_CPUS)
DECL|function|rebalance_tick
r_static
r_void
id|rebalance_tick
c_func
(paren
r_int
id|this_cpu
comma
id|runqueue_t
op_star
id|this_rq
comma
r_enum
id|idle_type
id|idle
)paren
(brace
r_int
r_int
id|old_load
comma
id|this_load
suffix:semicolon
r_int
r_int
id|j
op_assign
id|jiffies
op_plus
id|CPU_OFFSET
c_func
(paren
id|this_cpu
)paren
suffix:semicolon
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
multiline_comment|/* Update our load */
id|old_load
op_assign
id|this_rq-&gt;cpu_load
suffix:semicolon
id|this_load
op_assign
id|this_rq-&gt;nr_running
op_star
id|SCHED_LOAD_SCALE
suffix:semicolon
multiline_comment|/*&n;&t; * Round up the averaging division if load is increasing. This&n;&t; * prevents us from getting stuck on 9 if the load is 10, for&n;&t; * example.&n;&t; */
r_if
c_cond
(paren
id|this_load
OG
id|old_load
)paren
id|old_load
op_increment
suffix:semicolon
id|this_rq-&gt;cpu_load
op_assign
(paren
id|old_load
op_plus
id|this_load
)paren
op_div
l_int|2
suffix:semicolon
id|for_each_domain
c_func
(paren
id|this_cpu
comma
id|sd
)paren
(brace
r_int
r_int
id|interval
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
(paren
id|sd-&gt;flags
op_amp
id|SD_LOAD_BALANCE
)paren
)paren
r_continue
suffix:semicolon
id|interval
op_assign
id|sd-&gt;balance_interval
suffix:semicolon
r_if
c_cond
(paren
id|idle
op_ne
id|SCHED_IDLE
)paren
id|interval
op_mul_assign
id|sd-&gt;busy_factor
suffix:semicolon
multiline_comment|/* scale ms to jiffies */
id|interval
op_assign
id|msecs_to_jiffies
c_func
(paren
id|interval
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|interval
)paren
)paren
id|interval
op_assign
l_int|1
suffix:semicolon
r_if
c_cond
(paren
id|j
op_minus
id|sd-&gt;last_balance
op_ge
id|interval
)paren
(brace
r_if
c_cond
(paren
id|load_balance
c_func
(paren
id|this_cpu
comma
id|this_rq
comma
id|sd
comma
id|idle
)paren
)paren
(brace
multiline_comment|/* We&squot;ve pulled tasks over so no longer idle */
id|idle
op_assign
id|NOT_IDLE
suffix:semicolon
)brace
id|sd-&gt;last_balance
op_add_assign
id|interval
suffix:semicolon
)brace
)brace
)brace
macro_line|#else
multiline_comment|/*&n; * on UP we do not need to balance between CPUs:&n; */
DECL|function|rebalance_tick
r_static
r_inline
r_void
id|rebalance_tick
c_func
(paren
r_int
id|cpu
comma
id|runqueue_t
op_star
id|rq
comma
r_enum
id|idle_type
id|idle
)paren
(brace
)brace
DECL|function|idle_balance
r_static
r_inline
r_void
id|idle_balance
c_func
(paren
r_int
id|cpu
comma
id|runqueue_t
op_star
id|rq
)paren
(brace
)brace
macro_line|#endif
DECL|function|wake_priority_sleeper
r_static
r_inline
r_int
id|wake_priority_sleeper
c_func
(paren
id|runqueue_t
op_star
id|rq
)paren
(brace
r_int
id|ret
op_assign
l_int|0
suffix:semicolon
macro_line|#ifdef CONFIG_SCHED_SMT
id|spin_lock
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * If an SMT sibling task has been put to sleep for priority&n;&t; * reasons reschedule the idle task to see if it can now run.&n;&t; */
r_if
c_cond
(paren
id|rq-&gt;nr_running
)paren
(brace
id|resched_task
c_func
(paren
id|rq-&gt;idle
)paren
suffix:semicolon
id|ret
op_assign
l_int|1
suffix:semicolon
)brace
id|spin_unlock
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
macro_line|#endif
r_return
id|ret
suffix:semicolon
)brace
id|DEFINE_PER_CPU
c_func
(paren
r_struct
id|kernel_stat
comma
id|kstat
)paren
suffix:semicolon
DECL|variable|kstat
id|EXPORT_PER_CPU_SYMBOL
c_func
(paren
id|kstat
)paren
suffix:semicolon
multiline_comment|/*&n; * We place interactive tasks back into the active array, if possible.&n; *&n; * To guarantee that this does not starve expired tasks we ignore the&n; * interactivity of a task if the first expired task had to wait more&n; * than a &squot;reasonable&squot; amount of time. This deadline timeout is&n; * load-dependent, as the frequency of array switched decreases with&n; * increasing number of running tasks. We also ignore the interactivity&n; * if a better static_prio task has expired:&n; */
DECL|macro|EXPIRED_STARVING
mdefine_line|#define EXPIRED_STARVING(rq) &bslash;&n;&t;((STARVATION_LIMIT &amp;&amp; ((rq)-&gt;expired_timestamp &amp;&amp; &bslash;&n;&t;&t;(jiffies - (rq)-&gt;expired_timestamp &gt;= &bslash;&n;&t;&t;&t;STARVATION_LIMIT * ((rq)-&gt;nr_running) + 1))) || &bslash;&n;&t;&t;&t;((rq)-&gt;curr-&gt;static_prio &gt; (rq)-&gt;best_expired_prio))
multiline_comment|/*&n; * Do the virtual cpu time signal calculations.&n; * @p: the process that the cpu time gets accounted to&n; * @cputime: the cpu time spent in user space since the last update&n; */
DECL|function|account_it_virt
r_static
r_inline
r_void
id|account_it_virt
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|cputime_t
id|cputime
)paren
(brace
id|cputime_t
id|it_virt
op_assign
id|p-&gt;it_virt_value
suffix:semicolon
r_if
c_cond
(paren
id|cputime_gt
c_func
(paren
id|it_virt
comma
id|cputime_zero
)paren
op_logical_and
id|cputime_gt
c_func
(paren
id|cputime
comma
id|cputime_zero
)paren
)paren
(brace
r_if
c_cond
(paren
id|cputime_ge
c_func
(paren
id|cputime
comma
id|it_virt
)paren
)paren
(brace
id|it_virt
op_assign
id|cputime_add
c_func
(paren
id|it_virt
comma
id|p-&gt;it_virt_incr
)paren
suffix:semicolon
id|send_sig
c_func
(paren
id|SIGVTALRM
comma
id|p
comma
l_int|1
)paren
suffix:semicolon
)brace
id|it_virt
op_assign
id|cputime_sub
c_func
(paren
id|it_virt
comma
id|cputime
)paren
suffix:semicolon
id|p-&gt;it_virt_value
op_assign
id|it_virt
suffix:semicolon
)brace
)brace
multiline_comment|/*&n; * Do the virtual profiling signal calculations.&n; * @p: the process that the cpu time gets accounted to&n; * @cputime: the cpu time spent in user and kernel space since the last update&n; */
DECL|function|account_it_prof
r_static
r_void
id|account_it_prof
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|cputime_t
id|cputime
)paren
(brace
id|cputime_t
id|it_prof
op_assign
id|p-&gt;it_prof_value
suffix:semicolon
r_if
c_cond
(paren
id|cputime_gt
c_func
(paren
id|it_prof
comma
id|cputime_zero
)paren
op_logical_and
id|cputime_gt
c_func
(paren
id|cputime
comma
id|cputime_zero
)paren
)paren
(brace
r_if
c_cond
(paren
id|cputime_ge
c_func
(paren
id|cputime
comma
id|it_prof
)paren
)paren
(brace
id|it_prof
op_assign
id|cputime_add
c_func
(paren
id|it_prof
comma
id|p-&gt;it_prof_incr
)paren
suffix:semicolon
id|send_sig
c_func
(paren
id|SIGPROF
comma
id|p
comma
l_int|1
)paren
suffix:semicolon
)brace
id|it_prof
op_assign
id|cputime_sub
c_func
(paren
id|it_prof
comma
id|cputime
)paren
suffix:semicolon
id|p-&gt;it_prof_value
op_assign
id|it_prof
suffix:semicolon
)brace
)brace
multiline_comment|/*&n; * Check if the process went over its cputime resource limit after&n; * some cpu time got added to utime/stime.&n; * @p: the process that the cpu time gets accounted to&n; * @cputime: the cpu time spent in user and kernel space since the last update&n; */
DECL|function|check_rlimit
r_static
r_void
id|check_rlimit
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|cputime_t
id|cputime
)paren
(brace
id|cputime_t
id|total
comma
id|tmp
suffix:semicolon
r_int
r_int
id|secs
suffix:semicolon
id|total
op_assign
id|cputime_add
c_func
(paren
id|p-&gt;utime
comma
id|p-&gt;stime
)paren
suffix:semicolon
id|secs
op_assign
id|cputime_to_secs
c_func
(paren
id|total
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|secs
op_ge
id|p-&gt;signal-&gt;rlim
(braket
id|RLIMIT_CPU
)braket
dot
id|rlim_cur
)paren
)paren
(brace
multiline_comment|/* Send SIGXCPU every second. */
id|tmp
op_assign
id|cputime_sub
c_func
(paren
id|total
comma
id|cputime
)paren
suffix:semicolon
r_if
c_cond
(paren
id|cputime_to_secs
c_func
(paren
id|tmp
)paren
OL
id|secs
)paren
id|send_sig
c_func
(paren
id|SIGXCPU
comma
id|p
comma
l_int|1
)paren
suffix:semicolon
multiline_comment|/* and SIGKILL when we go over max.. */
r_if
c_cond
(paren
id|secs
op_ge
id|p-&gt;signal-&gt;rlim
(braket
id|RLIMIT_CPU
)braket
dot
id|rlim_max
)paren
id|send_sig
c_func
(paren
id|SIGKILL
comma
id|p
comma
l_int|1
)paren
suffix:semicolon
)brace
)brace
multiline_comment|/*&n; * Account user cpu time to a process.&n; * @p: the process that the cpu time gets accounted to&n; * @hardirq_offset: the offset to subtract from hardirq_count()&n; * @cputime: the cpu time spent in user space since the last update&n; */
DECL|function|account_user_time
r_void
id|account_user_time
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|cputime_t
id|cputime
)paren
(brace
r_struct
id|cpu_usage_stat
op_star
id|cpustat
op_assign
op_amp
id|kstat_this_cpu.cpustat
suffix:semicolon
id|cputime64_t
id|tmp
suffix:semicolon
id|p-&gt;utime
op_assign
id|cputime_add
c_func
(paren
id|p-&gt;utime
comma
id|cputime
)paren
suffix:semicolon
multiline_comment|/* Check for signals (SIGVTALRM, SIGPROF, SIGXCPU &amp; SIGKILL). */
id|check_rlimit
c_func
(paren
id|p
comma
id|cputime
)paren
suffix:semicolon
id|account_it_virt
c_func
(paren
id|p
comma
id|cputime
)paren
suffix:semicolon
id|account_it_prof
c_func
(paren
id|p
comma
id|cputime
)paren
suffix:semicolon
multiline_comment|/* Add user time to cpustat. */
id|tmp
op_assign
id|cputime_to_cputime64
c_func
(paren
id|cputime
)paren
suffix:semicolon
r_if
c_cond
(paren
id|TASK_NICE
c_func
(paren
id|p
)paren
OG
l_int|0
)paren
id|cpustat-&gt;nice
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;nice
comma
id|tmp
)paren
suffix:semicolon
r_else
id|cpustat-&gt;user
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;user
comma
id|tmp
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Account system cpu time to a process.&n; * @p: the process that the cpu time gets accounted to&n; * @hardirq_offset: the offset to subtract from hardirq_count()&n; * @cputime: the cpu time spent in kernel space since the last update&n; */
DECL|function|account_system_time
r_void
id|account_system_time
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
r_int
id|hardirq_offset
comma
id|cputime_t
id|cputime
)paren
(brace
r_struct
id|cpu_usage_stat
op_star
id|cpustat
op_assign
op_amp
id|kstat_this_cpu.cpustat
suffix:semicolon
id|runqueue_t
op_star
id|rq
op_assign
id|this_rq
c_func
(paren
)paren
suffix:semicolon
id|cputime64_t
id|tmp
suffix:semicolon
id|p-&gt;stime
op_assign
id|cputime_add
c_func
(paren
id|p-&gt;stime
comma
id|cputime
)paren
suffix:semicolon
multiline_comment|/* Check for signals (SIGPROF, SIGXCPU &amp; SIGKILL). */
r_if
c_cond
(paren
id|likely
c_func
(paren
id|p-&gt;signal
op_logical_and
id|p-&gt;exit_state
OL
id|EXIT_ZOMBIE
)paren
)paren
(brace
id|check_rlimit
c_func
(paren
id|p
comma
id|cputime
)paren
suffix:semicolon
id|account_it_prof
c_func
(paren
id|p
comma
id|cputime
)paren
suffix:semicolon
)brace
multiline_comment|/* Add system time to cpustat. */
id|tmp
op_assign
id|cputime_to_cputime64
c_func
(paren
id|cputime
)paren
suffix:semicolon
r_if
c_cond
(paren
id|hardirq_count
c_func
(paren
)paren
op_minus
id|hardirq_offset
)paren
id|cpustat-&gt;irq
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;irq
comma
id|tmp
)paren
suffix:semicolon
r_else
r_if
c_cond
(paren
id|softirq_count
c_func
(paren
)paren
)paren
id|cpustat-&gt;softirq
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;softirq
comma
id|tmp
)paren
suffix:semicolon
r_else
r_if
c_cond
(paren
id|p
op_ne
id|rq-&gt;idle
)paren
id|cpustat-&gt;system
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;system
comma
id|tmp
)paren
suffix:semicolon
r_else
r_if
c_cond
(paren
id|atomic_read
c_func
(paren
op_amp
id|rq-&gt;nr_iowait
)paren
OG
l_int|0
)paren
id|cpustat-&gt;iowait
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;iowait
comma
id|tmp
)paren
suffix:semicolon
r_else
id|cpustat-&gt;idle
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;idle
comma
id|tmp
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Account for involuntary wait time.&n; * @p: the process from which the cpu time has been stolen&n; * @steal: the cpu time spent in involuntary wait&n; */
DECL|function|account_steal_time
r_void
id|account_steal_time
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
id|cputime_t
id|steal
)paren
(brace
r_struct
id|cpu_usage_stat
op_star
id|cpustat
op_assign
op_amp
id|kstat_this_cpu.cpustat
suffix:semicolon
id|cputime64_t
id|tmp
op_assign
id|cputime_to_cputime64
c_func
(paren
id|steal
)paren
suffix:semicolon
id|runqueue_t
op_star
id|rq
op_assign
id|this_rq
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|p
op_eq
id|rq-&gt;idle
)paren
(brace
id|p-&gt;stime
op_assign
id|cputime_add
c_func
(paren
id|p-&gt;stime
comma
id|steal
)paren
suffix:semicolon
r_if
c_cond
(paren
id|atomic_read
c_func
(paren
op_amp
id|rq-&gt;nr_iowait
)paren
OG
l_int|0
)paren
id|cpustat-&gt;iowait
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;iowait
comma
id|tmp
)paren
suffix:semicolon
r_else
id|cpustat-&gt;idle
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;idle
comma
id|tmp
)paren
suffix:semicolon
)brace
r_else
id|cpustat-&gt;steal
op_assign
id|cputime64_add
c_func
(paren
id|cpustat-&gt;steal
comma
id|tmp
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * This function gets called by the timer code, with HZ frequency.&n; * We call it with interrupts disabled.&n; *&n; * It also gets called by the fork code, when changing the parent&squot;s&n; * timeslices.&n; */
DECL|function|scheduler_tick
r_void
id|scheduler_tick
c_func
(paren
r_void
)paren
(brace
r_int
id|cpu
op_assign
id|smp_processor_id
c_func
(paren
)paren
suffix:semicolon
id|runqueue_t
op_star
id|rq
op_assign
id|this_rq
c_func
(paren
)paren
suffix:semicolon
id|task_t
op_star
id|p
op_assign
id|current
suffix:semicolon
id|rq-&gt;timestamp_last_tick
op_assign
id|sched_clock
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|p
op_eq
id|rq-&gt;idle
)paren
(brace
r_if
c_cond
(paren
id|wake_priority_sleeper
c_func
(paren
id|rq
)paren
)paren
r_goto
id|out
suffix:semicolon
id|rebalance_tick
c_func
(paren
id|cpu
comma
id|rq
comma
id|SCHED_IDLE
)paren
suffix:semicolon
r_return
suffix:semicolon
)brace
multiline_comment|/* Task might have expired already, but not scheduled off yet */
r_if
c_cond
(paren
id|p-&gt;array
op_ne
id|rq-&gt;active
)paren
(brace
id|set_tsk_need_resched
c_func
(paren
id|p
)paren
suffix:semicolon
r_goto
id|out
suffix:semicolon
)brace
id|spin_lock
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * The task was running during this tick - update the&n;&t; * time slice counter. Note: we do not update a thread&squot;s&n;&t; * priority until it either goes to sleep or uses up its&n;&t; * timeslice. This makes it possible for interactive tasks&n;&t; * to use up their timeslices at their highest priority levels.&n;&t; */
r_if
c_cond
(paren
id|rt_task
c_func
(paren
id|p
)paren
)paren
(brace
multiline_comment|/*&n;&t;&t; * RR tasks need a special form of timeslice management.&n;&t;&t; * FIFO tasks have no timeslices.&n;&t;&t; */
r_if
c_cond
(paren
(paren
id|p-&gt;policy
op_eq
id|SCHED_RR
)paren
op_logical_and
op_logical_neg
op_decrement
id|p-&gt;time_slice
)paren
(brace
id|p-&gt;time_slice
op_assign
id|task_timeslice
c_func
(paren
id|p
)paren
suffix:semicolon
id|p-&gt;first_time_slice
op_assign
l_int|0
suffix:semicolon
id|set_tsk_need_resched
c_func
(paren
id|p
)paren
suffix:semicolon
multiline_comment|/* put it at the end of the queue: */
id|requeue_task
c_func
(paren
id|p
comma
id|rq-&gt;active
)paren
suffix:semicolon
)brace
r_goto
id|out_unlock
suffix:semicolon
)brace
r_if
c_cond
(paren
op_logical_neg
op_decrement
id|p-&gt;time_slice
)paren
(brace
id|dequeue_task
c_func
(paren
id|p
comma
id|rq-&gt;active
)paren
suffix:semicolon
id|set_tsk_need_resched
c_func
(paren
id|p
)paren
suffix:semicolon
id|p-&gt;prio
op_assign
id|effective_prio
c_func
(paren
id|p
)paren
suffix:semicolon
id|p-&gt;time_slice
op_assign
id|task_timeslice
c_func
(paren
id|p
)paren
suffix:semicolon
id|p-&gt;first_time_slice
op_assign
l_int|0
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|rq-&gt;expired_timestamp
)paren
id|rq-&gt;expired_timestamp
op_assign
id|jiffies
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|TASK_INTERACTIVE
c_func
(paren
id|p
)paren
op_logical_or
id|EXPIRED_STARVING
c_func
(paren
id|rq
)paren
)paren
(brace
id|enqueue_task
c_func
(paren
id|p
comma
id|rq-&gt;expired
)paren
suffix:semicolon
r_if
c_cond
(paren
id|p-&gt;static_prio
OL
id|rq-&gt;best_expired_prio
)paren
id|rq-&gt;best_expired_prio
op_assign
id|p-&gt;static_prio
suffix:semicolon
)brace
r_else
id|enqueue_task
c_func
(paren
id|p
comma
id|rq-&gt;active
)paren
suffix:semicolon
)brace
r_else
(brace
multiline_comment|/*&n;&t;&t; * Prevent a too long timeslice allowing a task to monopolize&n;&t;&t; * the CPU. We do this by splitting up the timeslice into&n;&t;&t; * smaller pieces.&n;&t;&t; *&n;&t;&t; * Note: this does not mean the task&squot;s timeslices expire or&n;&t;&t; * get lost in any way, they just might be preempted by&n;&t;&t; * another task of equal priority. (one with higher&n;&t;&t; * priority would have preempted this task already.) We&n;&t;&t; * requeue this task to the end of the list on this priority&n;&t;&t; * level, which is in essence a round-robin of tasks with&n;&t;&t; * equal priority.&n;&t;&t; *&n;&t;&t; * This only applies to tasks in the interactive&n;&t;&t; * delta range with at least TIMESLICE_GRANULARITY to requeue.&n;&t;&t; */
r_if
c_cond
(paren
id|TASK_INTERACTIVE
c_func
(paren
id|p
)paren
op_logical_and
op_logical_neg
(paren
(paren
id|task_timeslice
c_func
(paren
id|p
)paren
op_minus
id|p-&gt;time_slice
)paren
op_mod
id|TIMESLICE_GRANULARITY
c_func
(paren
id|p
)paren
)paren
op_logical_and
(paren
id|p-&gt;time_slice
op_ge
id|TIMESLICE_GRANULARITY
c_func
(paren
id|p
)paren
)paren
op_logical_and
(paren
id|p-&gt;array
op_eq
id|rq-&gt;active
)paren
)paren
(brace
id|requeue_task
c_func
(paren
id|p
comma
id|rq-&gt;active
)paren
suffix:semicolon
id|set_tsk_need_resched
c_func
(paren
id|p
)paren
suffix:semicolon
)brace
)brace
id|out_unlock
suffix:colon
id|spin_unlock
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
id|out
suffix:colon
id|rebalance_tick
c_func
(paren
id|cpu
comma
id|rq
comma
id|NOT_IDLE
)paren
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_SCHED_SMT
DECL|function|wake_sleeping_dependent
r_static
r_inline
r_void
id|wake_sleeping_dependent
c_func
(paren
r_int
id|this_cpu
comma
id|runqueue_t
op_star
id|this_rq
)paren
(brace
r_struct
id|sched_domain
op_star
id|sd
op_assign
id|this_rq-&gt;sd
suffix:semicolon
id|cpumask_t
id|sibling_map
suffix:semicolon
r_int
id|i
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
(paren
id|sd-&gt;flags
op_amp
id|SD_SHARE_CPUPOWER
)paren
)paren
r_return
suffix:semicolon
multiline_comment|/*&n;&t; * Unlock the current runqueue because we have to lock in&n;&t; * CPU order to avoid deadlocks. Caller knows that we might&n;&t; * unlock. We keep IRQs disabled.&n;&t; */
id|spin_unlock
c_func
(paren
op_amp
id|this_rq-&gt;lock
)paren
suffix:semicolon
id|sibling_map
op_assign
id|sd-&gt;span
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|sibling_map
)paren
id|spin_lock
c_func
(paren
op_amp
id|cpu_rq
c_func
(paren
id|i
)paren
op_member_access_from_pointer
id|lock
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * We clear this CPU from the mask. This both simplifies the&n;&t; * inner loop and keps this_rq locked when we exit:&n;&t; */
id|cpu_clear
c_func
(paren
id|this_cpu
comma
id|sibling_map
)paren
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|sibling_map
)paren
(brace
id|runqueue_t
op_star
id|smt_rq
op_assign
id|cpu_rq
c_func
(paren
id|i
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t; * If an SMT sibling task is sleeping due to priority&n;&t;&t; * reasons wake it up now.&n;&t;&t; */
r_if
c_cond
(paren
id|smt_rq-&gt;curr
op_eq
id|smt_rq-&gt;idle
op_logical_and
id|smt_rq-&gt;nr_running
)paren
id|resched_task
c_func
(paren
id|smt_rq-&gt;idle
)paren
suffix:semicolon
)brace
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|sibling_map
)paren
id|spin_unlock
c_func
(paren
op_amp
id|cpu_rq
c_func
(paren
id|i
)paren
op_member_access_from_pointer
id|lock
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * We exit with this_cpu&squot;s rq still held and IRQs&n;&t; * still disabled:&n;&t; */
)brace
DECL|function|dependent_sleeper
r_static
r_inline
r_int
id|dependent_sleeper
c_func
(paren
r_int
id|this_cpu
comma
id|runqueue_t
op_star
id|this_rq
)paren
(brace
r_struct
id|sched_domain
op_star
id|sd
op_assign
id|this_rq-&gt;sd
suffix:semicolon
id|cpumask_t
id|sibling_map
suffix:semicolon
id|prio_array_t
op_star
id|array
suffix:semicolon
r_int
id|ret
op_assign
l_int|0
comma
id|i
suffix:semicolon
id|task_t
op_star
id|p
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
(paren
id|sd-&gt;flags
op_amp
id|SD_SHARE_CPUPOWER
)paren
)paren
r_return
l_int|0
suffix:semicolon
multiline_comment|/*&n;&t; * The same locking rules and details apply as for&n;&t; * wake_sleeping_dependent():&n;&t; */
id|spin_unlock
c_func
(paren
op_amp
id|this_rq-&gt;lock
)paren
suffix:semicolon
id|sibling_map
op_assign
id|sd-&gt;span
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|sibling_map
)paren
id|spin_lock
c_func
(paren
op_amp
id|cpu_rq
c_func
(paren
id|i
)paren
op_member_access_from_pointer
id|lock
)paren
suffix:semicolon
id|cpu_clear
c_func
(paren
id|this_cpu
comma
id|sibling_map
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Establish next task to be run - it might have gone away because&n;&t; * we released the runqueue lock above:&n;&t; */
r_if
c_cond
(paren
op_logical_neg
id|this_rq-&gt;nr_running
)paren
r_goto
id|out_unlock
suffix:semicolon
id|array
op_assign
id|this_rq-&gt;active
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|array-&gt;nr_active
)paren
id|array
op_assign
id|this_rq-&gt;expired
suffix:semicolon
id|BUG_ON
c_func
(paren
op_logical_neg
id|array-&gt;nr_active
)paren
suffix:semicolon
id|p
op_assign
id|list_entry
c_func
(paren
id|array-&gt;queue
(braket
id|sched_find_first_bit
c_func
(paren
id|array-&gt;bitmap
)paren
)braket
dot
id|next
comma
id|task_t
comma
id|run_list
)paren
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|sibling_map
)paren
(brace
id|runqueue_t
op_star
id|smt_rq
op_assign
id|cpu_rq
c_func
(paren
id|i
)paren
suffix:semicolon
id|task_t
op_star
id|smt_curr
op_assign
id|smt_rq-&gt;curr
suffix:semicolon
multiline_comment|/*&n;&t;&t; * If a user task with lower static priority than the&n;&t;&t; * running task on the SMT sibling is trying to schedule,&n;&t;&t; * delay it till there is proportionately less timeslice&n;&t;&t; * left of the sibling task to prevent a lower priority&n;&t;&t; * task from using an unfair proportion of the&n;&t;&t; * physical cpu&squot;s resources. -ck&n;&t;&t; */
r_if
c_cond
(paren
(paren
(paren
id|smt_curr-&gt;time_slice
op_star
(paren
l_int|100
op_minus
id|sd-&gt;per_cpu_gain
)paren
op_div
l_int|100
)paren
OG
id|task_timeslice
c_func
(paren
id|p
)paren
op_logical_or
id|rt_task
c_func
(paren
id|smt_curr
)paren
)paren
op_logical_and
id|p-&gt;mm
op_logical_and
id|smt_curr-&gt;mm
op_logical_and
op_logical_neg
id|rt_task
c_func
(paren
id|p
)paren
)paren
id|ret
op_assign
l_int|1
suffix:semicolon
multiline_comment|/*&n;&t;&t; * Reschedule a lower priority task on the SMT sibling,&n;&t;&t; * or wake it up if it has been put to sleep for priority&n;&t;&t; * reasons.&n;&t;&t; */
r_if
c_cond
(paren
(paren
(paren
(paren
id|p-&gt;time_slice
op_star
(paren
l_int|100
op_minus
id|sd-&gt;per_cpu_gain
)paren
op_div
l_int|100
)paren
OG
id|task_timeslice
c_func
(paren
id|smt_curr
)paren
op_logical_or
id|rt_task
c_func
(paren
id|p
)paren
)paren
op_logical_and
id|smt_curr-&gt;mm
op_logical_and
id|p-&gt;mm
op_logical_and
op_logical_neg
id|rt_task
c_func
(paren
id|smt_curr
)paren
)paren
op_logical_or
(paren
id|smt_curr
op_eq
id|smt_rq-&gt;idle
op_logical_and
id|smt_rq-&gt;nr_running
)paren
)paren
id|resched_task
c_func
(paren
id|smt_curr
)paren
suffix:semicolon
)brace
id|out_unlock
suffix:colon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|sibling_map
)paren
id|spin_unlock
c_func
(paren
op_amp
id|cpu_rq
c_func
(paren
id|i
)paren
op_member_access_from_pointer
id|lock
)paren
suffix:semicolon
r_return
id|ret
suffix:semicolon
)brace
macro_line|#else
DECL|function|wake_sleeping_dependent
r_static
r_inline
r_void
id|wake_sleeping_dependent
c_func
(paren
r_int
id|this_cpu
comma
id|runqueue_t
op_star
id|this_rq
)paren
(brace
)brace
DECL|function|dependent_sleeper
r_static
r_inline
r_int
id|dependent_sleeper
c_func
(paren
r_int
id|this_cpu
comma
id|runqueue_t
op_star
id|this_rq
)paren
(brace
r_return
l_int|0
suffix:semicolon
)brace
macro_line|#endif
macro_line|#if defined(CONFIG_PREEMPT) &amp;&amp; defined(CONFIG_DEBUG_PREEMPT)
DECL|function|add_preempt_count
r_void
id|fastcall
id|add_preempt_count
c_func
(paren
r_int
id|val
)paren
(brace
multiline_comment|/*&n;&t; * Underflow?&n;&t; */
id|BUG_ON
c_func
(paren
(paren
(paren
r_int
)paren
id|preempt_count
c_func
(paren
)paren
OL
l_int|0
)paren
)paren
suffix:semicolon
id|preempt_count
c_func
(paren
)paren
op_add_assign
id|val
suffix:semicolon
multiline_comment|/*&n;&t; * Spinlock count overflowing soon?&n;&t; */
id|BUG_ON
c_func
(paren
(paren
id|preempt_count
c_func
(paren
)paren
op_amp
id|PREEMPT_MASK
)paren
op_ge
id|PREEMPT_MASK
op_minus
l_int|10
)paren
suffix:semicolon
)brace
DECL|variable|add_preempt_count
id|EXPORT_SYMBOL
c_func
(paren
id|add_preempt_count
)paren
suffix:semicolon
DECL|function|sub_preempt_count
r_void
id|fastcall
id|sub_preempt_count
c_func
(paren
r_int
id|val
)paren
(brace
multiline_comment|/*&n;&t; * Underflow?&n;&t; */
id|BUG_ON
c_func
(paren
id|val
OG
id|preempt_count
c_func
(paren
)paren
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Is the spinlock portion underflowing?&n;&t; */
id|BUG_ON
c_func
(paren
(paren
id|val
OL
id|PREEMPT_MASK
)paren
op_logical_and
op_logical_neg
(paren
id|preempt_count
c_func
(paren
)paren
op_amp
id|PREEMPT_MASK
)paren
)paren
suffix:semicolon
id|preempt_count
c_func
(paren
)paren
op_sub_assign
id|val
suffix:semicolon
)brace
DECL|variable|sub_preempt_count
id|EXPORT_SYMBOL
c_func
(paren
id|sub_preempt_count
)paren
suffix:semicolon
macro_line|#endif
multiline_comment|/*&n; * schedule() is the main scheduler function.&n; */
DECL|function|schedule
id|asmlinkage
r_void
id|__sched
id|schedule
c_func
(paren
r_void
)paren
(brace
r_int
op_star
id|switch_count
suffix:semicolon
id|task_t
op_star
id|prev
comma
op_star
id|next
suffix:semicolon
id|runqueue_t
op_star
id|rq
suffix:semicolon
id|prio_array_t
op_star
id|array
suffix:semicolon
r_struct
id|list_head
op_star
id|queue
suffix:semicolon
r_int
r_int
r_int
id|now
suffix:semicolon
r_int
r_int
id|run_time
suffix:semicolon
r_int
id|cpu
comma
id|idx
suffix:semicolon
multiline_comment|/*&n;&t; * Test if we are atomic.  Since do_exit() needs to call into&n;&t; * schedule() atomically, we ignore that path for now.&n;&t; * Otherwise, whine if we are scheduling when we should not be.&n;&t; */
r_if
c_cond
(paren
id|likely
c_func
(paren
op_logical_neg
id|current-&gt;exit_state
)paren
)paren
(brace
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|in_atomic
c_func
(paren
)paren
)paren
)paren
(brace
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;scheduling while atomic: &quot;
l_string|&quot;%s/0x%08x/%d&bslash;n&quot;
comma
id|current-&gt;comm
comma
id|preempt_count
c_func
(paren
)paren
comma
id|current-&gt;pid
)paren
suffix:semicolon
id|dump_stack
c_func
(paren
)paren
suffix:semicolon
)brace
)brace
id|profile_hit
c_func
(paren
id|SCHED_PROFILING
comma
id|__builtin_return_address
c_func
(paren
l_int|0
)paren
)paren
suffix:semicolon
id|need_resched
suffix:colon
id|preempt_disable
c_func
(paren
)paren
suffix:semicolon
id|prev
op_assign
id|current
suffix:semicolon
id|release_kernel_lock
c_func
(paren
id|prev
)paren
suffix:semicolon
id|need_resched_nonpreemptible
suffix:colon
id|rq
op_assign
id|this_rq
c_func
(paren
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * The idle thread is not allowed to schedule!&n;&t; * Remove this check after it has been exercised a bit.&n;&t; */
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|prev
op_eq
id|rq-&gt;idle
)paren
op_logical_and
id|prev-&gt;state
op_ne
id|TASK_RUNNING
)paren
(brace
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;bad: scheduling from the idle thread!&bslash;n&quot;
)paren
suffix:semicolon
id|dump_stack
c_func
(paren
)paren
suffix:semicolon
)brace
id|schedstat_inc
c_func
(paren
id|rq
comma
id|sched_cnt
)paren
suffix:semicolon
id|now
op_assign
id|sched_clock
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|likely
c_func
(paren
id|now
op_minus
id|prev-&gt;timestamp
OL
id|NS_MAX_SLEEP_AVG
)paren
)paren
id|run_time
op_assign
id|now
op_minus
id|prev-&gt;timestamp
suffix:semicolon
r_else
id|run_time
op_assign
id|NS_MAX_SLEEP_AVG
suffix:semicolon
multiline_comment|/*&n;&t; * Tasks charged proportionately less run_time at high sleep_avg to&n;&t; * delay them losing their interactive status&n;&t; */
id|run_time
op_div_assign
(paren
id|CURRENT_BONUS
c_func
(paren
id|prev
)paren
ques
c_cond
suffix:colon
l_int|1
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|prev-&gt;flags
op_amp
id|PF_DEAD
)paren
)paren
id|prev-&gt;state
op_assign
id|EXIT_DEAD
suffix:semicolon
id|switch_count
op_assign
op_amp
id|prev-&gt;nivcsw
suffix:semicolon
r_if
c_cond
(paren
id|prev-&gt;state
op_logical_and
op_logical_neg
(paren
id|preempt_count
c_func
(paren
)paren
op_amp
id|PREEMPT_ACTIVE
)paren
)paren
(brace
id|switch_count
op_assign
op_amp
id|prev-&gt;nvcsw
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
(paren
id|prev-&gt;state
op_amp
id|TASK_INTERRUPTIBLE
)paren
op_logical_and
id|unlikely
c_func
(paren
id|signal_pending
c_func
(paren
id|prev
)paren
)paren
)paren
)paren
id|prev-&gt;state
op_assign
id|TASK_RUNNING
suffix:semicolon
r_else
(brace
r_if
c_cond
(paren
id|prev-&gt;state
op_eq
id|TASK_UNINTERRUPTIBLE
)paren
id|rq-&gt;nr_uninterruptible
op_increment
suffix:semicolon
id|deactivate_task
c_func
(paren
id|prev
comma
id|rq
)paren
suffix:semicolon
)brace
)brace
id|cpu
op_assign
id|smp_processor_id
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|rq-&gt;nr_running
)paren
)paren
(brace
id|go_idle
suffix:colon
id|idle_balance
c_func
(paren
id|cpu
comma
id|rq
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|rq-&gt;nr_running
)paren
(brace
id|next
op_assign
id|rq-&gt;idle
suffix:semicolon
id|rq-&gt;expired_timestamp
op_assign
l_int|0
suffix:semicolon
id|wake_sleeping_dependent
c_func
(paren
id|cpu
comma
id|rq
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t;&t; * wake_sleeping_dependent() might have released&n;&t;&t;&t; * the runqueue, so break out if we got new&n;&t;&t;&t; * tasks meanwhile:&n;&t;&t;&t; */
r_if
c_cond
(paren
op_logical_neg
id|rq-&gt;nr_running
)paren
r_goto
id|switch_tasks
suffix:semicolon
)brace
)brace
r_else
(brace
r_if
c_cond
(paren
id|dependent_sleeper
c_func
(paren
id|cpu
comma
id|rq
)paren
)paren
(brace
id|next
op_assign
id|rq-&gt;idle
suffix:semicolon
r_goto
id|switch_tasks
suffix:semicolon
)brace
multiline_comment|/*&n;&t;&t; * dependent_sleeper() releases and reacquires the runqueue&n;&t;&t; * lock, hence go into the idle loop if the rq went&n;&t;&t; * empty meanwhile:&n;&t;&t; */
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|rq-&gt;nr_running
)paren
)paren
r_goto
id|go_idle
suffix:semicolon
)brace
id|array
op_assign
id|rq-&gt;active
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|array-&gt;nr_active
)paren
)paren
(brace
multiline_comment|/*&n;&t;&t; * Switch the active and expired arrays.&n;&t;&t; */
id|schedstat_inc
c_func
(paren
id|rq
comma
id|sched_switch
)paren
suffix:semicolon
id|rq-&gt;active
op_assign
id|rq-&gt;expired
suffix:semicolon
id|rq-&gt;expired
op_assign
id|array
suffix:semicolon
id|array
op_assign
id|rq-&gt;active
suffix:semicolon
id|rq-&gt;expired_timestamp
op_assign
l_int|0
suffix:semicolon
id|rq-&gt;best_expired_prio
op_assign
id|MAX_PRIO
suffix:semicolon
)brace
r_else
id|schedstat_inc
c_func
(paren
id|rq
comma
id|sched_noswitch
)paren
suffix:semicolon
id|idx
op_assign
id|sched_find_first_bit
c_func
(paren
id|array-&gt;bitmap
)paren
suffix:semicolon
id|queue
op_assign
id|array-&gt;queue
op_plus
id|idx
suffix:semicolon
id|next
op_assign
id|list_entry
c_func
(paren
id|queue-&gt;next
comma
id|task_t
comma
id|run_list
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|rt_task
c_func
(paren
id|next
)paren
op_logical_and
id|next-&gt;activated
OG
l_int|0
)paren
(brace
r_int
r_int
r_int
id|delta
op_assign
id|now
op_minus
id|next-&gt;timestamp
suffix:semicolon
r_if
c_cond
(paren
id|next-&gt;activated
op_eq
l_int|1
)paren
id|delta
op_assign
id|delta
op_star
(paren
id|ON_RUNQUEUE_WEIGHT
op_star
l_int|128
op_div
l_int|100
)paren
op_div
l_int|128
suffix:semicolon
id|array
op_assign
id|next-&gt;array
suffix:semicolon
id|dequeue_task
c_func
(paren
id|next
comma
id|array
)paren
suffix:semicolon
id|recalc_task_prio
c_func
(paren
id|next
comma
id|next-&gt;timestamp
op_plus
id|delta
)paren
suffix:semicolon
id|enqueue_task
c_func
(paren
id|next
comma
id|array
)paren
suffix:semicolon
)brace
id|next-&gt;activated
op_assign
l_int|0
suffix:semicolon
id|switch_tasks
suffix:colon
r_if
c_cond
(paren
id|next
op_eq
id|rq-&gt;idle
)paren
id|schedstat_inc
c_func
(paren
id|rq
comma
id|sched_goidle
)paren
suffix:semicolon
id|prefetch
c_func
(paren
id|next
)paren
suffix:semicolon
id|clear_tsk_need_resched
c_func
(paren
id|prev
)paren
suffix:semicolon
id|rcu_qsctr_inc
c_func
(paren
id|task_cpu
c_func
(paren
id|prev
)paren
)paren
suffix:semicolon
id|prev-&gt;sleep_avg
op_sub_assign
id|run_time
suffix:semicolon
r_if
c_cond
(paren
(paren
r_int
)paren
id|prev-&gt;sleep_avg
op_le
l_int|0
)paren
id|prev-&gt;sleep_avg
op_assign
l_int|0
suffix:semicolon
id|prev-&gt;timestamp
op_assign
id|prev-&gt;last_ran
op_assign
id|now
suffix:semicolon
id|sched_info_switch
c_func
(paren
id|prev
comma
id|next
)paren
suffix:semicolon
r_if
c_cond
(paren
id|likely
c_func
(paren
id|prev
op_ne
id|next
)paren
)paren
(brace
id|next-&gt;timestamp
op_assign
id|now
suffix:semicolon
id|rq-&gt;nr_switches
op_increment
suffix:semicolon
id|rq-&gt;curr
op_assign
id|next
suffix:semicolon
op_increment
op_star
id|switch_count
suffix:semicolon
id|prepare_arch_switch
c_func
(paren
id|rq
comma
id|next
)paren
suffix:semicolon
id|prev
op_assign
id|context_switch
c_func
(paren
id|rq
comma
id|prev
comma
id|next
)paren
suffix:semicolon
id|barrier
c_func
(paren
)paren
suffix:semicolon
id|finish_task_switch
c_func
(paren
id|prev
)paren
suffix:semicolon
)brace
r_else
id|spin_unlock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
id|prev
op_assign
id|current
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|reacquire_kernel_lock
c_func
(paren
id|prev
)paren
OL
l_int|0
)paren
)paren
r_goto
id|need_resched_nonpreemptible
suffix:semicolon
id|preempt_enable_no_resched
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|test_thread_flag
c_func
(paren
id|TIF_NEED_RESCHED
)paren
)paren
)paren
r_goto
id|need_resched
suffix:semicolon
)brace
DECL|variable|schedule
id|EXPORT_SYMBOL
c_func
(paren
id|schedule
)paren
suffix:semicolon
macro_line|#ifdef CONFIG_PREEMPT
multiline_comment|/*&n; * this is is the entry point to schedule() from in-kernel preemption&n; * off of preempt_enable.  Kernel preemptions off return from interrupt&n; * occur there and call schedule directly.&n; */
DECL|function|preempt_schedule
id|asmlinkage
r_void
id|__sched
id|preempt_schedule
c_func
(paren
r_void
)paren
(brace
r_struct
id|thread_info
op_star
id|ti
op_assign
id|current_thread_info
c_func
(paren
)paren
suffix:semicolon
macro_line|#ifdef CONFIG_PREEMPT_BKL
r_struct
id|task_struct
op_star
id|task
op_assign
id|current
suffix:semicolon
r_int
id|saved_lock_depth
suffix:semicolon
macro_line|#endif
multiline_comment|/*&n;&t; * If there is a non-zero preempt_count or interrupts are disabled,&n;&t; * we do not want to preempt the current task.  Just return..&n;&t; */
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|ti-&gt;preempt_count
op_logical_or
id|irqs_disabled
c_func
(paren
)paren
)paren
)paren
r_return
suffix:semicolon
id|need_resched
suffix:colon
id|add_preempt_count
c_func
(paren
id|PREEMPT_ACTIVE
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * We keep the big kernel semaphore locked, but we&n;&t; * clear -&gt;lock_depth so that schedule() doesnt&n;&t; * auto-release the semaphore:&n;&t; */
macro_line|#ifdef CONFIG_PREEMPT_BKL
id|saved_lock_depth
op_assign
id|task-&gt;lock_depth
suffix:semicolon
id|task-&gt;lock_depth
op_assign
op_minus
l_int|1
suffix:semicolon
macro_line|#endif
id|schedule
c_func
(paren
)paren
suffix:semicolon
macro_line|#ifdef CONFIG_PREEMPT_BKL
id|task-&gt;lock_depth
op_assign
id|saved_lock_depth
suffix:semicolon
macro_line|#endif
id|sub_preempt_count
c_func
(paren
id|PREEMPT_ACTIVE
)paren
suffix:semicolon
multiline_comment|/* we could miss a preemption opportunity between schedule and now */
id|barrier
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|test_thread_flag
c_func
(paren
id|TIF_NEED_RESCHED
)paren
)paren
)paren
r_goto
id|need_resched
suffix:semicolon
)brace
DECL|variable|preempt_schedule
id|EXPORT_SYMBOL
c_func
(paren
id|preempt_schedule
)paren
suffix:semicolon
macro_line|#endif /* CONFIG_PREEMPT */
DECL|function|default_wake_function
r_int
id|default_wake_function
c_func
(paren
id|wait_queue_t
op_star
id|curr
comma
r_int
id|mode
comma
r_int
id|sync
comma
r_void
op_star
id|key
)paren
(brace
id|task_t
op_star
id|p
op_assign
id|curr-&gt;task
suffix:semicolon
r_return
id|try_to_wake_up
c_func
(paren
id|p
comma
id|mode
comma
id|sync
)paren
suffix:semicolon
)brace
DECL|variable|default_wake_function
id|EXPORT_SYMBOL
c_func
(paren
id|default_wake_function
)paren
suffix:semicolon
multiline_comment|/*&n; * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just&n; * wake everything up.  If it&squot;s an exclusive wakeup (nr_exclusive == small +ve&n; * number) then we wake all the non-exclusive tasks and one exclusive task.&n; *&n; * There are circumstances in which we can try to wake a task which has already&n; * started to run but is not in state TASK_RUNNING.  try_to_wake_up() returns&n; * zero in this (rare) case, and we handle it by continuing to scan the queue.&n; */
DECL|function|__wake_up_common
r_static
r_void
id|__wake_up_common
c_func
(paren
id|wait_queue_head_t
op_star
id|q
comma
r_int
r_int
id|mode
comma
r_int
id|nr_exclusive
comma
r_int
id|sync
comma
r_void
op_star
id|key
)paren
(brace
r_struct
id|list_head
op_star
id|tmp
comma
op_star
id|next
suffix:semicolon
id|list_for_each_safe
c_func
(paren
id|tmp
comma
id|next
comma
op_amp
id|q-&gt;task_list
)paren
(brace
id|wait_queue_t
op_star
id|curr
suffix:semicolon
r_int
id|flags
suffix:semicolon
id|curr
op_assign
id|list_entry
c_func
(paren
id|tmp
comma
id|wait_queue_t
comma
id|task_list
)paren
suffix:semicolon
id|flags
op_assign
id|curr-&gt;flags
suffix:semicolon
r_if
c_cond
(paren
id|curr
op_member_access_from_pointer
id|func
c_func
(paren
id|curr
comma
id|mode
comma
id|sync
comma
id|key
)paren
op_logical_and
(paren
id|flags
op_amp
id|WQ_FLAG_EXCLUSIVE
)paren
op_logical_and
op_logical_neg
op_decrement
id|nr_exclusive
)paren
r_break
suffix:semicolon
)brace
)brace
multiline_comment|/**&n; * __wake_up - wake up threads blocked on a waitqueue.&n; * @q: the waitqueue&n; * @mode: which threads&n; * @nr_exclusive: how many wake-one or wake-many threads to wake up&n; */
DECL|function|__wake_up
r_void
id|fastcall
id|__wake_up
c_func
(paren
id|wait_queue_head_t
op_star
id|q
comma
r_int
r_int
id|mode
comma
r_int
id|nr_exclusive
comma
r_void
op_star
id|key
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
id|spin_lock_irqsave
c_func
(paren
op_amp
id|q-&gt;lock
comma
id|flags
)paren
suffix:semicolon
id|__wake_up_common
c_func
(paren
id|q
comma
id|mode
comma
id|nr_exclusive
comma
l_int|0
comma
id|key
)paren
suffix:semicolon
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|q-&gt;lock
comma
id|flags
)paren
suffix:semicolon
)brace
DECL|variable|__wake_up
id|EXPORT_SYMBOL
c_func
(paren
id|__wake_up
)paren
suffix:semicolon
multiline_comment|/*&n; * Same as __wake_up but called with the spinlock in wait_queue_head_t held.&n; */
DECL|function|__wake_up_locked
r_void
id|fastcall
id|__wake_up_locked
c_func
(paren
id|wait_queue_head_t
op_star
id|q
comma
r_int
r_int
id|mode
)paren
(brace
id|__wake_up_common
c_func
(paren
id|q
comma
id|mode
comma
l_int|1
comma
l_int|0
comma
l_int|NULL
)paren
suffix:semicolon
)brace
multiline_comment|/**&n; * __wake_up - sync- wake up threads blocked on a waitqueue.&n; * @q: the waitqueue&n; * @mode: which threads&n; * @nr_exclusive: how many wake-one or wake-many threads to wake up&n; *&n; * The sync wakeup differs that the waker knows that it will schedule&n; * away soon, so while the target thread will be woken up, it will not&n; * be migrated to another CPU - ie. the two threads are &squot;synchronized&squot;&n; * with each other. This can prevent needless bouncing between CPUs.&n; *&n; * On UP it can prevent extra preemption.&n; */
DECL|function|__wake_up_sync
r_void
id|fastcall
id|__wake_up_sync
c_func
(paren
id|wait_queue_head_t
op_star
id|q
comma
r_int
r_int
id|mode
comma
r_int
id|nr_exclusive
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
r_int
id|sync
op_assign
l_int|1
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|q
)paren
)paren
r_return
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
op_logical_neg
id|nr_exclusive
)paren
)paren
id|sync
op_assign
l_int|0
suffix:semicolon
id|spin_lock_irqsave
c_func
(paren
op_amp
id|q-&gt;lock
comma
id|flags
)paren
suffix:semicolon
id|__wake_up_common
c_func
(paren
id|q
comma
id|mode
comma
id|nr_exclusive
comma
id|sync
comma
l_int|NULL
)paren
suffix:semicolon
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|q-&gt;lock
comma
id|flags
)paren
suffix:semicolon
)brace
DECL|variable|__wake_up_sync
id|EXPORT_SYMBOL_GPL
c_func
(paren
id|__wake_up_sync
)paren
suffix:semicolon
multiline_comment|/* For internal use only */
DECL|function|complete
r_void
id|fastcall
id|complete
c_func
(paren
r_struct
id|completion
op_star
id|x
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
id|spin_lock_irqsave
c_func
(paren
op_amp
id|x-&gt;wait.lock
comma
id|flags
)paren
suffix:semicolon
id|x-&gt;done
op_increment
suffix:semicolon
id|__wake_up_common
c_func
(paren
op_amp
id|x-&gt;wait
comma
id|TASK_UNINTERRUPTIBLE
op_or
id|TASK_INTERRUPTIBLE
comma
l_int|1
comma
l_int|0
comma
l_int|NULL
)paren
suffix:semicolon
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|x-&gt;wait.lock
comma
id|flags
)paren
suffix:semicolon
)brace
DECL|variable|complete
id|EXPORT_SYMBOL
c_func
(paren
id|complete
)paren
suffix:semicolon
DECL|function|complete_all
r_void
id|fastcall
id|complete_all
c_func
(paren
r_struct
id|completion
op_star
id|x
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
id|spin_lock_irqsave
c_func
(paren
op_amp
id|x-&gt;wait.lock
comma
id|flags
)paren
suffix:semicolon
id|x-&gt;done
op_add_assign
id|UINT_MAX
op_div
l_int|2
suffix:semicolon
id|__wake_up_common
c_func
(paren
op_amp
id|x-&gt;wait
comma
id|TASK_UNINTERRUPTIBLE
op_or
id|TASK_INTERRUPTIBLE
comma
l_int|0
comma
l_int|0
comma
l_int|NULL
)paren
suffix:semicolon
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|x-&gt;wait.lock
comma
id|flags
)paren
suffix:semicolon
)brace
DECL|variable|complete_all
id|EXPORT_SYMBOL
c_func
(paren
id|complete_all
)paren
suffix:semicolon
DECL|function|wait_for_completion
r_void
id|fastcall
id|__sched
id|wait_for_completion
c_func
(paren
r_struct
id|completion
op_star
id|x
)paren
(brace
id|might_sleep
c_func
(paren
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|x-&gt;done
)paren
(brace
id|DECLARE_WAITQUEUE
c_func
(paren
id|wait
comma
id|current
)paren
suffix:semicolon
id|wait.flags
op_or_assign
id|WQ_FLAG_EXCLUSIVE
suffix:semicolon
id|__add_wait_queue_tail
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
r_do
(brace
id|__set_current_state
c_func
(paren
id|TASK_UNINTERRUPTIBLE
)paren
suffix:semicolon
id|spin_unlock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
id|schedule
c_func
(paren
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
)brace
r_while
c_loop
(paren
op_logical_neg
id|x-&gt;done
)paren
suffix:semicolon
id|__remove_wait_queue
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
)brace
id|x-&gt;done
op_decrement
suffix:semicolon
id|spin_unlock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
)brace
DECL|variable|wait_for_completion
id|EXPORT_SYMBOL
c_func
(paren
id|wait_for_completion
)paren
suffix:semicolon
r_int
r_int
id|fastcall
id|__sched
DECL|function|wait_for_completion_timeout
id|wait_for_completion_timeout
c_func
(paren
r_struct
id|completion
op_star
id|x
comma
r_int
r_int
id|timeout
)paren
(brace
id|might_sleep
c_func
(paren
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|x-&gt;done
)paren
(brace
id|DECLARE_WAITQUEUE
c_func
(paren
id|wait
comma
id|current
)paren
suffix:semicolon
id|wait.flags
op_or_assign
id|WQ_FLAG_EXCLUSIVE
suffix:semicolon
id|__add_wait_queue_tail
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
r_do
(brace
id|__set_current_state
c_func
(paren
id|TASK_UNINTERRUPTIBLE
)paren
suffix:semicolon
id|spin_unlock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
id|timeout
op_assign
id|schedule_timeout
c_func
(paren
id|timeout
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|timeout
)paren
(brace
id|__remove_wait_queue
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
r_goto
id|out
suffix:semicolon
)brace
)brace
r_while
c_loop
(paren
op_logical_neg
id|x-&gt;done
)paren
suffix:semicolon
id|__remove_wait_queue
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
)brace
id|x-&gt;done
op_decrement
suffix:semicolon
id|out
suffix:colon
id|spin_unlock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
r_return
id|timeout
suffix:semicolon
)brace
DECL|variable|wait_for_completion_timeout
id|EXPORT_SYMBOL
c_func
(paren
id|wait_for_completion_timeout
)paren
suffix:semicolon
DECL|function|wait_for_completion_interruptible
r_int
id|fastcall
id|__sched
id|wait_for_completion_interruptible
c_func
(paren
r_struct
id|completion
op_star
id|x
)paren
(brace
r_int
id|ret
op_assign
l_int|0
suffix:semicolon
id|might_sleep
c_func
(paren
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|x-&gt;done
)paren
(brace
id|DECLARE_WAITQUEUE
c_func
(paren
id|wait
comma
id|current
)paren
suffix:semicolon
id|wait.flags
op_or_assign
id|WQ_FLAG_EXCLUSIVE
suffix:semicolon
id|__add_wait_queue_tail
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
r_do
(brace
r_if
c_cond
(paren
id|signal_pending
c_func
(paren
id|current
)paren
)paren
(brace
id|ret
op_assign
op_minus
id|ERESTARTSYS
suffix:semicolon
id|__remove_wait_queue
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
r_goto
id|out
suffix:semicolon
)brace
id|__set_current_state
c_func
(paren
id|TASK_INTERRUPTIBLE
)paren
suffix:semicolon
id|spin_unlock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
id|schedule
c_func
(paren
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
)brace
r_while
c_loop
(paren
op_logical_neg
id|x-&gt;done
)paren
suffix:semicolon
id|__remove_wait_queue
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
)brace
id|x-&gt;done
op_decrement
suffix:semicolon
id|out
suffix:colon
id|spin_unlock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
r_return
id|ret
suffix:semicolon
)brace
DECL|variable|wait_for_completion_interruptible
id|EXPORT_SYMBOL
c_func
(paren
id|wait_for_completion_interruptible
)paren
suffix:semicolon
r_int
r_int
id|fastcall
id|__sched
DECL|function|wait_for_completion_interruptible_timeout
id|wait_for_completion_interruptible_timeout
c_func
(paren
r_struct
id|completion
op_star
id|x
comma
r_int
r_int
id|timeout
)paren
(brace
id|might_sleep
c_func
(paren
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|x-&gt;done
)paren
(brace
id|DECLARE_WAITQUEUE
c_func
(paren
id|wait
comma
id|current
)paren
suffix:semicolon
id|wait.flags
op_or_assign
id|WQ_FLAG_EXCLUSIVE
suffix:semicolon
id|__add_wait_queue_tail
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
r_do
(brace
r_if
c_cond
(paren
id|signal_pending
c_func
(paren
id|current
)paren
)paren
(brace
id|timeout
op_assign
op_minus
id|ERESTARTSYS
suffix:semicolon
id|__remove_wait_queue
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
r_goto
id|out
suffix:semicolon
)brace
id|__set_current_state
c_func
(paren
id|TASK_INTERRUPTIBLE
)paren
suffix:semicolon
id|spin_unlock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
id|timeout
op_assign
id|schedule_timeout
c_func
(paren
id|timeout
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|timeout
)paren
(brace
id|__remove_wait_queue
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
r_goto
id|out
suffix:semicolon
)brace
)brace
r_while
c_loop
(paren
op_logical_neg
id|x-&gt;done
)paren
suffix:semicolon
id|__remove_wait_queue
c_func
(paren
op_amp
id|x-&gt;wait
comma
op_amp
id|wait
)paren
suffix:semicolon
)brace
id|x-&gt;done
op_decrement
suffix:semicolon
id|out
suffix:colon
id|spin_unlock_irq
c_func
(paren
op_amp
id|x-&gt;wait.lock
)paren
suffix:semicolon
r_return
id|timeout
suffix:semicolon
)brace
DECL|variable|wait_for_completion_interruptible_timeout
id|EXPORT_SYMBOL
c_func
(paren
id|wait_for_completion_interruptible_timeout
)paren
suffix:semicolon
DECL|macro|SLEEP_ON_VAR
mdefine_line|#define&t;SLEEP_ON_VAR&t;&t;&t;&t;&t;&bslash;&n;&t;unsigned long flags;&t;&t;&t;&t;&bslash;&n;&t;wait_queue_t wait;&t;&t;&t;&t;&bslash;&n;&t;init_waitqueue_entry(&amp;wait, current);
DECL|macro|SLEEP_ON_HEAD
mdefine_line|#define SLEEP_ON_HEAD&t;&t;&t;&t;&t;&bslash;&n;&t;spin_lock_irqsave(&amp;q-&gt;lock,flags);&t;&t;&bslash;&n;&t;__add_wait_queue(q, &amp;wait);&t;&t;&t;&bslash;&n;&t;spin_unlock(&amp;q-&gt;lock);
DECL|macro|SLEEP_ON_TAIL
mdefine_line|#define&t;SLEEP_ON_TAIL&t;&t;&t;&t;&t;&bslash;&n;&t;spin_lock_irq(&amp;q-&gt;lock);&t;&t;&t;&bslash;&n;&t;__remove_wait_queue(q, &amp;wait);&t;&t;&t;&bslash;&n;&t;spin_unlock_irqrestore(&amp;q-&gt;lock, flags);
DECL|function|interruptible_sleep_on
r_void
id|fastcall
id|__sched
id|interruptible_sleep_on
c_func
(paren
id|wait_queue_head_t
op_star
id|q
)paren
(brace
id|SLEEP_ON_VAR
id|current-&gt;state
op_assign
id|TASK_INTERRUPTIBLE
suffix:semicolon
id|SLEEP_ON_HEAD
id|schedule
c_func
(paren
)paren
suffix:semicolon
id|SLEEP_ON_TAIL
)brace
DECL|variable|interruptible_sleep_on
id|EXPORT_SYMBOL
c_func
(paren
id|interruptible_sleep_on
)paren
suffix:semicolon
DECL|function|interruptible_sleep_on_timeout
r_int
id|fastcall
id|__sched
id|interruptible_sleep_on_timeout
c_func
(paren
id|wait_queue_head_t
op_star
id|q
comma
r_int
id|timeout
)paren
(brace
id|SLEEP_ON_VAR
id|current-&gt;state
op_assign
id|TASK_INTERRUPTIBLE
suffix:semicolon
id|SLEEP_ON_HEAD
id|timeout
op_assign
id|schedule_timeout
c_func
(paren
id|timeout
)paren
suffix:semicolon
id|SLEEP_ON_TAIL
r_return
id|timeout
suffix:semicolon
)brace
DECL|variable|interruptible_sleep_on_timeout
id|EXPORT_SYMBOL
c_func
(paren
id|interruptible_sleep_on_timeout
)paren
suffix:semicolon
DECL|function|sleep_on
r_void
id|fastcall
id|__sched
id|sleep_on
c_func
(paren
id|wait_queue_head_t
op_star
id|q
)paren
(brace
id|SLEEP_ON_VAR
id|current-&gt;state
op_assign
id|TASK_UNINTERRUPTIBLE
suffix:semicolon
id|SLEEP_ON_HEAD
id|schedule
c_func
(paren
)paren
suffix:semicolon
id|SLEEP_ON_TAIL
)brace
DECL|variable|sleep_on
id|EXPORT_SYMBOL
c_func
(paren
id|sleep_on
)paren
suffix:semicolon
DECL|function|sleep_on_timeout
r_int
id|fastcall
id|__sched
id|sleep_on_timeout
c_func
(paren
id|wait_queue_head_t
op_star
id|q
comma
r_int
id|timeout
)paren
(brace
id|SLEEP_ON_VAR
id|current-&gt;state
op_assign
id|TASK_UNINTERRUPTIBLE
suffix:semicolon
id|SLEEP_ON_HEAD
id|timeout
op_assign
id|schedule_timeout
c_func
(paren
id|timeout
)paren
suffix:semicolon
id|SLEEP_ON_TAIL
r_return
id|timeout
suffix:semicolon
)brace
DECL|variable|sleep_on_timeout
id|EXPORT_SYMBOL
c_func
(paren
id|sleep_on_timeout
)paren
suffix:semicolon
DECL|function|set_user_nice
r_void
id|set_user_nice
c_func
(paren
id|task_t
op_star
id|p
comma
r_int
id|nice
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
id|prio_array_t
op_star
id|array
suffix:semicolon
id|runqueue_t
op_star
id|rq
suffix:semicolon
r_int
id|old_prio
comma
id|new_prio
comma
id|delta
suffix:semicolon
r_if
c_cond
(paren
id|TASK_NICE
c_func
(paren
id|p
)paren
op_eq
id|nice
op_logical_or
id|nice
template_param
l_int|19
)paren
r_return
suffix:semicolon
multiline_comment|/*&n;&t; * We have to be careful, if called from sys_setpriority(),&n;&t; * the task might be in the middle of scheduling on another CPU.&n;&t; */
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * The RT priorities are set via sched_setscheduler(), but we still&n;&t; * allow the &squot;normal&squot; nice value to be set - but as expected&n;&t; * it wont have any effect on scheduling until the task is&n;&t; * not SCHED_NORMAL:&n;&t; */
r_if
c_cond
(paren
id|rt_task
c_func
(paren
id|p
)paren
)paren
(brace
id|p-&gt;static_prio
op_assign
id|NICE_TO_PRIO
c_func
(paren
id|nice
)paren
suffix:semicolon
r_goto
id|out_unlock
suffix:semicolon
)brace
id|array
op_assign
id|p-&gt;array
suffix:semicolon
r_if
c_cond
(paren
id|array
)paren
id|dequeue_task
c_func
(paren
id|p
comma
id|array
)paren
suffix:semicolon
id|old_prio
op_assign
id|p-&gt;prio
suffix:semicolon
id|new_prio
op_assign
id|NICE_TO_PRIO
c_func
(paren
id|nice
)paren
suffix:semicolon
id|delta
op_assign
id|new_prio
op_minus
id|old_prio
suffix:semicolon
id|p-&gt;static_prio
op_assign
id|NICE_TO_PRIO
c_func
(paren
id|nice
)paren
suffix:semicolon
id|p-&gt;prio
op_add_assign
id|delta
suffix:semicolon
r_if
c_cond
(paren
id|array
)paren
(brace
id|enqueue_task
c_func
(paren
id|p
comma
id|array
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t; * If the task increased its priority or is running and&n;&t;&t; * lowered its priority, then reschedule its CPU:&n;&t;&t; */
r_if
c_cond
(paren
id|delta
OL
l_int|0
op_logical_or
(paren
id|delta
OG
l_int|0
op_logical_and
id|task_running
c_func
(paren
id|rq
comma
id|p
)paren
)paren
)paren
id|resched_task
c_func
(paren
id|rq-&gt;curr
)paren
suffix:semicolon
)brace
id|out_unlock
suffix:colon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
)brace
DECL|variable|set_user_nice
id|EXPORT_SYMBOL
c_func
(paren
id|set_user_nice
)paren
suffix:semicolon
macro_line|#ifdef __ARCH_WANT_SYS_NICE
multiline_comment|/*&n; * sys_nice - change the priority of the current process.&n; * @increment: priority increment&n; *&n; * sys_setpriority is a more generic, but much slower function that&n; * does similar things.&n; */
DECL|function|sys_nice
id|asmlinkage
r_int
id|sys_nice
c_func
(paren
r_int
id|increment
)paren
(brace
r_int
id|retval
suffix:semicolon
r_int
id|nice
suffix:semicolon
multiline_comment|/*&n;&t; * Setpriority might change our priority at the same moment.&n;&t; * We don&squot;t have to worry. Conceptually one call occurs first&n;&t; * and we have a single winner.&n;&t; */
r_if
c_cond
(paren
id|increment
OL
l_int|0
)paren
(brace
r_if
c_cond
(paren
op_logical_neg
id|capable
c_func
(paren
id|CAP_SYS_NICE
)paren
)paren
r_return
op_minus
id|EPERM
suffix:semicolon
r_if
c_cond
(paren
id|increment
OL
op_minus
l_int|40
)paren
id|increment
op_assign
op_minus
l_int|40
suffix:semicolon
)brace
r_if
c_cond
(paren
id|increment
OG
l_int|40
)paren
id|increment
op_assign
l_int|40
suffix:semicolon
id|nice
op_assign
id|PRIO_TO_NICE
c_func
(paren
id|current-&gt;static_prio
)paren
op_plus
id|increment
suffix:semicolon
r_if
c_cond
(paren
id|nice
OL
op_minus
l_int|20
)paren
id|nice
op_assign
op_minus
l_int|20
suffix:semicolon
r_if
c_cond
(paren
id|nice
OG
l_int|19
)paren
id|nice
op_assign
l_int|19
suffix:semicolon
id|retval
op_assign
id|security_task_setnice
c_func
(paren
id|current
comma
id|nice
)paren
suffix:semicolon
r_if
c_cond
(paren
id|retval
)paren
r_return
id|retval
suffix:semicolon
id|set_user_nice
c_func
(paren
id|current
comma
id|nice
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
macro_line|#endif
multiline_comment|/**&n; * task_prio - return the priority value of a given task.&n; * @p: the task in question.&n; *&n; * This is the priority value as seen by users in /proc.&n; * RT tasks are offset by -200. Normal tasks are centered&n; * around 0, value goes from -16 to +15.&n; */
DECL|function|task_prio
r_int
id|task_prio
c_func
(paren
r_const
id|task_t
op_star
id|p
)paren
(brace
r_return
id|p-&gt;prio
op_minus
id|MAX_RT_PRIO
suffix:semicolon
)brace
multiline_comment|/**&n; * task_nice - return the nice value of a given task.&n; * @p: the task in question.&n; */
DECL|function|task_nice
r_int
id|task_nice
c_func
(paren
r_const
id|task_t
op_star
id|p
)paren
(brace
r_return
id|TASK_NICE
c_func
(paren
id|p
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * The only users of task_nice are binfmt_elf and binfmt_elf32.&n; * binfmt_elf is no longer modular, but binfmt_elf32 still is.&n; * Therefore, task_nice is needed if there is a compat_mode.&n; */
macro_line|#ifdef CONFIG_COMPAT
DECL|variable|task_nice
id|EXPORT_SYMBOL_GPL
c_func
(paren
id|task_nice
)paren
suffix:semicolon
macro_line|#endif
multiline_comment|/**&n; * idle_cpu - is a given cpu idle currently?&n; * @cpu: the processor in question.&n; */
DECL|function|idle_cpu
r_int
id|idle_cpu
c_func
(paren
r_int
id|cpu
)paren
(brace
r_return
id|cpu_curr
c_func
(paren
id|cpu
)paren
op_eq
id|cpu_rq
c_func
(paren
id|cpu
)paren
op_member_access_from_pointer
id|idle
suffix:semicolon
)brace
DECL|variable|idle_cpu
id|EXPORT_SYMBOL_GPL
c_func
(paren
id|idle_cpu
)paren
suffix:semicolon
multiline_comment|/**&n; * idle_task - return the idle task for a given cpu.&n; * @cpu: the processor in question.&n; */
DECL|function|idle_task
id|task_t
op_star
id|idle_task
c_func
(paren
r_int
id|cpu
)paren
(brace
r_return
id|cpu_rq
c_func
(paren
id|cpu
)paren
op_member_access_from_pointer
id|idle
suffix:semicolon
)brace
multiline_comment|/**&n; * find_process_by_pid - find a process with a matching PID value.&n; * @pid: the pid in question.&n; */
DECL|function|find_process_by_pid
r_static
r_inline
id|task_t
op_star
id|find_process_by_pid
c_func
(paren
id|pid_t
id|pid
)paren
(brace
r_return
id|pid
ques
c_cond
id|find_task_by_pid
c_func
(paren
id|pid
)paren
suffix:colon
id|current
suffix:semicolon
)brace
multiline_comment|/* Actually do priority change: must hold rq lock. */
DECL|function|__setscheduler
r_static
r_void
id|__setscheduler
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
r_int
id|policy
comma
r_int
id|prio
)paren
(brace
id|BUG_ON
c_func
(paren
id|p-&gt;array
)paren
suffix:semicolon
id|p-&gt;policy
op_assign
id|policy
suffix:semicolon
id|p-&gt;rt_priority
op_assign
id|prio
suffix:semicolon
r_if
c_cond
(paren
id|policy
op_ne
id|SCHED_NORMAL
)paren
id|p-&gt;prio
op_assign
id|MAX_USER_RT_PRIO
op_minus
l_int|1
op_minus
id|p-&gt;rt_priority
suffix:semicolon
r_else
id|p-&gt;prio
op_assign
id|p-&gt;static_prio
suffix:semicolon
)brace
multiline_comment|/**&n; * sched_setscheduler - change the scheduling policy and/or RT priority of&n; * a thread.&n; * @p: the task in question.&n; * @policy: new policy.&n; * @param: structure containing the new RT priority.&n; */
DECL|function|sched_setscheduler
r_int
id|sched_setscheduler
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
r_int
id|policy
comma
r_struct
id|sched_param
op_star
id|param
)paren
(brace
r_int
id|retval
suffix:semicolon
r_int
id|oldprio
comma
id|oldpolicy
op_assign
op_minus
l_int|1
suffix:semicolon
id|prio_array_t
op_star
id|array
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
id|runqueue_t
op_star
id|rq
suffix:semicolon
id|recheck
suffix:colon
multiline_comment|/* double check policy once rq lock held */
r_if
c_cond
(paren
id|policy
OL
l_int|0
)paren
id|policy
op_assign
id|oldpolicy
op_assign
id|p-&gt;policy
suffix:semicolon
r_else
r_if
c_cond
(paren
id|policy
op_ne
id|SCHED_FIFO
op_logical_and
id|policy
op_ne
id|SCHED_RR
op_logical_and
id|policy
op_ne
id|SCHED_NORMAL
)paren
r_return
op_minus
id|EINVAL
suffix:semicolon
multiline_comment|/*&n;&t; * Valid priorities for SCHED_FIFO and SCHED_RR are&n;&t; * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL is 0.&n;&t; */
r_if
c_cond
(paren
id|param-&gt;sched_priority
template_param
id|MAX_USER_RT_PRIO
op_minus
l_int|1
)paren
r_return
op_minus
id|EINVAL
suffix:semicolon
r_if
c_cond
(paren
(paren
id|policy
op_eq
id|SCHED_NORMAL
)paren
op_ne
(paren
id|param-&gt;sched_priority
op_eq
l_int|0
)paren
)paren
r_return
op_minus
id|EINVAL
suffix:semicolon
r_if
c_cond
(paren
(paren
id|policy
op_eq
id|SCHED_FIFO
op_logical_or
id|policy
op_eq
id|SCHED_RR
)paren
op_logical_and
op_logical_neg
id|capable
c_func
(paren
id|CAP_SYS_NICE
)paren
)paren
r_return
op_minus
id|EPERM
suffix:semicolon
r_if
c_cond
(paren
(paren
id|current-&gt;euid
op_ne
id|p-&gt;euid
)paren
op_logical_and
(paren
id|current-&gt;euid
op_ne
id|p-&gt;uid
)paren
op_logical_and
op_logical_neg
id|capable
c_func
(paren
id|CAP_SYS_NICE
)paren
)paren
r_return
op_minus
id|EPERM
suffix:semicolon
id|retval
op_assign
id|security_task_setscheduler
c_func
(paren
id|p
comma
id|policy
comma
id|param
)paren
suffix:semicolon
r_if
c_cond
(paren
id|retval
)paren
r_return
id|retval
suffix:semicolon
multiline_comment|/*&n;&t; * To be able to change p-&gt;policy safely, the apropriate&n;&t; * runqueue lock must be held.&n;&t; */
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
multiline_comment|/* recheck policy now with rq lock held */
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|oldpolicy
op_ne
op_minus
l_int|1
op_logical_and
id|oldpolicy
op_ne
id|p-&gt;policy
)paren
)paren
(brace
id|policy
op_assign
id|oldpolicy
op_assign
op_minus
l_int|1
suffix:semicolon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
r_goto
id|recheck
suffix:semicolon
)brace
id|array
op_assign
id|p-&gt;array
suffix:semicolon
r_if
c_cond
(paren
id|array
)paren
id|deactivate_task
c_func
(paren
id|p
comma
id|rq
)paren
suffix:semicolon
id|oldprio
op_assign
id|p-&gt;prio
suffix:semicolon
id|__setscheduler
c_func
(paren
id|p
comma
id|policy
comma
id|param-&gt;sched_priority
)paren
suffix:semicolon
r_if
c_cond
(paren
id|array
)paren
(brace
id|__activate_task
c_func
(paren
id|p
comma
id|rq
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t; * Reschedule if we are currently running on this runqueue and&n;&t;&t; * our priority decreased, or if we are not currently running on&n;&t;&t; * this runqueue and our priority is higher than the current&squot;s&n;&t;&t; */
r_if
c_cond
(paren
id|task_running
c_func
(paren
id|rq
comma
id|p
)paren
)paren
(brace
r_if
c_cond
(paren
id|p-&gt;prio
OG
id|oldprio
)paren
id|resched_task
c_func
(paren
id|rq-&gt;curr
)paren
suffix:semicolon
)brace
r_else
r_if
c_cond
(paren
id|TASK_PREEMPTS_CURR
c_func
(paren
id|p
comma
id|rq
)paren
)paren
id|resched_task
c_func
(paren
id|rq-&gt;curr
)paren
suffix:semicolon
)brace
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
DECL|variable|sched_setscheduler
id|EXPORT_SYMBOL_GPL
c_func
(paren
id|sched_setscheduler
)paren
suffix:semicolon
DECL|function|do_sched_setscheduler
r_static
r_int
id|do_sched_setscheduler
c_func
(paren
id|pid_t
id|pid
comma
r_int
id|policy
comma
r_struct
id|sched_param
id|__user
op_star
id|param
)paren
(brace
r_int
id|retval
suffix:semicolon
r_struct
id|sched_param
id|lparam
suffix:semicolon
r_struct
id|task_struct
op_star
id|p
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|param
op_logical_or
id|pid
OL
l_int|0
)paren
r_return
op_minus
id|EINVAL
suffix:semicolon
r_if
c_cond
(paren
id|copy_from_user
c_func
(paren
op_amp
id|lparam
comma
id|param
comma
r_sizeof
(paren
r_struct
id|sched_param
)paren
)paren
)paren
r_return
op_minus
id|EFAULT
suffix:semicolon
id|read_lock_irq
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|p
op_assign
id|find_process_by_pid
c_func
(paren
id|pid
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|p
)paren
(brace
id|read_unlock_irq
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
r_return
op_minus
id|ESRCH
suffix:semicolon
)brace
id|retval
op_assign
id|sched_setscheduler
c_func
(paren
id|p
comma
id|policy
comma
op_amp
id|lparam
)paren
suffix:semicolon
id|read_unlock_irq
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
r_return
id|retval
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_setscheduler - set/change the scheduler policy and RT priority&n; * @pid: the pid in question.&n; * @policy: new policy.&n; * @param: structure containing the new RT priority.&n; */
DECL|function|sys_sched_setscheduler
id|asmlinkage
r_int
id|sys_sched_setscheduler
c_func
(paren
id|pid_t
id|pid
comma
r_int
id|policy
comma
r_struct
id|sched_param
id|__user
op_star
id|param
)paren
(brace
r_return
id|do_sched_setscheduler
c_func
(paren
id|pid
comma
id|policy
comma
id|param
)paren
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_setparam - set/change the RT priority of a thread&n; * @pid: the pid in question.&n; * @param: structure containing the new RT priority.&n; */
DECL|function|sys_sched_setparam
id|asmlinkage
r_int
id|sys_sched_setparam
c_func
(paren
id|pid_t
id|pid
comma
r_struct
id|sched_param
id|__user
op_star
id|param
)paren
(brace
r_return
id|do_sched_setscheduler
c_func
(paren
id|pid
comma
op_minus
l_int|1
comma
id|param
)paren
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_getscheduler - get the policy (scheduling class) of a thread&n; * @pid: the pid in question.&n; */
DECL|function|sys_sched_getscheduler
id|asmlinkage
r_int
id|sys_sched_getscheduler
c_func
(paren
id|pid_t
id|pid
)paren
(brace
r_int
id|retval
op_assign
op_minus
id|EINVAL
suffix:semicolon
id|task_t
op_star
id|p
suffix:semicolon
r_if
c_cond
(paren
id|pid
OL
l_int|0
)paren
r_goto
id|out_nounlock
suffix:semicolon
id|retval
op_assign
op_minus
id|ESRCH
suffix:semicolon
id|read_lock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|p
op_assign
id|find_process_by_pid
c_func
(paren
id|pid
)paren
suffix:semicolon
r_if
c_cond
(paren
id|p
)paren
(brace
id|retval
op_assign
id|security_task_getscheduler
c_func
(paren
id|p
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|retval
)paren
id|retval
op_assign
id|p-&gt;policy
suffix:semicolon
)brace
id|read_unlock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|out_nounlock
suffix:colon
r_return
id|retval
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_getscheduler - get the RT priority of a thread&n; * @pid: the pid in question.&n; * @param: structure containing the RT priority.&n; */
DECL|function|sys_sched_getparam
id|asmlinkage
r_int
id|sys_sched_getparam
c_func
(paren
id|pid_t
id|pid
comma
r_struct
id|sched_param
id|__user
op_star
id|param
)paren
(brace
r_struct
id|sched_param
id|lp
suffix:semicolon
r_int
id|retval
op_assign
op_minus
id|EINVAL
suffix:semicolon
id|task_t
op_star
id|p
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|param
op_logical_or
id|pid
OL
l_int|0
)paren
r_goto
id|out_nounlock
suffix:semicolon
id|read_lock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|p
op_assign
id|find_process_by_pid
c_func
(paren
id|pid
)paren
suffix:semicolon
id|retval
op_assign
op_minus
id|ESRCH
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|p
)paren
r_goto
id|out_unlock
suffix:semicolon
id|retval
op_assign
id|security_task_getscheduler
c_func
(paren
id|p
)paren
suffix:semicolon
r_if
c_cond
(paren
id|retval
)paren
r_goto
id|out_unlock
suffix:semicolon
id|lp.sched_priority
op_assign
id|p-&gt;rt_priority
suffix:semicolon
id|read_unlock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * This one might sleep, we cannot do it with a spinlock held ...&n;&t; */
id|retval
op_assign
id|copy_to_user
c_func
(paren
id|param
comma
op_amp
id|lp
comma
r_sizeof
(paren
op_star
id|param
)paren
)paren
ques
c_cond
op_minus
id|EFAULT
suffix:colon
l_int|0
suffix:semicolon
id|out_nounlock
suffix:colon
r_return
id|retval
suffix:semicolon
id|out_unlock
suffix:colon
id|read_unlock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
r_return
id|retval
suffix:semicolon
)brace
DECL|function|sched_setaffinity
r_int
id|sched_setaffinity
c_func
(paren
id|pid_t
id|pid
comma
id|cpumask_t
id|new_mask
)paren
(brace
id|task_t
op_star
id|p
suffix:semicolon
r_int
id|retval
suffix:semicolon
id|lock_cpu_hotplug
c_func
(paren
)paren
suffix:semicolon
id|read_lock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|p
op_assign
id|find_process_by_pid
c_func
(paren
id|pid
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|p
)paren
(brace
id|read_unlock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|unlock_cpu_hotplug
c_func
(paren
)paren
suffix:semicolon
r_return
op_minus
id|ESRCH
suffix:semicolon
)brace
multiline_comment|/*&n;&t; * It is not safe to call set_cpus_allowed with the&n;&t; * tasklist_lock held.  We will bump the task_struct&squot;s&n;&t; * usage count and then drop tasklist_lock.&n;&t; */
id|get_task_struct
c_func
(paren
id|p
)paren
suffix:semicolon
id|read_unlock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|retval
op_assign
op_minus
id|EPERM
suffix:semicolon
r_if
c_cond
(paren
(paren
id|current-&gt;euid
op_ne
id|p-&gt;euid
)paren
op_logical_and
(paren
id|current-&gt;euid
op_ne
id|p-&gt;uid
)paren
op_logical_and
op_logical_neg
id|capable
c_func
(paren
id|CAP_SYS_NICE
)paren
)paren
r_goto
id|out_unlock
suffix:semicolon
id|retval
op_assign
id|set_cpus_allowed
c_func
(paren
id|p
comma
id|new_mask
)paren
suffix:semicolon
id|out_unlock
suffix:colon
id|put_task_struct
c_func
(paren
id|p
)paren
suffix:semicolon
id|unlock_cpu_hotplug
c_func
(paren
)paren
suffix:semicolon
r_return
id|retval
suffix:semicolon
)brace
DECL|function|get_user_cpu_mask
r_static
r_int
id|get_user_cpu_mask
c_func
(paren
r_int
r_int
id|__user
op_star
id|user_mask_ptr
comma
r_int
id|len
comma
id|cpumask_t
op_star
id|new_mask
)paren
(brace
r_if
c_cond
(paren
id|len
OL
r_sizeof
(paren
id|cpumask_t
)paren
)paren
(brace
id|memset
c_func
(paren
id|new_mask
comma
l_int|0
comma
r_sizeof
(paren
id|cpumask_t
)paren
)paren
suffix:semicolon
)brace
r_else
r_if
c_cond
(paren
id|len
OG
r_sizeof
(paren
id|cpumask_t
)paren
)paren
(brace
id|len
op_assign
r_sizeof
(paren
id|cpumask_t
)paren
suffix:semicolon
)brace
r_return
id|copy_from_user
c_func
(paren
id|new_mask
comma
id|user_mask_ptr
comma
id|len
)paren
ques
c_cond
op_minus
id|EFAULT
suffix:colon
l_int|0
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_setaffinity - set the cpu affinity of a process&n; * @pid: pid of the process&n; * @len: length in bytes of the bitmask pointed to by user_mask_ptr&n; * @user_mask_ptr: user-space pointer to the new cpu mask&n; */
DECL|function|sys_sched_setaffinity
id|asmlinkage
r_int
id|sys_sched_setaffinity
c_func
(paren
id|pid_t
id|pid
comma
r_int
r_int
id|len
comma
r_int
r_int
id|__user
op_star
id|user_mask_ptr
)paren
(brace
id|cpumask_t
id|new_mask
suffix:semicolon
r_int
id|retval
suffix:semicolon
id|retval
op_assign
id|get_user_cpu_mask
c_func
(paren
id|user_mask_ptr
comma
id|len
comma
op_amp
id|new_mask
)paren
suffix:semicolon
r_if
c_cond
(paren
id|retval
)paren
r_return
id|retval
suffix:semicolon
r_return
id|sched_setaffinity
c_func
(paren
id|pid
comma
id|new_mask
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * Represents all cpu&squot;s present in the system&n; * In systems capable of hotplug, this map could dynamically grow&n; * as new cpu&squot;s are detected in the system via any platform specific&n; * method, such as ACPI for e.g.&n; */
DECL|variable|cpu_present_map
id|cpumask_t
id|cpu_present_map
suffix:semicolon
DECL|variable|cpu_present_map
id|EXPORT_SYMBOL
c_func
(paren
id|cpu_present_map
)paren
suffix:semicolon
macro_line|#ifndef CONFIG_SMP
DECL|variable|cpu_online_map
id|cpumask_t
id|cpu_online_map
op_assign
id|CPU_MASK_ALL
suffix:semicolon
DECL|variable|cpu_possible_map
id|cpumask_t
id|cpu_possible_map
op_assign
id|CPU_MASK_ALL
suffix:semicolon
macro_line|#endif
DECL|function|sched_getaffinity
r_int
id|sched_getaffinity
c_func
(paren
id|pid_t
id|pid
comma
id|cpumask_t
op_star
id|mask
)paren
(brace
r_int
id|retval
suffix:semicolon
id|task_t
op_star
id|p
suffix:semicolon
id|lock_cpu_hotplug
c_func
(paren
)paren
suffix:semicolon
id|read_lock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|retval
op_assign
op_minus
id|ESRCH
suffix:semicolon
id|p
op_assign
id|find_process_by_pid
c_func
(paren
id|pid
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|p
)paren
r_goto
id|out_unlock
suffix:semicolon
id|retval
op_assign
l_int|0
suffix:semicolon
id|cpus_and
c_func
(paren
op_star
id|mask
comma
id|p-&gt;cpus_allowed
comma
id|cpu_possible_map
)paren
suffix:semicolon
id|out_unlock
suffix:colon
id|read_unlock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|unlock_cpu_hotplug
c_func
(paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|retval
)paren
r_return
id|retval
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_getaffinity - get the cpu affinity of a process&n; * @pid: pid of the process&n; * @len: length in bytes of the bitmask pointed to by user_mask_ptr&n; * @user_mask_ptr: user-space pointer to hold the current cpu mask&n; */
DECL|function|sys_sched_getaffinity
id|asmlinkage
r_int
id|sys_sched_getaffinity
c_func
(paren
id|pid_t
id|pid
comma
r_int
r_int
id|len
comma
r_int
r_int
id|__user
op_star
id|user_mask_ptr
)paren
(brace
r_int
id|ret
suffix:semicolon
id|cpumask_t
id|mask
suffix:semicolon
r_if
c_cond
(paren
id|len
OL
r_sizeof
(paren
id|cpumask_t
)paren
)paren
r_return
op_minus
id|EINVAL
suffix:semicolon
id|ret
op_assign
id|sched_getaffinity
c_func
(paren
id|pid
comma
op_amp
id|mask
)paren
suffix:semicolon
r_if
c_cond
(paren
id|ret
OL
l_int|0
)paren
r_return
id|ret
suffix:semicolon
r_if
c_cond
(paren
id|copy_to_user
c_func
(paren
id|user_mask_ptr
comma
op_amp
id|mask
comma
r_sizeof
(paren
id|cpumask_t
)paren
)paren
)paren
r_return
op_minus
id|EFAULT
suffix:semicolon
r_return
r_sizeof
(paren
id|cpumask_t
)paren
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_yield - yield the current processor to other threads.&n; *&n; * this function yields the current CPU by moving the calling thread&n; * to the expired array. If there are no other threads running on this&n; * CPU then this function will return.&n; */
DECL|function|sys_sched_yield
id|asmlinkage
r_int
id|sys_sched_yield
c_func
(paren
r_void
)paren
(brace
id|runqueue_t
op_star
id|rq
op_assign
id|this_rq_lock
c_func
(paren
)paren
suffix:semicolon
id|prio_array_t
op_star
id|array
op_assign
id|current-&gt;array
suffix:semicolon
id|prio_array_t
op_star
id|target
op_assign
id|rq-&gt;expired
suffix:semicolon
id|schedstat_inc
c_func
(paren
id|rq
comma
id|yld_cnt
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * We implement yielding by moving the task into the expired&n;&t; * queue.&n;&t; *&n;&t; * (special rule: RT tasks will just roundrobin in the active&n;&t; *  array.)&n;&t; */
r_if
c_cond
(paren
id|rt_task
c_func
(paren
id|current
)paren
)paren
id|target
op_assign
id|rq-&gt;active
suffix:semicolon
r_if
c_cond
(paren
id|current-&gt;array-&gt;nr_active
op_eq
l_int|1
)paren
(brace
id|schedstat_inc
c_func
(paren
id|rq
comma
id|yld_act_empty
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|rq-&gt;expired-&gt;nr_active
)paren
id|schedstat_inc
c_func
(paren
id|rq
comma
id|yld_both_empty
)paren
suffix:semicolon
)brace
r_else
r_if
c_cond
(paren
op_logical_neg
id|rq-&gt;expired-&gt;nr_active
)paren
id|schedstat_inc
c_func
(paren
id|rq
comma
id|yld_exp_empty
)paren
suffix:semicolon
r_if
c_cond
(paren
id|array
op_ne
id|target
)paren
(brace
id|dequeue_task
c_func
(paren
id|current
comma
id|array
)paren
suffix:semicolon
id|enqueue_task
c_func
(paren
id|current
comma
id|target
)paren
suffix:semicolon
)brace
r_else
multiline_comment|/*&n;&t;&t; * requeue_task is cheaper so perform that if possible.&n;&t;&t; */
id|requeue_task
c_func
(paren
id|current
comma
id|array
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Since we are going to call schedule() anyway, there&squot;s&n;&t; * no need to preempt or enable interrupts:&n;&t; */
id|__release
c_func
(paren
id|rq-&gt;lock
)paren
suffix:semicolon
id|_raw_spin_unlock
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
id|preempt_enable_no_resched
c_func
(paren
)paren
suffix:semicolon
id|schedule
c_func
(paren
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
DECL|function|__cond_resched
r_static
r_inline
r_void
id|__cond_resched
c_func
(paren
r_void
)paren
(brace
r_do
(brace
id|add_preempt_count
c_func
(paren
id|PREEMPT_ACTIVE
)paren
suffix:semicolon
id|schedule
c_func
(paren
)paren
suffix:semicolon
id|sub_preempt_count
c_func
(paren
id|PREEMPT_ACTIVE
)paren
suffix:semicolon
)brace
r_while
c_loop
(paren
id|need_resched
c_func
(paren
)paren
)paren
suffix:semicolon
)brace
DECL|function|cond_resched
r_int
id|__sched
id|cond_resched
c_func
(paren
r_void
)paren
(brace
r_if
c_cond
(paren
id|need_resched
c_func
(paren
)paren
)paren
(brace
id|__cond_resched
c_func
(paren
)paren
suffix:semicolon
r_return
l_int|1
suffix:semicolon
)brace
r_return
l_int|0
suffix:semicolon
)brace
DECL|variable|cond_resched
id|EXPORT_SYMBOL
c_func
(paren
id|cond_resched
)paren
suffix:semicolon
multiline_comment|/*&n; * cond_resched_lock() - if a reschedule is pending, drop the given lock,&n; * call schedule, and on return reacquire the lock.&n; *&n; * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level&n; * operations here to prevent schedule() from being called twice (once via&n; * spin_unlock(), once by hand).&n; */
DECL|function|cond_resched_lock
r_int
id|cond_resched_lock
c_func
(paren
id|spinlock_t
op_star
id|lock
)paren
(brace
macro_line|#if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_PREEMPT)
r_if
c_cond
(paren
id|lock-&gt;break_lock
)paren
(brace
id|lock-&gt;break_lock
op_assign
l_int|0
suffix:semicolon
id|spin_unlock
c_func
(paren
id|lock
)paren
suffix:semicolon
id|cpu_relax
c_func
(paren
)paren
suffix:semicolon
id|spin_lock
c_func
(paren
id|lock
)paren
suffix:semicolon
)brace
macro_line|#endif
r_if
c_cond
(paren
id|need_resched
c_func
(paren
)paren
)paren
(brace
id|_raw_spin_unlock
c_func
(paren
id|lock
)paren
suffix:semicolon
id|preempt_enable_no_resched
c_func
(paren
)paren
suffix:semicolon
id|__cond_resched
c_func
(paren
)paren
suffix:semicolon
id|spin_lock
c_func
(paren
id|lock
)paren
suffix:semicolon
r_return
l_int|1
suffix:semicolon
)brace
r_return
l_int|0
suffix:semicolon
)brace
DECL|variable|cond_resched_lock
id|EXPORT_SYMBOL
c_func
(paren
id|cond_resched_lock
)paren
suffix:semicolon
DECL|function|cond_resched_softirq
r_int
id|__sched
id|cond_resched_softirq
c_func
(paren
r_void
)paren
(brace
id|BUG_ON
c_func
(paren
op_logical_neg
id|in_softirq
c_func
(paren
)paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|need_resched
c_func
(paren
)paren
)paren
(brace
id|__local_bh_enable
c_func
(paren
)paren
suffix:semicolon
id|__cond_resched
c_func
(paren
)paren
suffix:semicolon
id|local_bh_disable
c_func
(paren
)paren
suffix:semicolon
r_return
l_int|1
suffix:semicolon
)brace
r_return
l_int|0
suffix:semicolon
)brace
DECL|variable|cond_resched_softirq
id|EXPORT_SYMBOL
c_func
(paren
id|cond_resched_softirq
)paren
suffix:semicolon
multiline_comment|/**&n; * yield - yield the current processor to other threads.&n; *&n; * this is a shortcut for kernel-space yielding - it marks the&n; * thread runnable and calls sys_sched_yield().&n; */
DECL|function|yield
r_void
id|__sched
id|yield
c_func
(paren
r_void
)paren
(brace
id|set_current_state
c_func
(paren
id|TASK_RUNNING
)paren
suffix:semicolon
id|sys_sched_yield
c_func
(paren
)paren
suffix:semicolon
)brace
DECL|variable|yield
id|EXPORT_SYMBOL
c_func
(paren
id|yield
)paren
suffix:semicolon
multiline_comment|/*&n; * This task is about to go to sleep on IO.  Increment rq-&gt;nr_iowait so&n; * that process accounting knows that this is a task in IO wait state.&n; *&n; * But don&squot;t do that if it is a deliberate, throttling IO wait (this task&n; * has set its backing_dev_info: the queue against which it should throttle)&n; */
DECL|function|io_schedule
r_void
id|__sched
id|io_schedule
c_func
(paren
r_void
)paren
(brace
r_struct
id|runqueue
op_star
id|rq
op_assign
op_amp
id|per_cpu
c_func
(paren
id|runqueues
comma
id|_smp_processor_id
c_func
(paren
)paren
)paren
suffix:semicolon
id|atomic_inc
c_func
(paren
op_amp
id|rq-&gt;nr_iowait
)paren
suffix:semicolon
id|schedule
c_func
(paren
)paren
suffix:semicolon
id|atomic_dec
c_func
(paren
op_amp
id|rq-&gt;nr_iowait
)paren
suffix:semicolon
)brace
DECL|variable|io_schedule
id|EXPORT_SYMBOL
c_func
(paren
id|io_schedule
)paren
suffix:semicolon
DECL|function|io_schedule_timeout
r_int
id|__sched
id|io_schedule_timeout
c_func
(paren
r_int
id|timeout
)paren
(brace
r_struct
id|runqueue
op_star
id|rq
op_assign
op_amp
id|per_cpu
c_func
(paren
id|runqueues
comma
id|_smp_processor_id
c_func
(paren
)paren
)paren
suffix:semicolon
r_int
id|ret
suffix:semicolon
id|atomic_inc
c_func
(paren
op_amp
id|rq-&gt;nr_iowait
)paren
suffix:semicolon
id|ret
op_assign
id|schedule_timeout
c_func
(paren
id|timeout
)paren
suffix:semicolon
id|atomic_dec
c_func
(paren
op_amp
id|rq-&gt;nr_iowait
)paren
suffix:semicolon
r_return
id|ret
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_get_priority_max - return maximum RT priority.&n; * @policy: scheduling class.&n; *&n; * this syscall returns the maximum rt_priority that can be used&n; * by a given scheduling class.&n; */
DECL|function|sys_sched_get_priority_max
id|asmlinkage
r_int
id|sys_sched_get_priority_max
c_func
(paren
r_int
id|policy
)paren
(brace
r_int
id|ret
op_assign
op_minus
id|EINVAL
suffix:semicolon
r_switch
c_cond
(paren
id|policy
)paren
(brace
r_case
id|SCHED_FIFO
suffix:colon
r_case
id|SCHED_RR
suffix:colon
id|ret
op_assign
id|MAX_USER_RT_PRIO
op_minus
l_int|1
suffix:semicolon
r_break
suffix:semicolon
r_case
id|SCHED_NORMAL
suffix:colon
id|ret
op_assign
l_int|0
suffix:semicolon
r_break
suffix:semicolon
)brace
r_return
id|ret
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_get_priority_min - return minimum RT priority.&n; * @policy: scheduling class.&n; *&n; * this syscall returns the minimum rt_priority that can be used&n; * by a given scheduling class.&n; */
DECL|function|sys_sched_get_priority_min
id|asmlinkage
r_int
id|sys_sched_get_priority_min
c_func
(paren
r_int
id|policy
)paren
(brace
r_int
id|ret
op_assign
op_minus
id|EINVAL
suffix:semicolon
r_switch
c_cond
(paren
id|policy
)paren
(brace
r_case
id|SCHED_FIFO
suffix:colon
r_case
id|SCHED_RR
suffix:colon
id|ret
op_assign
l_int|1
suffix:semicolon
r_break
suffix:semicolon
r_case
id|SCHED_NORMAL
suffix:colon
id|ret
op_assign
l_int|0
suffix:semicolon
)brace
r_return
id|ret
suffix:semicolon
)brace
multiline_comment|/**&n; * sys_sched_rr_get_interval - return the default timeslice of a process.&n; * @pid: pid of the process.&n; * @interval: userspace pointer to the timeslice value.&n; *&n; * this syscall writes the default timeslice value of a given process&n; * into the user-space timespec buffer. A value of &squot;0&squot; means infinity.&n; */
id|asmlinkage
DECL|function|sys_sched_rr_get_interval
r_int
id|sys_sched_rr_get_interval
c_func
(paren
id|pid_t
id|pid
comma
r_struct
id|timespec
id|__user
op_star
id|interval
)paren
(brace
r_int
id|retval
op_assign
op_minus
id|EINVAL
suffix:semicolon
r_struct
id|timespec
id|t
suffix:semicolon
id|task_t
op_star
id|p
suffix:semicolon
r_if
c_cond
(paren
id|pid
OL
l_int|0
)paren
r_goto
id|out_nounlock
suffix:semicolon
id|retval
op_assign
op_minus
id|ESRCH
suffix:semicolon
id|read_lock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|p
op_assign
id|find_process_by_pid
c_func
(paren
id|pid
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|p
)paren
r_goto
id|out_unlock
suffix:semicolon
id|retval
op_assign
id|security_task_getscheduler
c_func
(paren
id|p
)paren
suffix:semicolon
r_if
c_cond
(paren
id|retval
)paren
r_goto
id|out_unlock
suffix:semicolon
id|jiffies_to_timespec
c_func
(paren
id|p-&gt;policy
op_amp
id|SCHED_FIFO
ques
c_cond
l_int|0
suffix:colon
id|task_timeslice
c_func
(paren
id|p
)paren
comma
op_amp
id|t
)paren
suffix:semicolon
id|read_unlock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|retval
op_assign
id|copy_to_user
c_func
(paren
id|interval
comma
op_amp
id|t
comma
r_sizeof
(paren
id|t
)paren
)paren
ques
c_cond
op_minus
id|EFAULT
suffix:colon
l_int|0
suffix:semicolon
id|out_nounlock
suffix:colon
r_return
id|retval
suffix:semicolon
id|out_unlock
suffix:colon
id|read_unlock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
r_return
id|retval
suffix:semicolon
)brace
DECL|function|eldest_child
r_static
r_inline
r_struct
id|task_struct
op_star
id|eldest_child
c_func
(paren
r_struct
id|task_struct
op_star
id|p
)paren
(brace
r_if
c_cond
(paren
id|list_empty
c_func
(paren
op_amp
id|p-&gt;children
)paren
)paren
r_return
l_int|NULL
suffix:semicolon
r_return
id|list_entry
c_func
(paren
id|p-&gt;children.next
comma
r_struct
id|task_struct
comma
id|sibling
)paren
suffix:semicolon
)brace
DECL|function|older_sibling
r_static
r_inline
r_struct
id|task_struct
op_star
id|older_sibling
c_func
(paren
r_struct
id|task_struct
op_star
id|p
)paren
(brace
r_if
c_cond
(paren
id|p-&gt;sibling.prev
op_eq
op_amp
id|p-&gt;parent-&gt;children
)paren
r_return
l_int|NULL
suffix:semicolon
r_return
id|list_entry
c_func
(paren
id|p-&gt;sibling.prev
comma
r_struct
id|task_struct
comma
id|sibling
)paren
suffix:semicolon
)brace
DECL|function|younger_sibling
r_static
r_inline
r_struct
id|task_struct
op_star
id|younger_sibling
c_func
(paren
r_struct
id|task_struct
op_star
id|p
)paren
(brace
r_if
c_cond
(paren
id|p-&gt;sibling.next
op_eq
op_amp
id|p-&gt;parent-&gt;children
)paren
r_return
l_int|NULL
suffix:semicolon
r_return
id|list_entry
c_func
(paren
id|p-&gt;sibling.next
comma
r_struct
id|task_struct
comma
id|sibling
)paren
suffix:semicolon
)brace
DECL|function|show_task
r_static
r_void
id|show_task
c_func
(paren
id|task_t
op_star
id|p
)paren
(brace
id|task_t
op_star
id|relative
suffix:semicolon
r_int
id|state
suffix:semicolon
r_int
r_int
id|free
op_assign
l_int|0
suffix:semicolon
r_static
r_const
r_char
op_star
id|stat_nam
(braket
)braket
op_assign
(brace
l_string|&quot;R&quot;
comma
l_string|&quot;S&quot;
comma
l_string|&quot;D&quot;
comma
l_string|&quot;T&quot;
comma
l_string|&quot;t&quot;
comma
l_string|&quot;Z&quot;
comma
l_string|&quot;X&quot;
)brace
suffix:semicolon
id|printk
c_func
(paren
l_string|&quot;%-13.13s &quot;
comma
id|p-&gt;comm
)paren
suffix:semicolon
id|state
op_assign
id|p-&gt;state
ques
c_cond
id|__ffs
c_func
(paren
id|p-&gt;state
)paren
op_plus
l_int|1
suffix:colon
l_int|0
suffix:semicolon
r_if
c_cond
(paren
id|state
OL
id|ARRAY_SIZE
c_func
(paren
id|stat_nam
)paren
)paren
id|printk
c_func
(paren
id|stat_nam
(braket
id|state
)braket
)paren
suffix:semicolon
r_else
id|printk
c_func
(paren
l_string|&quot;?&quot;
)paren
suffix:semicolon
macro_line|#if (BITS_PER_LONG == 32)
r_if
c_cond
(paren
id|state
op_eq
id|TASK_RUNNING
)paren
id|printk
c_func
(paren
l_string|&quot; running &quot;
)paren
suffix:semicolon
r_else
id|printk
c_func
(paren
l_string|&quot; %08lX &quot;
comma
id|thread_saved_pc
c_func
(paren
id|p
)paren
)paren
suffix:semicolon
macro_line|#else
r_if
c_cond
(paren
id|state
op_eq
id|TASK_RUNNING
)paren
id|printk
c_func
(paren
l_string|&quot;  running task   &quot;
)paren
suffix:semicolon
r_else
id|printk
c_func
(paren
l_string|&quot; %016lx &quot;
comma
id|thread_saved_pc
c_func
(paren
id|p
)paren
)paren
suffix:semicolon
macro_line|#endif
macro_line|#ifdef CONFIG_DEBUG_STACK_USAGE
(brace
r_int
r_int
op_star
id|n
op_assign
(paren
r_int
r_int
op_star
)paren
(paren
id|p-&gt;thread_info
op_plus
l_int|1
)paren
suffix:semicolon
r_while
c_loop
(paren
op_logical_neg
op_star
id|n
)paren
id|n
op_increment
suffix:semicolon
id|free
op_assign
(paren
r_int
r_int
)paren
id|n
op_minus
(paren
r_int
r_int
)paren
(paren
id|p-&gt;thread_info
op_plus
l_int|1
)paren
suffix:semicolon
)brace
macro_line|#endif
id|printk
c_func
(paren
l_string|&quot;%5lu %5d %6d &quot;
comma
id|free
comma
id|p-&gt;pid
comma
id|p-&gt;parent-&gt;pid
)paren
suffix:semicolon
r_if
c_cond
(paren
(paren
id|relative
op_assign
id|eldest_child
c_func
(paren
id|p
)paren
)paren
)paren
id|printk
c_func
(paren
l_string|&quot;%5d &quot;
comma
id|relative-&gt;pid
)paren
suffix:semicolon
r_else
id|printk
c_func
(paren
l_string|&quot;      &quot;
)paren
suffix:semicolon
r_if
c_cond
(paren
(paren
id|relative
op_assign
id|younger_sibling
c_func
(paren
id|p
)paren
)paren
)paren
id|printk
c_func
(paren
l_string|&quot;%7d&quot;
comma
id|relative-&gt;pid
)paren
suffix:semicolon
r_else
id|printk
c_func
(paren
l_string|&quot;       &quot;
)paren
suffix:semicolon
r_if
c_cond
(paren
(paren
id|relative
op_assign
id|older_sibling
c_func
(paren
id|p
)paren
)paren
)paren
id|printk
c_func
(paren
l_string|&quot; %5d&quot;
comma
id|relative-&gt;pid
)paren
suffix:semicolon
r_else
id|printk
c_func
(paren
l_string|&quot;      &quot;
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|p-&gt;mm
)paren
id|printk
c_func
(paren
l_string|&quot; (L-TLB)&bslash;n&quot;
)paren
suffix:semicolon
r_else
id|printk
c_func
(paren
l_string|&quot; (NOTLB)&bslash;n&quot;
)paren
suffix:semicolon
r_if
c_cond
(paren
id|state
op_ne
id|TASK_RUNNING
)paren
id|show_stack
c_func
(paren
id|p
comma
l_int|NULL
)paren
suffix:semicolon
)brace
DECL|function|show_state
r_void
id|show_state
c_func
(paren
r_void
)paren
(brace
id|task_t
op_star
id|g
comma
op_star
id|p
suffix:semicolon
macro_line|#if (BITS_PER_LONG == 32)
id|printk
c_func
(paren
l_string|&quot;&bslash;n&quot;
l_string|&quot;                                               sibling&bslash;n&quot;
)paren
suffix:semicolon
id|printk
c_func
(paren
l_string|&quot;  task             PC      pid father child younger older&bslash;n&quot;
)paren
suffix:semicolon
macro_line|#else
id|printk
c_func
(paren
l_string|&quot;&bslash;n&quot;
l_string|&quot;                                                       sibling&bslash;n&quot;
)paren
suffix:semicolon
id|printk
c_func
(paren
l_string|&quot;  task                 PC          pid father child younger older&bslash;n&quot;
)paren
suffix:semicolon
macro_line|#endif
id|read_lock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|do_each_thread
c_func
(paren
id|g
comma
id|p
)paren
(brace
multiline_comment|/*&n;&t;&t; * reset the NMI-timeout, listing all files on a slow&n;&t;&t; * console might take alot of time:&n;&t;&t; */
id|touch_nmi_watchdog
c_func
(paren
)paren
suffix:semicolon
id|show_task
c_func
(paren
id|p
)paren
suffix:semicolon
)brace
id|while_each_thread
c_func
(paren
id|g
comma
id|p
)paren
suffix:semicolon
id|read_unlock
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
)brace
DECL|function|init_idle
r_void
id|__devinit
id|init_idle
c_func
(paren
id|task_t
op_star
id|idle
comma
r_int
id|cpu
)paren
(brace
id|runqueue_t
op_star
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|cpu
)paren
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
id|idle-&gt;sleep_avg
op_assign
l_int|0
suffix:semicolon
id|idle-&gt;array
op_assign
l_int|NULL
suffix:semicolon
id|idle-&gt;prio
op_assign
id|MAX_PRIO
suffix:semicolon
id|idle-&gt;state
op_assign
id|TASK_RUNNING
suffix:semicolon
id|set_task_cpu
c_func
(paren
id|idle
comma
id|cpu
)paren
suffix:semicolon
id|spin_lock_irqsave
c_func
(paren
op_amp
id|rq-&gt;lock
comma
id|flags
)paren
suffix:semicolon
id|rq-&gt;curr
op_assign
id|rq-&gt;idle
op_assign
id|idle
suffix:semicolon
id|set_tsk_need_resched
c_func
(paren
id|idle
)paren
suffix:semicolon
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|rq-&gt;lock
comma
id|flags
)paren
suffix:semicolon
multiline_comment|/* Set the preempt count _outside_ the spinlocks! */
macro_line|#if defined(CONFIG_PREEMPT) &amp;&amp; !defined(CONFIG_PREEMPT_BKL)
id|idle-&gt;thread_info-&gt;preempt_count
op_assign
(paren
id|idle-&gt;lock_depth
op_ge
l_int|0
)paren
suffix:semicolon
macro_line|#else
id|idle-&gt;thread_info-&gt;preempt_count
op_assign
l_int|0
suffix:semicolon
macro_line|#endif
)brace
multiline_comment|/*&n; * In a system that switches off the HZ timer nohz_cpu_mask&n; * indicates which cpus entered this state. This is used&n; * in the rcu update to wait only for active cpus. For system&n; * which do not switch off the HZ timer nohz_cpu_mask should&n; * always be CPU_MASK_NONE.&n; */
DECL|variable|nohz_cpu_mask
id|cpumask_t
id|nohz_cpu_mask
op_assign
id|CPU_MASK_NONE
suffix:semicolon
macro_line|#ifdef CONFIG_SMP
multiline_comment|/*&n; * This is how migration works:&n; *&n; * 1) we queue a migration_req_t structure in the source CPU&squot;s&n; *    runqueue and wake up that CPU&squot;s migration thread.&n; * 2) we down() the locked semaphore =&gt; thread blocks.&n; * 3) migration thread wakes up (implicitly it forces the migrated&n; *    thread off the CPU)&n; * 4) it gets the migration request and checks whether the migrated&n; *    task is still in the wrong runqueue.&n; * 5) if it&squot;s in the wrong runqueue then the migration thread removes&n; *    it and puts it into the right queue.&n; * 6) migration thread up()s the semaphore.&n; * 7) we wake up and the migration is done.&n; */
multiline_comment|/*&n; * Change a given task&squot;s CPU affinity. Migrate the thread to a&n; * proper CPU and schedule it away if the CPU it&squot;s executing on&n; * is removed from the allowed bitmask.&n; *&n; * NOTE: the caller must have a valid reference to the task, the&n; * task must not exit() &amp; deallocate itself prematurely.  The&n; * call is not atomic; no spinlocks may be held.&n; */
DECL|function|set_cpus_allowed
r_int
id|set_cpus_allowed
c_func
(paren
id|task_t
op_star
id|p
comma
id|cpumask_t
id|new_mask
)paren
(brace
r_int
r_int
id|flags
suffix:semicolon
r_int
id|ret
op_assign
l_int|0
suffix:semicolon
id|migration_req_t
id|req
suffix:semicolon
id|runqueue_t
op_star
id|rq
suffix:semicolon
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|cpus_intersects
c_func
(paren
id|new_mask
comma
id|cpu_online_map
)paren
)paren
(brace
id|ret
op_assign
op_minus
id|EINVAL
suffix:semicolon
r_goto
id|out
suffix:semicolon
)brace
id|p-&gt;cpus_allowed
op_assign
id|new_mask
suffix:semicolon
multiline_comment|/* Can the task run on the task&squot;s current CPU? If so, we&squot;re done */
r_if
c_cond
(paren
id|cpu_isset
c_func
(paren
id|task_cpu
c_func
(paren
id|p
)paren
comma
id|new_mask
)paren
)paren
r_goto
id|out
suffix:semicolon
r_if
c_cond
(paren
id|migrate_task
c_func
(paren
id|p
comma
id|any_online_cpu
c_func
(paren
id|new_mask
)paren
comma
op_amp
id|req
)paren
)paren
(brace
multiline_comment|/* Need help from migration thread: drop lock and wait. */
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
id|wake_up_process
c_func
(paren
id|rq-&gt;migration_thread
)paren
suffix:semicolon
id|wait_for_completion
c_func
(paren
op_amp
id|req.done
)paren
suffix:semicolon
id|tlb_migrate_finish
c_func
(paren
id|p-&gt;mm
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
id|out
suffix:colon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
r_return
id|ret
suffix:semicolon
)brace
DECL|variable|set_cpus_allowed
id|EXPORT_SYMBOL_GPL
c_func
(paren
id|set_cpus_allowed
)paren
suffix:semicolon
multiline_comment|/*&n; * Move (not current) task off this cpu, onto dest cpu.  We&squot;re doing&n; * this because either it can&squot;t run here any more (set_cpus_allowed()&n; * away from this CPU, or CPU going down), or because we&squot;re&n; * attempting to rebalance this task on exec (sched_exec).&n; *&n; * So we race with normal scheduler movements, but that&squot;s OK, as long&n; * as the task is no longer on this CPU.&n; */
DECL|function|__migrate_task
r_static
r_void
id|__migrate_task
c_func
(paren
r_struct
id|task_struct
op_star
id|p
comma
r_int
id|src_cpu
comma
r_int
id|dest_cpu
)paren
(brace
id|runqueue_t
op_star
id|rq_dest
comma
op_star
id|rq_src
suffix:semicolon
r_if
c_cond
(paren
id|unlikely
c_func
(paren
id|cpu_is_offline
c_func
(paren
id|dest_cpu
)paren
)paren
)paren
r_return
suffix:semicolon
id|rq_src
op_assign
id|cpu_rq
c_func
(paren
id|src_cpu
)paren
suffix:semicolon
id|rq_dest
op_assign
id|cpu_rq
c_func
(paren
id|dest_cpu
)paren
suffix:semicolon
id|double_rq_lock
c_func
(paren
id|rq_src
comma
id|rq_dest
)paren
suffix:semicolon
multiline_comment|/* Already moved. */
r_if
c_cond
(paren
id|task_cpu
c_func
(paren
id|p
)paren
op_ne
id|src_cpu
)paren
r_goto
id|out
suffix:semicolon
multiline_comment|/* Affinity changed (again). */
r_if
c_cond
(paren
op_logical_neg
id|cpu_isset
c_func
(paren
id|dest_cpu
comma
id|p-&gt;cpus_allowed
)paren
)paren
r_goto
id|out
suffix:semicolon
id|set_task_cpu
c_func
(paren
id|p
comma
id|dest_cpu
)paren
suffix:semicolon
r_if
c_cond
(paren
id|p-&gt;array
)paren
(brace
multiline_comment|/*&n;&t;&t; * Sync timestamp with rq_dest&squot;s before activating.&n;&t;&t; * The same thing could be achieved by doing this step&n;&t;&t; * afterwards, and pretending it was a local activate.&n;&t;&t; * This way is cleaner and logically correct.&n;&t;&t; */
id|p-&gt;timestamp
op_assign
id|p-&gt;timestamp
op_minus
id|rq_src-&gt;timestamp_last_tick
op_plus
id|rq_dest-&gt;timestamp_last_tick
suffix:semicolon
id|deactivate_task
c_func
(paren
id|p
comma
id|rq_src
)paren
suffix:semicolon
id|activate_task
c_func
(paren
id|p
comma
id|rq_dest
comma
l_int|0
)paren
suffix:semicolon
r_if
c_cond
(paren
id|TASK_PREEMPTS_CURR
c_func
(paren
id|p
comma
id|rq_dest
)paren
)paren
id|resched_task
c_func
(paren
id|rq_dest-&gt;curr
)paren
suffix:semicolon
)brace
id|out
suffix:colon
id|double_rq_unlock
c_func
(paren
id|rq_src
comma
id|rq_dest
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * migration_thread - this is a highprio system thread that performs&n; * thread migration by bumping thread off CPU then &squot;pushing&squot; onto&n; * another runqueue.&n; */
DECL|function|migration_thread
r_static
r_int
id|migration_thread
c_func
(paren
r_void
op_star
id|data
)paren
(brace
id|runqueue_t
op_star
id|rq
suffix:semicolon
r_int
id|cpu
op_assign
(paren
r_int
)paren
id|data
suffix:semicolon
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|cpu
)paren
suffix:semicolon
id|BUG_ON
c_func
(paren
id|rq-&gt;migration_thread
op_ne
id|current
)paren
suffix:semicolon
id|set_current_state
c_func
(paren
id|TASK_INTERRUPTIBLE
)paren
suffix:semicolon
r_while
c_loop
(paren
op_logical_neg
id|kthread_should_stop
c_func
(paren
)paren
)paren
(brace
r_struct
id|list_head
op_star
id|head
suffix:semicolon
id|migration_req_t
op_star
id|req
suffix:semicolon
r_if
c_cond
(paren
id|current-&gt;flags
op_amp
id|PF_FREEZE
)paren
id|refrigerator
c_func
(paren
id|PF_FREEZE
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
r_if
c_cond
(paren
id|cpu_is_offline
c_func
(paren
id|cpu
)paren
)paren
(brace
id|spin_unlock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
r_goto
id|wait_to_die
suffix:semicolon
)brace
r_if
c_cond
(paren
id|rq-&gt;active_balance
)paren
(brace
id|active_load_balance
c_func
(paren
id|rq
comma
id|cpu
)paren
suffix:semicolon
id|rq-&gt;active_balance
op_assign
l_int|0
suffix:semicolon
)brace
id|head
op_assign
op_amp
id|rq-&gt;migration_queue
suffix:semicolon
r_if
c_cond
(paren
id|list_empty
c_func
(paren
id|head
)paren
)paren
(brace
id|spin_unlock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
id|schedule
c_func
(paren
)paren
suffix:semicolon
id|set_current_state
c_func
(paren
id|TASK_INTERRUPTIBLE
)paren
suffix:semicolon
r_continue
suffix:semicolon
)brace
id|req
op_assign
id|list_entry
c_func
(paren
id|head-&gt;next
comma
id|migration_req_t
comma
id|list
)paren
suffix:semicolon
id|list_del_init
c_func
(paren
id|head-&gt;next
)paren
suffix:semicolon
r_if
c_cond
(paren
id|req-&gt;type
op_eq
id|REQ_MOVE_TASK
)paren
(brace
id|spin_unlock
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
id|__migrate_task
c_func
(paren
id|req-&gt;task
comma
id|cpu
comma
id|req-&gt;dest_cpu
)paren
suffix:semicolon
id|local_irq_enable
c_func
(paren
)paren
suffix:semicolon
)brace
r_else
r_if
c_cond
(paren
id|req-&gt;type
op_eq
id|REQ_SET_DOMAIN
)paren
(brace
id|rq-&gt;sd
op_assign
id|req-&gt;sd
suffix:semicolon
id|spin_unlock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
)brace
r_else
(brace
id|spin_unlock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
id|WARN_ON
c_func
(paren
l_int|1
)paren
suffix:semicolon
)brace
id|complete
c_func
(paren
op_amp
id|req-&gt;done
)paren
suffix:semicolon
)brace
id|__set_current_state
c_func
(paren
id|TASK_RUNNING
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
id|wait_to_die
suffix:colon
multiline_comment|/* Wait for kthread_stop */
id|set_current_state
c_func
(paren
id|TASK_INTERRUPTIBLE
)paren
suffix:semicolon
r_while
c_loop
(paren
op_logical_neg
id|kthread_should_stop
c_func
(paren
)paren
)paren
(brace
id|schedule
c_func
(paren
)paren
suffix:semicolon
id|set_current_state
c_func
(paren
id|TASK_INTERRUPTIBLE
)paren
suffix:semicolon
)brace
id|__set_current_state
c_func
(paren
id|TASK_RUNNING
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_HOTPLUG_CPU
multiline_comment|/* Figure out where task on dead CPU should go, use force if neccessary. */
DECL|function|move_task_off_dead_cpu
r_static
r_void
id|move_task_off_dead_cpu
c_func
(paren
r_int
id|dead_cpu
comma
r_struct
id|task_struct
op_star
id|tsk
)paren
(brace
r_int
id|dest_cpu
suffix:semicolon
id|cpumask_t
id|mask
suffix:semicolon
multiline_comment|/* On same node? */
id|mask
op_assign
id|node_to_cpumask
c_func
(paren
id|cpu_to_node
c_func
(paren
id|dead_cpu
)paren
)paren
suffix:semicolon
id|cpus_and
c_func
(paren
id|mask
comma
id|mask
comma
id|tsk-&gt;cpus_allowed
)paren
suffix:semicolon
id|dest_cpu
op_assign
id|any_online_cpu
c_func
(paren
id|mask
)paren
suffix:semicolon
multiline_comment|/* On any allowed CPU? */
r_if
c_cond
(paren
id|dest_cpu
op_eq
id|NR_CPUS
)paren
id|dest_cpu
op_assign
id|any_online_cpu
c_func
(paren
id|tsk-&gt;cpus_allowed
)paren
suffix:semicolon
multiline_comment|/* No more Mr. Nice Guy. */
r_if
c_cond
(paren
id|dest_cpu
op_eq
id|NR_CPUS
)paren
(brace
id|cpus_setall
c_func
(paren
id|tsk-&gt;cpus_allowed
)paren
suffix:semicolon
id|dest_cpu
op_assign
id|any_online_cpu
c_func
(paren
id|tsk-&gt;cpus_allowed
)paren
suffix:semicolon
multiline_comment|/*&n;&t;&t; * Don&squot;t tell them about moving exiting tasks or&n;&t;&t; * kernel threads (both mm NULL), since they never&n;&t;&t; * leave kernel.&n;&t;&t; */
r_if
c_cond
(paren
id|tsk-&gt;mm
op_logical_and
id|printk_ratelimit
c_func
(paren
)paren
)paren
id|printk
c_func
(paren
id|KERN_INFO
l_string|&quot;process %d (%s) no &quot;
l_string|&quot;longer affine to cpu%d&bslash;n&quot;
comma
id|tsk-&gt;pid
comma
id|tsk-&gt;comm
comma
id|dead_cpu
)paren
suffix:semicolon
)brace
id|__migrate_task
c_func
(paren
id|tsk
comma
id|dead_cpu
comma
id|dest_cpu
)paren
suffix:semicolon
)brace
multiline_comment|/*&n; * While a dead CPU has no uninterruptible tasks queued at this point,&n; * it might still have a nonzero -&gt;nr_uninterruptible counter, because&n; * for performance reasons the counter is not stricly tracking tasks to&n; * their home CPUs. So we just add the counter to another CPU&squot;s counter,&n; * to keep the global sum constant after CPU-down:&n; */
DECL|function|migrate_nr_uninterruptible
r_static
r_void
id|migrate_nr_uninterruptible
c_func
(paren
id|runqueue_t
op_star
id|rq_src
)paren
(brace
id|runqueue_t
op_star
id|rq_dest
op_assign
id|cpu_rq
c_func
(paren
id|any_online_cpu
c_func
(paren
id|CPU_MASK_ALL
)paren
)paren
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
id|local_irq_save
c_func
(paren
id|flags
)paren
suffix:semicolon
id|double_rq_lock
c_func
(paren
id|rq_src
comma
id|rq_dest
)paren
suffix:semicolon
id|rq_dest-&gt;nr_uninterruptible
op_add_assign
id|rq_src-&gt;nr_uninterruptible
suffix:semicolon
id|rq_src-&gt;nr_uninterruptible
op_assign
l_int|0
suffix:semicolon
id|double_rq_unlock
c_func
(paren
id|rq_src
comma
id|rq_dest
)paren
suffix:semicolon
id|local_irq_restore
c_func
(paren
id|flags
)paren
suffix:semicolon
)brace
multiline_comment|/* Run through task list and migrate tasks from the dead cpu. */
DECL|function|migrate_live_tasks
r_static
r_void
id|migrate_live_tasks
c_func
(paren
r_int
id|src_cpu
)paren
(brace
r_struct
id|task_struct
op_star
id|tsk
comma
op_star
id|t
suffix:semicolon
id|write_lock_irq
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|do_each_thread
c_func
(paren
id|t
comma
id|tsk
)paren
(brace
r_if
c_cond
(paren
id|tsk
op_eq
id|current
)paren
r_continue
suffix:semicolon
r_if
c_cond
(paren
id|task_cpu
c_func
(paren
id|tsk
)paren
op_eq
id|src_cpu
)paren
id|move_task_off_dead_cpu
c_func
(paren
id|src_cpu
comma
id|tsk
)paren
suffix:semicolon
)brace
id|while_each_thread
c_func
(paren
id|t
comma
id|tsk
)paren
suffix:semicolon
id|write_unlock_irq
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
)brace
multiline_comment|/* Schedules idle task to be the next runnable task on current CPU.&n; * It does so by boosting its priority to highest possible and adding it to&n; * the _front_ of runqueue. Used by CPU offline code.&n; */
DECL|function|sched_idle_next
r_void
id|sched_idle_next
c_func
(paren
r_void
)paren
(brace
r_int
id|cpu
op_assign
id|smp_processor_id
c_func
(paren
)paren
suffix:semicolon
id|runqueue_t
op_star
id|rq
op_assign
id|this_rq
c_func
(paren
)paren
suffix:semicolon
r_struct
id|task_struct
op_star
id|p
op_assign
id|rq-&gt;idle
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
multiline_comment|/* cpu has to be offline */
id|BUG_ON
c_func
(paren
id|cpu_online
c_func
(paren
id|cpu
)paren
)paren
suffix:semicolon
multiline_comment|/* Strictly not necessary since rest of the CPUs are stopped by now&n;&t; * and interrupts disabled on current cpu.&n;&t; */
id|spin_lock_irqsave
c_func
(paren
op_amp
id|rq-&gt;lock
comma
id|flags
)paren
suffix:semicolon
id|__setscheduler
c_func
(paren
id|p
comma
id|SCHED_FIFO
comma
id|MAX_RT_PRIO
op_minus
l_int|1
)paren
suffix:semicolon
multiline_comment|/* Add idle task to _front_ of it&squot;s priority queue */
id|__activate_idle_task
c_func
(paren
id|p
comma
id|rq
)paren
suffix:semicolon
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|rq-&gt;lock
comma
id|flags
)paren
suffix:semicolon
)brace
multiline_comment|/* Ensures that the idle task is using init_mm right before its cpu goes&n; * offline.&n; */
DECL|function|idle_task_exit
r_void
id|idle_task_exit
c_func
(paren
r_void
)paren
(brace
r_struct
id|mm_struct
op_star
id|mm
op_assign
id|current-&gt;active_mm
suffix:semicolon
id|BUG_ON
c_func
(paren
id|cpu_online
c_func
(paren
id|smp_processor_id
c_func
(paren
)paren
)paren
)paren
suffix:semicolon
r_if
c_cond
(paren
id|mm
op_ne
op_amp
id|init_mm
)paren
id|switch_mm
c_func
(paren
id|mm
comma
op_amp
id|init_mm
comma
id|current
)paren
suffix:semicolon
id|mmdrop
c_func
(paren
id|mm
)paren
suffix:semicolon
)brace
DECL|function|migrate_dead
r_static
r_void
id|migrate_dead
c_func
(paren
r_int
r_int
id|dead_cpu
comma
id|task_t
op_star
id|tsk
)paren
(brace
r_struct
id|runqueue
op_star
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|dead_cpu
)paren
suffix:semicolon
multiline_comment|/* Must be exiting, otherwise would be on tasklist. */
id|BUG_ON
c_func
(paren
id|tsk-&gt;exit_state
op_ne
id|EXIT_ZOMBIE
op_logical_and
id|tsk-&gt;exit_state
op_ne
id|EXIT_DEAD
)paren
suffix:semicolon
multiline_comment|/* Cannot have done final schedule yet: would have vanished. */
id|BUG_ON
c_func
(paren
id|tsk-&gt;flags
op_amp
id|PF_DEAD
)paren
suffix:semicolon
id|get_task_struct
c_func
(paren
id|tsk
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Drop lock around migration; if someone else moves it,&n;&t; * that&squot;s OK.  No task can be added to this CPU, so iteration is&n;&t; * fine.&n;&t; */
id|spin_unlock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
id|move_task_off_dead_cpu
c_func
(paren
id|dead_cpu
comma
id|tsk
)paren
suffix:semicolon
id|spin_lock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
id|put_task_struct
c_func
(paren
id|tsk
)paren
suffix:semicolon
)brace
multiline_comment|/* release_task() removes task from tasklist, so we won&squot;t find dead tasks. */
DECL|function|migrate_dead_tasks
r_static
r_void
id|migrate_dead_tasks
c_func
(paren
r_int
r_int
id|dead_cpu
)paren
(brace
r_int
id|arr
comma
id|i
suffix:semicolon
r_struct
id|runqueue
op_star
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|dead_cpu
)paren
suffix:semicolon
r_for
c_loop
(paren
id|arr
op_assign
l_int|0
suffix:semicolon
id|arr
OL
l_int|2
suffix:semicolon
id|arr
op_increment
)paren
(brace
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
id|MAX_PRIO
suffix:semicolon
id|i
op_increment
)paren
(brace
r_struct
id|list_head
op_star
id|list
op_assign
op_amp
id|rq-&gt;arrays
(braket
id|arr
)braket
dot
id|queue
(braket
id|i
)braket
suffix:semicolon
r_while
c_loop
(paren
op_logical_neg
id|list_empty
c_func
(paren
id|list
)paren
)paren
id|migrate_dead
c_func
(paren
id|dead_cpu
comma
id|list_entry
c_func
(paren
id|list-&gt;next
comma
id|task_t
comma
id|run_list
)paren
)paren
suffix:semicolon
)brace
)brace
)brace
macro_line|#endif /* CONFIG_HOTPLUG_CPU */
multiline_comment|/*&n; * migration_call - callback that gets triggered when a CPU is added.&n; * Here we can start up the necessary migration thread for the new CPU.&n; */
DECL|function|migration_call
r_static
r_int
id|migration_call
c_func
(paren
r_struct
id|notifier_block
op_star
id|nfb
comma
r_int
r_int
id|action
comma
r_void
op_star
id|hcpu
)paren
(brace
r_int
id|cpu
op_assign
(paren
r_int
)paren
id|hcpu
suffix:semicolon
r_struct
id|task_struct
op_star
id|p
suffix:semicolon
r_struct
id|runqueue
op_star
id|rq
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
r_switch
c_cond
(paren
id|action
)paren
(brace
r_case
id|CPU_UP_PREPARE
suffix:colon
id|p
op_assign
id|kthread_create
c_func
(paren
id|migration_thread
comma
id|hcpu
comma
l_string|&quot;migration/%d&quot;
comma
id|cpu
)paren
suffix:semicolon
r_if
c_cond
(paren
id|IS_ERR
c_func
(paren
id|p
)paren
)paren
r_return
id|NOTIFY_BAD
suffix:semicolon
id|p-&gt;flags
op_or_assign
id|PF_NOFREEZE
suffix:semicolon
id|kthread_bind
c_func
(paren
id|p
comma
id|cpu
)paren
suffix:semicolon
multiline_comment|/* Must be high prio: stop_machine expects to yield to it. */
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
id|__setscheduler
c_func
(paren
id|p
comma
id|SCHED_FIFO
comma
id|MAX_RT_PRIO
op_minus
l_int|1
)paren
suffix:semicolon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
id|cpu_rq
c_func
(paren
id|cpu
)paren
op_member_access_from_pointer
id|migration_thread
op_assign
id|p
suffix:semicolon
r_break
suffix:semicolon
r_case
id|CPU_ONLINE
suffix:colon
multiline_comment|/* Strictly unneccessary, as first user will wake it. */
id|wake_up_process
c_func
(paren
id|cpu_rq
c_func
(paren
id|cpu
)paren
op_member_access_from_pointer
id|migration_thread
)paren
suffix:semicolon
r_break
suffix:semicolon
macro_line|#ifdef CONFIG_HOTPLUG_CPU
r_case
id|CPU_UP_CANCELED
suffix:colon
multiline_comment|/* Unbind it from offline cpu so it can run.  Fall thru. */
id|kthread_bind
c_func
(paren
id|cpu_rq
c_func
(paren
id|cpu
)paren
op_member_access_from_pointer
id|migration_thread
comma
id|smp_processor_id
c_func
(paren
)paren
)paren
suffix:semicolon
id|kthread_stop
c_func
(paren
id|cpu_rq
c_func
(paren
id|cpu
)paren
op_member_access_from_pointer
id|migration_thread
)paren
suffix:semicolon
id|cpu_rq
c_func
(paren
id|cpu
)paren
op_member_access_from_pointer
id|migration_thread
op_assign
l_int|NULL
suffix:semicolon
r_break
suffix:semicolon
r_case
id|CPU_DEAD
suffix:colon
id|migrate_live_tasks
c_func
(paren
id|cpu
)paren
suffix:semicolon
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|cpu
)paren
suffix:semicolon
id|kthread_stop
c_func
(paren
id|rq-&gt;migration_thread
)paren
suffix:semicolon
id|rq-&gt;migration_thread
op_assign
l_int|NULL
suffix:semicolon
multiline_comment|/* Idle task back to normal (off runqueue, low prio) */
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|rq-&gt;idle
comma
op_amp
id|flags
)paren
suffix:semicolon
id|deactivate_task
c_func
(paren
id|rq-&gt;idle
comma
id|rq
)paren
suffix:semicolon
id|rq-&gt;idle-&gt;static_prio
op_assign
id|MAX_PRIO
suffix:semicolon
id|__setscheduler
c_func
(paren
id|rq-&gt;idle
comma
id|SCHED_NORMAL
comma
l_int|0
)paren
suffix:semicolon
id|migrate_dead_tasks
c_func
(paren
id|cpu
)paren
suffix:semicolon
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
id|migrate_nr_uninterruptible
c_func
(paren
id|rq
)paren
suffix:semicolon
id|BUG_ON
c_func
(paren
id|rq-&gt;nr_running
op_ne
l_int|0
)paren
suffix:semicolon
multiline_comment|/* No need to migrate the tasks: it was best-effort if&n;&t;&t; * they didn&squot;t do lock_cpu_hotplug().  Just wake up&n;&t;&t; * the requestors. */
id|spin_lock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
r_while
c_loop
(paren
op_logical_neg
id|list_empty
c_func
(paren
op_amp
id|rq-&gt;migration_queue
)paren
)paren
(brace
id|migration_req_t
op_star
id|req
suffix:semicolon
id|req
op_assign
id|list_entry
c_func
(paren
id|rq-&gt;migration_queue.next
comma
id|migration_req_t
comma
id|list
)paren
suffix:semicolon
id|BUG_ON
c_func
(paren
id|req-&gt;type
op_ne
id|REQ_MOVE_TASK
)paren
suffix:semicolon
id|list_del_init
c_func
(paren
op_amp
id|req-&gt;list
)paren
suffix:semicolon
id|complete
c_func
(paren
op_amp
id|req-&gt;done
)paren
suffix:semicolon
)brace
id|spin_unlock_irq
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
r_break
suffix:semicolon
macro_line|#endif
)brace
r_return
id|NOTIFY_OK
suffix:semicolon
)brace
multiline_comment|/* Register at highest priority so that task migration (migrate_all_tasks)&n; * happens before everything else.&n; */
DECL|variable|migration_notifier
r_static
r_struct
id|notifier_block
id|__devinitdata
id|migration_notifier
op_assign
(brace
dot
id|notifier_call
op_assign
id|migration_call
comma
dot
id|priority
op_assign
l_int|10
)brace
suffix:semicolon
DECL|function|migration_init
r_int
id|__init
id|migration_init
c_func
(paren
r_void
)paren
(brace
r_void
op_star
id|cpu
op_assign
(paren
r_void
op_star
)paren
(paren
r_int
)paren
id|smp_processor_id
c_func
(paren
)paren
suffix:semicolon
multiline_comment|/* Start one for boot CPU. */
id|migration_call
c_func
(paren
op_amp
id|migration_notifier
comma
id|CPU_UP_PREPARE
comma
id|cpu
)paren
suffix:semicolon
id|migration_call
c_func
(paren
op_amp
id|migration_notifier
comma
id|CPU_ONLINE
comma
id|cpu
)paren
suffix:semicolon
id|register_cpu_notifier
c_func
(paren
op_amp
id|migration_notifier
)paren
suffix:semicolon
r_return
l_int|0
suffix:semicolon
)brace
macro_line|#endif
macro_line|#ifdef CONFIG_SMP
DECL|macro|SCHED_DOMAIN_DEBUG
mdefine_line|#define SCHED_DOMAIN_DEBUG
macro_line|#ifdef SCHED_DOMAIN_DEBUG
DECL|function|sched_domain_debug
r_static
r_void
id|sched_domain_debug
c_func
(paren
r_struct
id|sched_domain
op_star
id|sd
comma
r_int
id|cpu
)paren
(brace
r_int
id|level
op_assign
l_int|0
suffix:semicolon
id|printk
c_func
(paren
id|KERN_DEBUG
l_string|&quot;CPU%d attaching sched-domain:&bslash;n&quot;
comma
id|cpu
)paren
suffix:semicolon
r_do
(brace
r_int
id|i
suffix:semicolon
r_char
id|str
(braket
id|NR_CPUS
)braket
suffix:semicolon
r_struct
id|sched_group
op_star
id|group
op_assign
id|sd-&gt;groups
suffix:semicolon
id|cpumask_t
id|groupmask
suffix:semicolon
id|cpumask_scnprintf
c_func
(paren
id|str
comma
id|NR_CPUS
comma
id|sd-&gt;span
)paren
suffix:semicolon
id|cpus_clear
c_func
(paren
id|groupmask
)paren
suffix:semicolon
id|printk
c_func
(paren
id|KERN_DEBUG
)paren
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
id|level
op_plus
l_int|1
suffix:semicolon
id|i
op_increment
)paren
id|printk
c_func
(paren
l_string|&quot; &quot;
)paren
suffix:semicolon
id|printk
c_func
(paren
l_string|&quot;domain %d: &quot;
comma
id|level
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
(paren
id|sd-&gt;flags
op_amp
id|SD_LOAD_BALANCE
)paren
)paren
(brace
id|printk
c_func
(paren
l_string|&quot;does not load-balance&bslash;n&quot;
)paren
suffix:semicolon
r_if
c_cond
(paren
id|sd-&gt;parent
)paren
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;ERROR: !SD_LOAD_BALANCE domain has parent&quot;
)paren
suffix:semicolon
r_break
suffix:semicolon
)brace
id|printk
c_func
(paren
l_string|&quot;span %s&bslash;n&quot;
comma
id|str
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|cpu_isset
c_func
(paren
id|cpu
comma
id|sd-&gt;span
)paren
)paren
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;ERROR: domain-&gt;span does not contain CPU%d&bslash;n&quot;
comma
id|cpu
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|cpu_isset
c_func
(paren
id|cpu
comma
id|group-&gt;cpumask
)paren
)paren
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;ERROR: domain-&gt;groups does not contain CPU%d&bslash;n&quot;
comma
id|cpu
)paren
suffix:semicolon
id|printk
c_func
(paren
id|KERN_DEBUG
)paren
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
id|level
op_plus
l_int|2
suffix:semicolon
id|i
op_increment
)paren
id|printk
c_func
(paren
l_string|&quot; &quot;
)paren
suffix:semicolon
id|printk
c_func
(paren
l_string|&quot;groups:&quot;
)paren
suffix:semicolon
r_do
(brace
r_if
c_cond
(paren
op_logical_neg
id|group
)paren
(brace
id|printk
c_func
(paren
l_string|&quot;&bslash;n&quot;
)paren
suffix:semicolon
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;ERROR: group is NULL&bslash;n&quot;
)paren
suffix:semicolon
r_break
suffix:semicolon
)brace
r_if
c_cond
(paren
op_logical_neg
id|group-&gt;cpu_power
)paren
(brace
id|printk
c_func
(paren
l_string|&quot;&bslash;n&quot;
)paren
suffix:semicolon
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;ERROR: domain-&gt;cpu_power not set&bslash;n&quot;
)paren
suffix:semicolon
)brace
r_if
c_cond
(paren
op_logical_neg
id|cpus_weight
c_func
(paren
id|group-&gt;cpumask
)paren
)paren
(brace
id|printk
c_func
(paren
l_string|&quot;&bslash;n&quot;
)paren
suffix:semicolon
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;ERROR: empty group&bslash;n&quot;
)paren
suffix:semicolon
)brace
r_if
c_cond
(paren
id|cpus_intersects
c_func
(paren
id|groupmask
comma
id|group-&gt;cpumask
)paren
)paren
(brace
id|printk
c_func
(paren
l_string|&quot;&bslash;n&quot;
)paren
suffix:semicolon
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;ERROR: repeated CPUs&bslash;n&quot;
)paren
suffix:semicolon
)brace
id|cpus_or
c_func
(paren
id|groupmask
comma
id|groupmask
comma
id|group-&gt;cpumask
)paren
suffix:semicolon
id|cpumask_scnprintf
c_func
(paren
id|str
comma
id|NR_CPUS
comma
id|group-&gt;cpumask
)paren
suffix:semicolon
id|printk
c_func
(paren
l_string|&quot; %s&quot;
comma
id|str
)paren
suffix:semicolon
id|group
op_assign
id|group-&gt;next
suffix:semicolon
)brace
r_while
c_loop
(paren
id|group
op_ne
id|sd-&gt;groups
)paren
suffix:semicolon
id|printk
c_func
(paren
l_string|&quot;&bslash;n&quot;
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|cpus_equal
c_func
(paren
id|sd-&gt;span
comma
id|groupmask
)paren
)paren
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;ERROR: groups don&squot;t span domain-&gt;span&bslash;n&quot;
)paren
suffix:semicolon
id|level
op_increment
suffix:semicolon
id|sd
op_assign
id|sd-&gt;parent
suffix:semicolon
r_if
c_cond
(paren
id|sd
)paren
(brace
r_if
c_cond
(paren
op_logical_neg
id|cpus_subset
c_func
(paren
id|groupmask
comma
id|sd-&gt;span
)paren
)paren
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;ERROR: parent span is not a superset of domain-&gt;span&bslash;n&quot;
)paren
suffix:semicolon
)brace
)brace
r_while
c_loop
(paren
id|sd
)paren
suffix:semicolon
)brace
macro_line|#else
DECL|macro|sched_domain_debug
mdefine_line|#define sched_domain_debug(sd, cpu) {}
macro_line|#endif
multiline_comment|/*&n; * Attach the domain &squot;sd&squot; to &squot;cpu&squot; as its base domain.  Callers must&n; * hold the hotplug lock.&n; */
DECL|function|cpu_attach_domain
r_void
id|__devinit
id|cpu_attach_domain
c_func
(paren
r_struct
id|sched_domain
op_star
id|sd
comma
r_int
id|cpu
)paren
(brace
id|migration_req_t
id|req
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
id|runqueue_t
op_star
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|cpu
)paren
suffix:semicolon
r_int
id|local
op_assign
l_int|1
suffix:semicolon
id|sched_domain_debug
c_func
(paren
id|sd
comma
id|cpu
)paren
suffix:semicolon
id|spin_lock_irqsave
c_func
(paren
op_amp
id|rq-&gt;lock
comma
id|flags
)paren
suffix:semicolon
r_if
c_cond
(paren
id|cpu
op_eq
id|smp_processor_id
c_func
(paren
)paren
op_logical_or
op_logical_neg
id|cpu_online
c_func
(paren
id|cpu
)paren
)paren
(brace
id|rq-&gt;sd
op_assign
id|sd
suffix:semicolon
)brace
r_else
(brace
id|init_completion
c_func
(paren
op_amp
id|req.done
)paren
suffix:semicolon
id|req.type
op_assign
id|REQ_SET_DOMAIN
suffix:semicolon
id|req.sd
op_assign
id|sd
suffix:semicolon
id|list_add
c_func
(paren
op_amp
id|req.list
comma
op_amp
id|rq-&gt;migration_queue
)paren
suffix:semicolon
id|local
op_assign
l_int|0
suffix:semicolon
)brace
id|spin_unlock_irqrestore
c_func
(paren
op_amp
id|rq-&gt;lock
comma
id|flags
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|local
)paren
(brace
id|wake_up_process
c_func
(paren
id|rq-&gt;migration_thread
)paren
suffix:semicolon
id|wait_for_completion
c_func
(paren
op_amp
id|req.done
)paren
suffix:semicolon
)brace
)brace
multiline_comment|/* cpus with isolated domains */
DECL|variable|cpu_isolated_map
id|cpumask_t
id|__devinitdata
id|cpu_isolated_map
op_assign
id|CPU_MASK_NONE
suffix:semicolon
multiline_comment|/* Setup the mask of cpus configured for isolated domains */
DECL|function|isolated_cpu_setup
r_static
r_int
id|__init
id|isolated_cpu_setup
c_func
(paren
r_char
op_star
id|str
)paren
(brace
r_int
id|ints
(braket
id|NR_CPUS
)braket
comma
id|i
suffix:semicolon
id|str
op_assign
id|get_options
c_func
(paren
id|str
comma
id|ARRAY_SIZE
c_func
(paren
id|ints
)paren
comma
id|ints
)paren
suffix:semicolon
id|cpus_clear
c_func
(paren
id|cpu_isolated_map
)paren
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
l_int|1
suffix:semicolon
id|i
op_le
id|ints
(braket
l_int|0
)braket
suffix:semicolon
id|i
op_increment
)paren
r_if
c_cond
(paren
id|ints
(braket
id|i
)braket
OL
id|NR_CPUS
)paren
id|cpu_set
c_func
(paren
id|ints
(braket
id|i
)braket
comma
id|cpu_isolated_map
)paren
suffix:semicolon
r_return
l_int|1
suffix:semicolon
)brace
id|__setup
(paren
l_string|&quot;isolcpus=&quot;
comma
id|isolated_cpu_setup
)paren
suffix:semicolon
multiline_comment|/*&n; * init_sched_build_groups takes an array of groups, the cpumask we wish&n; * to span, and a pointer to a function which identifies what group a CPU&n; * belongs to. The return value of group_fn must be a valid index into the&n; * groups[] array, and must be &gt;= 0 and &lt; NR_CPUS (due to the fact that we&n; * keep track of groups covered with a cpumask_t).&n; *&n; * init_sched_build_groups will build a circular linked list of the groups&n; * covered by the given span, and will set each group&squot;s -&gt;cpumask correctly,&n; * and -&gt;cpu_power to 0.&n; */
DECL|function|init_sched_build_groups
r_void
id|__devinit
id|init_sched_build_groups
c_func
(paren
r_struct
id|sched_group
id|groups
(braket
)braket
comma
id|cpumask_t
id|span
comma
r_int
(paren
op_star
id|group_fn
)paren
(paren
r_int
id|cpu
)paren
)paren
(brace
r_struct
id|sched_group
op_star
id|first
op_assign
l_int|NULL
comma
op_star
id|last
op_assign
l_int|NULL
suffix:semicolon
id|cpumask_t
id|covered
op_assign
id|CPU_MASK_NONE
suffix:semicolon
r_int
id|i
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|span
)paren
(brace
r_int
id|group
op_assign
id|group_fn
c_func
(paren
id|i
)paren
suffix:semicolon
r_struct
id|sched_group
op_star
id|sg
op_assign
op_amp
id|groups
(braket
id|group
)braket
suffix:semicolon
r_int
id|j
suffix:semicolon
r_if
c_cond
(paren
id|cpu_isset
c_func
(paren
id|i
comma
id|covered
)paren
)paren
r_continue
suffix:semicolon
id|sg-&gt;cpumask
op_assign
id|CPU_MASK_NONE
suffix:semicolon
id|sg-&gt;cpu_power
op_assign
l_int|0
suffix:semicolon
id|for_each_cpu_mask
c_func
(paren
id|j
comma
id|span
)paren
(brace
r_if
c_cond
(paren
id|group_fn
c_func
(paren
id|j
)paren
op_ne
id|group
)paren
r_continue
suffix:semicolon
id|cpu_set
c_func
(paren
id|j
comma
id|covered
)paren
suffix:semicolon
id|cpu_set
c_func
(paren
id|j
comma
id|sg-&gt;cpumask
)paren
suffix:semicolon
)brace
r_if
c_cond
(paren
op_logical_neg
id|first
)paren
id|first
op_assign
id|sg
suffix:semicolon
r_if
c_cond
(paren
id|last
)paren
id|last-&gt;next
op_assign
id|sg
suffix:semicolon
id|last
op_assign
id|sg
suffix:semicolon
)brace
id|last-&gt;next
op_assign
id|first
suffix:semicolon
)brace
macro_line|#ifdef ARCH_HAS_SCHED_DOMAIN
r_extern
r_void
id|__devinit
id|arch_init_sched_domains
c_func
(paren
r_void
)paren
suffix:semicolon
r_extern
r_void
id|__devinit
id|arch_destroy_sched_domains
c_func
(paren
r_void
)paren
suffix:semicolon
macro_line|#else
macro_line|#ifdef CONFIG_SCHED_SMT
r_static
id|DEFINE_PER_CPU
c_func
(paren
r_struct
id|sched_domain
comma
id|cpu_domains
)paren
suffix:semicolon
DECL|variable|sched_group_cpus
r_static
r_struct
id|sched_group
id|sched_group_cpus
(braket
id|NR_CPUS
)braket
suffix:semicolon
DECL|function|cpu_to_cpu_group
r_static
r_int
id|__devinit
id|cpu_to_cpu_group
c_func
(paren
r_int
id|cpu
)paren
(brace
r_return
id|cpu
suffix:semicolon
)brace
macro_line|#endif
r_static
id|DEFINE_PER_CPU
c_func
(paren
r_struct
id|sched_domain
comma
id|phys_domains
)paren
suffix:semicolon
DECL|variable|sched_group_phys
r_static
r_struct
id|sched_group
id|sched_group_phys
(braket
id|NR_CPUS
)braket
suffix:semicolon
DECL|function|cpu_to_phys_group
r_static
r_int
id|__devinit
id|cpu_to_phys_group
c_func
(paren
r_int
id|cpu
)paren
(brace
macro_line|#ifdef CONFIG_SCHED_SMT
r_return
id|first_cpu
c_func
(paren
id|cpu_sibling_map
(braket
id|cpu
)braket
)paren
suffix:semicolon
macro_line|#else
r_return
id|cpu
suffix:semicolon
macro_line|#endif
)brace
macro_line|#ifdef CONFIG_NUMA
r_static
id|DEFINE_PER_CPU
c_func
(paren
r_struct
id|sched_domain
comma
id|node_domains
)paren
suffix:semicolon
DECL|variable|sched_group_nodes
r_static
r_struct
id|sched_group
id|sched_group_nodes
(braket
id|MAX_NUMNODES
)braket
suffix:semicolon
DECL|function|cpu_to_node_group
r_static
r_int
id|__devinit
id|cpu_to_node_group
c_func
(paren
r_int
id|cpu
)paren
(brace
r_return
id|cpu_to_node
c_func
(paren
id|cpu
)paren
suffix:semicolon
)brace
macro_line|#endif
macro_line|#if defined(CONFIG_SCHED_SMT) &amp;&amp; defined(CONFIG_NUMA)
multiline_comment|/*&n; * The domains setup code relies on siblings not spanning&n; * multiple nodes. Make sure the architecture has a proper&n; * siblings map:&n; */
DECL|function|check_sibling_maps
r_static
r_void
id|check_sibling_maps
c_func
(paren
r_void
)paren
(brace
r_int
id|i
comma
id|j
suffix:semicolon
id|for_each_online_cpu
c_func
(paren
id|i
)paren
(brace
id|for_each_cpu_mask
c_func
(paren
id|j
comma
id|cpu_sibling_map
(braket
id|i
)braket
)paren
(brace
r_if
c_cond
(paren
id|cpu_to_node
c_func
(paren
id|i
)paren
op_ne
id|cpu_to_node
c_func
(paren
id|j
)paren
)paren
(brace
id|printk
c_func
(paren
id|KERN_INFO
l_string|&quot;warning: CPU %d siblings map &quot;
l_string|&quot;to different node - isolating &quot;
l_string|&quot;them.&bslash;n&quot;
comma
id|i
)paren
suffix:semicolon
id|cpu_sibling_map
(braket
id|i
)braket
op_assign
id|cpumask_of_cpu
c_func
(paren
id|i
)paren
suffix:semicolon
r_break
suffix:semicolon
)brace
)brace
)brace
)brace
macro_line|#endif
multiline_comment|/*&n; * Set up scheduler domains and groups.  Callers must hold the hotplug lock.&n; */
DECL|function|arch_init_sched_domains
r_static
r_void
id|__devinit
id|arch_init_sched_domains
c_func
(paren
r_void
)paren
(brace
r_int
id|i
suffix:semicolon
id|cpumask_t
id|cpu_default_map
suffix:semicolon
macro_line|#if defined(CONFIG_SCHED_SMT) &amp;&amp; defined(CONFIG_NUMA)
id|check_sibling_maps
c_func
(paren
)paren
suffix:semicolon
macro_line|#endif
multiline_comment|/*&n;&t; * Setup mask for cpus without special case scheduling requirements.&n;&t; * For now this just excludes isolated cpus, but could be used to&n;&t; * exclude other special cases in the future.&n;&t; */
id|cpus_complement
c_func
(paren
id|cpu_default_map
comma
id|cpu_isolated_map
)paren
suffix:semicolon
id|cpus_and
c_func
(paren
id|cpu_default_map
comma
id|cpu_default_map
comma
id|cpu_online_map
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Set up domains. Isolated domains just stay on the dummy domain.&n;&t; */
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|cpu_default_map
)paren
(brace
r_int
id|group
suffix:semicolon
r_struct
id|sched_domain
op_star
id|sd
op_assign
l_int|NULL
comma
op_star
id|p
suffix:semicolon
id|cpumask_t
id|nodemask
op_assign
id|node_to_cpumask
c_func
(paren
id|cpu_to_node
c_func
(paren
id|i
)paren
)paren
suffix:semicolon
id|cpus_and
c_func
(paren
id|nodemask
comma
id|nodemask
comma
id|cpu_default_map
)paren
suffix:semicolon
macro_line|#ifdef CONFIG_NUMA
id|sd
op_assign
op_amp
id|per_cpu
c_func
(paren
id|node_domains
comma
id|i
)paren
suffix:semicolon
id|group
op_assign
id|cpu_to_node_group
c_func
(paren
id|i
)paren
suffix:semicolon
op_star
id|sd
op_assign
id|SD_NODE_INIT
suffix:semicolon
id|sd-&gt;span
op_assign
id|cpu_default_map
suffix:semicolon
id|sd-&gt;groups
op_assign
op_amp
id|sched_group_nodes
(braket
id|group
)braket
suffix:semicolon
macro_line|#endif
id|p
op_assign
id|sd
suffix:semicolon
id|sd
op_assign
op_amp
id|per_cpu
c_func
(paren
id|phys_domains
comma
id|i
)paren
suffix:semicolon
id|group
op_assign
id|cpu_to_phys_group
c_func
(paren
id|i
)paren
suffix:semicolon
op_star
id|sd
op_assign
id|SD_CPU_INIT
suffix:semicolon
id|sd-&gt;span
op_assign
id|nodemask
suffix:semicolon
id|sd-&gt;parent
op_assign
id|p
suffix:semicolon
id|sd-&gt;groups
op_assign
op_amp
id|sched_group_phys
(braket
id|group
)braket
suffix:semicolon
macro_line|#ifdef CONFIG_SCHED_SMT
id|p
op_assign
id|sd
suffix:semicolon
id|sd
op_assign
op_amp
id|per_cpu
c_func
(paren
id|cpu_domains
comma
id|i
)paren
suffix:semicolon
id|group
op_assign
id|cpu_to_cpu_group
c_func
(paren
id|i
)paren
suffix:semicolon
op_star
id|sd
op_assign
id|SD_SIBLING_INIT
suffix:semicolon
id|sd-&gt;span
op_assign
id|cpu_sibling_map
(braket
id|i
)braket
suffix:semicolon
id|cpus_and
c_func
(paren
id|sd-&gt;span
comma
id|sd-&gt;span
comma
id|cpu_default_map
)paren
suffix:semicolon
id|sd-&gt;parent
op_assign
id|p
suffix:semicolon
id|sd-&gt;groups
op_assign
op_amp
id|sched_group_cpus
(braket
id|group
)braket
suffix:semicolon
macro_line|#endif
)brace
macro_line|#ifdef CONFIG_SCHED_SMT
multiline_comment|/* Set up CPU (sibling) groups */
id|for_each_online_cpu
c_func
(paren
id|i
)paren
(brace
id|cpumask_t
id|this_sibling_map
op_assign
id|cpu_sibling_map
(braket
id|i
)braket
suffix:semicolon
id|cpus_and
c_func
(paren
id|this_sibling_map
comma
id|this_sibling_map
comma
id|cpu_default_map
)paren
suffix:semicolon
r_if
c_cond
(paren
id|i
op_ne
id|first_cpu
c_func
(paren
id|this_sibling_map
)paren
)paren
r_continue
suffix:semicolon
id|init_sched_build_groups
c_func
(paren
id|sched_group_cpus
comma
id|this_sibling_map
comma
op_amp
id|cpu_to_cpu_group
)paren
suffix:semicolon
)brace
macro_line|#endif
multiline_comment|/* Set up physical groups */
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
id|MAX_NUMNODES
suffix:semicolon
id|i
op_increment
)paren
(brace
id|cpumask_t
id|nodemask
op_assign
id|node_to_cpumask
c_func
(paren
id|i
)paren
suffix:semicolon
id|cpus_and
c_func
(paren
id|nodemask
comma
id|nodemask
comma
id|cpu_default_map
)paren
suffix:semicolon
r_if
c_cond
(paren
id|cpus_empty
c_func
(paren
id|nodemask
)paren
)paren
r_continue
suffix:semicolon
id|init_sched_build_groups
c_func
(paren
id|sched_group_phys
comma
id|nodemask
comma
op_amp
id|cpu_to_phys_group
)paren
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_NUMA
multiline_comment|/* Set up node groups */
id|init_sched_build_groups
c_func
(paren
id|sched_group_nodes
comma
id|cpu_default_map
comma
op_amp
id|cpu_to_node_group
)paren
suffix:semicolon
macro_line|#endif
multiline_comment|/* Calculate CPU power for physical packages and nodes */
id|for_each_cpu_mask
c_func
(paren
id|i
comma
id|cpu_default_map
)paren
(brace
r_int
id|power
suffix:semicolon
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
macro_line|#ifdef CONFIG_SCHED_SMT
id|sd
op_assign
op_amp
id|per_cpu
c_func
(paren
id|cpu_domains
comma
id|i
)paren
suffix:semicolon
id|power
op_assign
id|SCHED_LOAD_SCALE
suffix:semicolon
id|sd-&gt;groups-&gt;cpu_power
op_assign
id|power
suffix:semicolon
macro_line|#endif
id|sd
op_assign
op_amp
id|per_cpu
c_func
(paren
id|phys_domains
comma
id|i
)paren
suffix:semicolon
id|power
op_assign
id|SCHED_LOAD_SCALE
op_plus
id|SCHED_LOAD_SCALE
op_star
(paren
id|cpus_weight
c_func
(paren
id|sd-&gt;groups-&gt;cpumask
)paren
op_minus
l_int|1
)paren
op_div
l_int|10
suffix:semicolon
id|sd-&gt;groups-&gt;cpu_power
op_assign
id|power
suffix:semicolon
macro_line|#ifdef CONFIG_NUMA
r_if
c_cond
(paren
id|i
op_eq
id|first_cpu
c_func
(paren
id|sd-&gt;groups-&gt;cpumask
)paren
)paren
(brace
multiline_comment|/* Only add &quot;power&quot; once for each physical package. */
id|sd
op_assign
op_amp
id|per_cpu
c_func
(paren
id|node_domains
comma
id|i
)paren
suffix:semicolon
id|sd-&gt;groups-&gt;cpu_power
op_add_assign
id|power
suffix:semicolon
)brace
macro_line|#endif
)brace
multiline_comment|/* Attach the domains */
id|for_each_online_cpu
c_func
(paren
id|i
)paren
(brace
r_struct
id|sched_domain
op_star
id|sd
suffix:semicolon
macro_line|#ifdef CONFIG_SCHED_SMT
id|sd
op_assign
op_amp
id|per_cpu
c_func
(paren
id|cpu_domains
comma
id|i
)paren
suffix:semicolon
macro_line|#else
id|sd
op_assign
op_amp
id|per_cpu
c_func
(paren
id|phys_domains
comma
id|i
)paren
suffix:semicolon
macro_line|#endif
id|cpu_attach_domain
c_func
(paren
id|sd
comma
id|i
)paren
suffix:semicolon
)brace
)brace
macro_line|#ifdef CONFIG_HOTPLUG_CPU
DECL|function|arch_destroy_sched_domains
r_static
r_void
id|__devinit
id|arch_destroy_sched_domains
c_func
(paren
r_void
)paren
(brace
multiline_comment|/* Do nothing: everything is statically allocated. */
)brace
macro_line|#endif
macro_line|#endif /* ARCH_HAS_SCHED_DOMAIN */
multiline_comment|/*&n; * Initial dummy domain for early boot and for hotplug cpu. Being static,&n; * it is initialized to zero, so all balancing flags are cleared which is&n; * what we want.&n; */
DECL|variable|sched_domain_dummy
r_static
r_struct
id|sched_domain
id|sched_domain_dummy
suffix:semicolon
macro_line|#ifdef CONFIG_HOTPLUG_CPU
multiline_comment|/*&n; * Force a reinitialization of the sched domains hierarchy.  The domains&n; * and groups cannot be updated in place without racing with the balancing&n; * code, so we temporarily attach all running cpus to a &quot;dummy&quot; domain&n; * which will prevent rebalancing while the sched domains are recalculated.&n; */
DECL|function|update_sched_domains
r_static
r_int
id|update_sched_domains
c_func
(paren
r_struct
id|notifier_block
op_star
id|nfb
comma
r_int
r_int
id|action
comma
r_void
op_star
id|hcpu
)paren
(brace
r_int
id|i
suffix:semicolon
r_switch
c_cond
(paren
id|action
)paren
(brace
r_case
id|CPU_UP_PREPARE
suffix:colon
r_case
id|CPU_DOWN_PREPARE
suffix:colon
id|for_each_online_cpu
c_func
(paren
id|i
)paren
id|cpu_attach_domain
c_func
(paren
op_amp
id|sched_domain_dummy
comma
id|i
)paren
suffix:semicolon
id|arch_destroy_sched_domains
c_func
(paren
)paren
suffix:semicolon
r_return
id|NOTIFY_OK
suffix:semicolon
r_case
id|CPU_UP_CANCELED
suffix:colon
r_case
id|CPU_DOWN_FAILED
suffix:colon
r_case
id|CPU_ONLINE
suffix:colon
r_case
id|CPU_DEAD
suffix:colon
multiline_comment|/*&n;&t;&t; * Fall through and re-initialise the domains.&n;&t;&t; */
r_break
suffix:semicolon
r_default
suffix:colon
r_return
id|NOTIFY_DONE
suffix:semicolon
)brace
multiline_comment|/* The hotplug lock is already held by cpu_up/cpu_down */
id|arch_init_sched_domains
c_func
(paren
)paren
suffix:semicolon
r_return
id|NOTIFY_OK
suffix:semicolon
)brace
macro_line|#endif
DECL|function|sched_init_smp
r_void
id|__init
id|sched_init_smp
c_func
(paren
r_void
)paren
(brace
id|lock_cpu_hotplug
c_func
(paren
)paren
suffix:semicolon
id|arch_init_sched_domains
c_func
(paren
)paren
suffix:semicolon
id|unlock_cpu_hotplug
c_func
(paren
)paren
suffix:semicolon
multiline_comment|/* XXX: Theoretical race here - CPU may be hotplugged now */
id|hotcpu_notifier
c_func
(paren
id|update_sched_domains
comma
l_int|0
)paren
suffix:semicolon
)brace
macro_line|#else
DECL|function|sched_init_smp
r_void
id|__init
id|sched_init_smp
c_func
(paren
r_void
)paren
(brace
)brace
macro_line|#endif /* CONFIG_SMP */
DECL|function|in_sched_functions
r_int
id|in_sched_functions
c_func
(paren
r_int
r_int
id|addr
)paren
(brace
multiline_comment|/* Linker adds these: start and end of __sched functions */
r_extern
r_char
id|__sched_text_start
(braket
)braket
comma
id|__sched_text_end
(braket
)braket
suffix:semicolon
r_return
id|in_lock_functions
c_func
(paren
id|addr
)paren
op_logical_or
(paren
id|addr
op_ge
(paren
r_int
r_int
)paren
id|__sched_text_start
op_logical_and
id|addr
OL
(paren
r_int
r_int
)paren
id|__sched_text_end
)paren
suffix:semicolon
)brace
DECL|function|sched_init
r_void
id|__init
id|sched_init
c_func
(paren
r_void
)paren
(brace
id|runqueue_t
op_star
id|rq
suffix:semicolon
r_int
id|i
comma
id|j
comma
id|k
suffix:semicolon
r_for
c_loop
(paren
id|i
op_assign
l_int|0
suffix:semicolon
id|i
OL
id|NR_CPUS
suffix:semicolon
id|i
op_increment
)paren
(brace
id|prio_array_t
op_star
id|array
suffix:semicolon
id|rq
op_assign
id|cpu_rq
c_func
(paren
id|i
)paren
suffix:semicolon
id|spin_lock_init
c_func
(paren
op_amp
id|rq-&gt;lock
)paren
suffix:semicolon
id|rq-&gt;active
op_assign
id|rq-&gt;arrays
suffix:semicolon
id|rq-&gt;expired
op_assign
id|rq-&gt;arrays
op_plus
l_int|1
suffix:semicolon
id|rq-&gt;best_expired_prio
op_assign
id|MAX_PRIO
suffix:semicolon
macro_line|#ifdef CONFIG_SMP
id|rq-&gt;sd
op_assign
op_amp
id|sched_domain_dummy
suffix:semicolon
id|rq-&gt;cpu_load
op_assign
l_int|0
suffix:semicolon
id|rq-&gt;active_balance
op_assign
l_int|0
suffix:semicolon
id|rq-&gt;push_cpu
op_assign
l_int|0
suffix:semicolon
id|rq-&gt;migration_thread
op_assign
l_int|NULL
suffix:semicolon
id|INIT_LIST_HEAD
c_func
(paren
op_amp
id|rq-&gt;migration_queue
)paren
suffix:semicolon
macro_line|#endif
id|atomic_set
c_func
(paren
op_amp
id|rq-&gt;nr_iowait
comma
l_int|0
)paren
suffix:semicolon
r_for
c_loop
(paren
id|j
op_assign
l_int|0
suffix:semicolon
id|j
OL
l_int|2
suffix:semicolon
id|j
op_increment
)paren
(brace
id|array
op_assign
id|rq-&gt;arrays
op_plus
id|j
suffix:semicolon
r_for
c_loop
(paren
id|k
op_assign
l_int|0
suffix:semicolon
id|k
OL
id|MAX_PRIO
suffix:semicolon
id|k
op_increment
)paren
(brace
id|INIT_LIST_HEAD
c_func
(paren
id|array-&gt;queue
op_plus
id|k
)paren
suffix:semicolon
id|__clear_bit
c_func
(paren
id|k
comma
id|array-&gt;bitmap
)paren
suffix:semicolon
)brace
singleline_comment|// delimiter for bitsearch
id|__set_bit
c_func
(paren
id|MAX_PRIO
comma
id|array-&gt;bitmap
)paren
suffix:semicolon
)brace
)brace
multiline_comment|/*&n;&t; * The boot idle thread does lazy MMU switching as well:&n;&t; */
id|atomic_inc
c_func
(paren
op_amp
id|init_mm.mm_count
)paren
suffix:semicolon
id|enter_lazy_tlb
c_func
(paren
op_amp
id|init_mm
comma
id|current
)paren
suffix:semicolon
multiline_comment|/*&n;&t; * Make us the idle thread. Technically, schedule() should not be&n;&t; * called from this thread, however somewhere below it might be,&n;&t; * but because we are the idle thread, we just pick up running again&n;&t; * when this runqueue becomes &quot;idle&quot;.&n;&t; */
id|init_idle
c_func
(paren
id|current
comma
id|smp_processor_id
c_func
(paren
)paren
)paren
suffix:semicolon
)brace
macro_line|#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
DECL|function|__might_sleep
r_void
id|__might_sleep
c_func
(paren
r_char
op_star
id|file
comma
r_int
id|line
)paren
(brace
macro_line|#if defined(in_atomic)
r_static
r_int
r_int
id|prev_jiffy
suffix:semicolon
multiline_comment|/* ratelimiting */
r_if
c_cond
(paren
(paren
id|in_atomic
c_func
(paren
)paren
op_logical_or
id|irqs_disabled
c_func
(paren
)paren
)paren
op_logical_and
id|system_state
op_eq
id|SYSTEM_RUNNING
op_logical_and
op_logical_neg
id|oops_in_progress
)paren
(brace
r_if
c_cond
(paren
id|time_before
c_func
(paren
id|jiffies
comma
id|prev_jiffy
op_plus
id|HZ
)paren
op_logical_and
id|prev_jiffy
)paren
r_return
suffix:semicolon
id|prev_jiffy
op_assign
id|jiffies
suffix:semicolon
id|printk
c_func
(paren
id|KERN_ERR
l_string|&quot;Debug: sleeping function called from invalid&quot;
l_string|&quot; context at %s:%d&bslash;n&quot;
comma
id|file
comma
id|line
)paren
suffix:semicolon
id|printk
c_func
(paren
l_string|&quot;in_atomic():%d, irqs_disabled():%d&bslash;n&quot;
comma
id|in_atomic
c_func
(paren
)paren
comma
id|irqs_disabled
c_func
(paren
)paren
)paren
suffix:semicolon
id|dump_stack
c_func
(paren
)paren
suffix:semicolon
)brace
macro_line|#endif
)brace
DECL|variable|__might_sleep
id|EXPORT_SYMBOL
c_func
(paren
id|__might_sleep
)paren
suffix:semicolon
macro_line|#endif
macro_line|#ifdef CONFIG_MAGIC_SYSRQ
DECL|function|normalize_rt_tasks
r_void
id|normalize_rt_tasks
c_func
(paren
r_void
)paren
(brace
r_struct
id|task_struct
op_star
id|p
suffix:semicolon
id|prio_array_t
op_star
id|array
suffix:semicolon
r_int
r_int
id|flags
suffix:semicolon
id|runqueue_t
op_star
id|rq
suffix:semicolon
id|read_lock_irq
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
id|for_each_process
(paren
id|p
)paren
(brace
r_if
c_cond
(paren
op_logical_neg
id|rt_task
c_func
(paren
id|p
)paren
)paren
r_continue
suffix:semicolon
id|rq
op_assign
id|task_rq_lock
c_func
(paren
id|p
comma
op_amp
id|flags
)paren
suffix:semicolon
id|array
op_assign
id|p-&gt;array
suffix:semicolon
r_if
c_cond
(paren
id|array
)paren
id|deactivate_task
c_func
(paren
id|p
comma
id|task_rq
c_func
(paren
id|p
)paren
)paren
suffix:semicolon
id|__setscheduler
c_func
(paren
id|p
comma
id|SCHED_NORMAL
comma
l_int|0
)paren
suffix:semicolon
r_if
c_cond
(paren
id|array
)paren
(brace
id|__activate_task
c_func
(paren
id|p
comma
id|task_rq
c_func
(paren
id|p
)paren
)paren
suffix:semicolon
id|resched_task
c_func
(paren
id|rq-&gt;curr
)paren
suffix:semicolon
)brace
id|task_rq_unlock
c_func
(paren
id|rq
comma
op_amp
id|flags
)paren
suffix:semicolon
)brace
id|read_unlock_irq
c_func
(paren
op_amp
id|tasklist_lock
)paren
suffix:semicolon
)brace
macro_line|#endif /* CONFIG_MAGIC_SYSRQ */
eof
